참조 URL : https://mindscale.kr/course/basic-stat-r/regression/

### 결정계수(R<sup>2</sup>)

- 설명력 : R<sup>2</sup>는 전체 데이터를 회귀모형이 설명할 수 있는 설명력을 의미

- SSR / SST (전체 제곱합에서 회귀제곱합의 비율), $0 \le R^2 \le 1$

    > $SST = \displaystyle \sum(y_i - \overline{y})^2 , \quad SSE = \displaystyle \sum (y_i - \hat{y_i})^2, \quad SSR = SST - SSE$

- 단순선형회귀분석에서는 결정계수는 상관계수 r의 제곱과 같다.



### 코드 참조

```R
> summary(lm(formula=dist ~ speed, data=cars))
Call:
lm(formula = dist ~ speed, data = cars)

Residuals:
    Min      1Q  Median      3Q     Max 
-29.069  -9.525  -2.272   9.215  43.201 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.38 on 48 degrees of freedom
Multiple R-squared:  0.6511,	Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

### 잔차(Residuals)

- 예측값과 실제값의 차이

- ```R
    Residuals:
      Min        1Q       Median     3Q       Max
    -29.056    -9.525    -2.272    9.215    43.201
    ```

- 잔차의 최솟값(Min), 사분위수(1Q, Median, 3Q), 최대값(Max)을 보여줌

- 중앙값이 0에 가깝고, 1, 3사분위 수가 거의 대칭을 이루고 있으므로, 잔차가 정규분포에서 것의 벗어나지 않았다고 볼수 있음

> 오차(error)와 잔차(residual)의 차이
>
> - 오차 : 모집단에서 실제값이 회귀선과 비교해 볼 때 나타나는 차이(정확치와 관측치의 차이)
> - 잔차 : 표본에서 나온 관측값이 회귀선과 비교해볼 때 나타나는 차이

### 회귀계수(Coefficients)

```R
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
```



- Estimate는 데이터로 부터 얻은 계수의 추정치(estimate)를 말함
- 절편(Intercept)의 추정치는 -17.5791로,  ```speed```가 0 일때 ```dist```의 값이다
- ```speed```의 계수 추정치는 3.9324로 ```speed```가 1 증가할 때마다 ```dist```가 3.9324 증가한다는 것을 의미함
- 이를 수식으로 정리하면 $dist = -17.5791 + 3.9324 \times speed$
- 추정치 오른쪽 끝의 ```Pr(>|t|)```는 모집단에서 계수가 0 일때, 현재와 같은 크기의 표본에서 이런한 계수가 추정될 확률인 p 값을 나타낸다. 이확률이 매우 작다는 것은, 모집단에서 ```speed```의 계수가 정확히 3.9324가 아니더라도 현재의 표본과 비슷하게 0보다 큰 어떤 범위에 있을 가능성이 높다는 것을 의미한다. 보통 5%와 같은 유의수준을 정하여 p값이 그 보다 작으면 (p < 0.05), ```"통계적으로 유의하다"``` 라고 한다.



### 모형적합도

```R
Multiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

Multiple R-squared, Adjusted R-squared, F-statistic, p-value는 모형이 데이터에 잘 맞는 정도를 보여주는 지표들이다.

- Multiple R-squared: 0.6511
    - 모형 적합도(혹은 설명력)
    - `dist`의 분산을 `speed`가 약 65%를 설명한다
    - 각 사례마다 `dist`에 차이가 있다.
- Adjusted R-squared: 0.6438
    - 독립변수가 여러 개인 다중회귀분석에서 사용
    - 독립변수의 개수와 표본의 크기를 고려하여 R-squared를 보정
    - 서로 다른 모형을 비교할 때는 이 지표가 높은 쪽은 선택한다
- F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12
    - 회귀모형에 대한 (통계적) 유의미성 검증 결과, 유의미함 (p < 0.05)
    - 즉, 이 모형은 주어진 표본 뿐 아니라 모집단에서도 의미있는 모형이라 할 수 있음



### 결과 보고

논문 등에서 회귀분석의 결과는 다음 순서대로 보고한다.

먼저 모형적합도를 보고한다. F 분포의 파라미터 2개와 그 때의 F 값, p-value와 유의수준의 비교를 적시한다.

> dist에 대하여 speed로 예측하는 회귀분석을 실시한 결과, 이 회귀모형은 통계적으로 유의미하였다(F(1,48) = 89.57, p < 0.05).

다음으로 독립변수에 대해 보고한다.

> speed의 회귀계수는 3.9324로, dist에 대하여 유의미한 예측변인인 것으로 나타났다(t(48) = 9.464, p < 0.05).



### 결정계수(R<sup>2</sup>)

- 결정계수(R<sup>2</sup>)를 통해 추정된 회귀식이 얼마나 타당한지 검토한다. (결정계수가 1에 가까울수록 회귀모형이 자료를 잘 설명함)

- 설명력 : R2는 전체 데이터를 회귀모형이 설명할 수 있는 설명력을 의미

- 독립변수가 종속변수 변동의 몇%를 설명하는지 나타내는 지표이다.

- SSR / SST (전체 제곱합에서 회귀제곱합의 비율), $0 \le R^2 \le 1$

    >   $SST = \displaystyle \sum(y_i - \overline{y})^2 , \quad SSE = \displaystyle \sum (y_i - \hat{y_i})^2, \quad SSR = SST - SSE$

- 단순선형회귀분석에서는 결정계수는 상관계수 r의 제곱과 같다.

- 다변량 회귀분석에서는 독립변수가 많아지면 결정계수가 높아지므로 독립변수가 유의하든, 그렇지 않든 독립변수의 수가 많아지면 결정계수가 높아지는 단점이 있다.

- 이러한 결정계수의 단점을 보완하기 위해 수정 결정계수(R<sup>2</sup>: adjusted R<sup>2</sup>)를 활용한다. 수정결정계수는 결정계수보다 작은 값으로 산출되는 특징이 있다.

- 수정 결정계수 (adjusted R<sup>2</sup>) 식

    $R_a^2 = 1 - \dfrac {(n-1)(1-R^2)} {n - k - 1} \newline = 1- \dfrac {(n-1) \times (\dfrac {SSE} {SST})} {n-k-1} \newline = 1 - (n-1)\dfrac {MSE} {SST} \newline (k:독립변수 개수, n: 데이터 개수)$

    



### 상관계수(r)

-   두 변수의 상관성을 나타내는 척도

-   항상 -1과 1 사이의 값을 가짐 ($-1 \le r \le 1 $)

-   1은 양의 상관관계, 1은 음의 상관관계 (0 일땐 상관관계가 없다고 봄)

    >   두 변수간 선형적인 관계를 나타내는 것이므로 상관계수 값이 0일 지라도, 두 변수간 비선형적인 관계가 있을 수 있다.

-   $r = \dfrac {cov(x, y)} {\displaystyle \sqrt{var(x) \times var(y)}} = \dfrac {\sum (x_i - \overline{x})(y_i - \overline{y})} {\displaystyle \sqrt{\sum (x_i - \overline{x})^2(y_i - \overline{y})^2}}$







### step 함수 이용한 전진 선택법

```R
> x1 <- c(7,1,11,11,7,11,3,1,2,21,1,11,10)
> x2 <- c(26,29,56,31,52,55,71,31,54,47,40,66,68)
> x3 <- c(6,15,8,8,6,9,17,22,18,4,23,9,8)
> x4 <- c(60,52,20,47,33,22,6,44,22,26,34,12,12)
> y <- c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4)
> df <- data.frame(x1, x2, x3, x4, y)
> head(df)
  x1 x2 x3 x4     y
1  7 26  6 60  78.5
2  1 29 15 52  74.3
3 11 56  8 20 104.3
4 11 31  8 47  87.6
5  7 52  6 33  95.9
6 11 55  9 22 109.2


> # step 함수를 사용한 전진선택법의 적용
> step(lm(y~1, data=df), scope=list(lower=~1, upper=~x1+x2+x3+x4), direction="forward")
Start:  AIC=71.44
y ~ 1

       Df Sum of Sq     RSS    AIC
+ x4    1   1831.90  883.87 58.852
+ x2    1   1809.43  906.34 59.178
+ x1    1   1450.08 1265.69 63.519
+ x3    1    776.36 1939.40 69.067
<none>              2715.76 71.444

Step:  AIC=58.85
y ~ x4

       Df Sum of Sq    RSS    AIC
+ x1    1    809.10  74.76 28.742
+ x3    1    708.13 175.74 39.853
<none>              883.87 58.852
+ x2    1     14.99 868.88 60.629

Step:  AIC=28.74
y ~ x4 + x1

       Df Sum of Sq    RSS    AIC
+ x2    1    26.789 47.973 24.974
+ x3    1    23.926 50.836 25.728
<none>              74.762 28.742

Step:  AIC=24.97
y ~ x4 + x1 + x2

       Df Sum of Sq    RSS    AIC
<none>              47.973 24.974
+ x3    1   0.10909 47.864 26.944

Call:
lm(formula = y ~ x4 + x1 + x2, data = df)

Coefficients:
(Intercept)           x4           x1           x2  
    71.6483      -0.2365       1.4519       0.4161  

> 
```

- 벌점화 방식을 적용한 전진선택법을 실시한 결과, 가장 먼저 선택된 변수는 ```AIC``` 값이 ```58.852```으로 가장 낮은 ```x4``` 였다.`x4`에 `x1`을 추가하였을 떄 `AIC` 값이 `28.742`로 낮아지게 되었고, `x2`를 추가하였을 때 `AIC` 값이 `24.974`로 최소화되어 더 이상 `AIC`를 낮출 수 없어 변수선택을 종료하게 되었다.
- 최종적으로 선택된 추정된 회귀식은 `y = 71.6483 - 0.23645*x4 + 1.4519*x1 + 0.4161*x2` 이다.

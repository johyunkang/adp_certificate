### 1과목 데이터의 이해

#### 데이터베이스의 특징 4가지

-   통합된 데이터, 저장된 데이터, 공용 데이터, 운영 데이터

####  

#### 빅데이터 분석에 경제성을 제공해 준 기술

-   클라우드 컴퓨팅의 보편화는 처리 비용을 획기적으로 낮춰 경제성을 제공했다.



#### 사물인터넷의 의미로 가장 적절한 것

-   모든 것의 데이터화(datafication)



#### 빅데이터 가치 산정이 어려운 이유

- 데이터 활용방식 : 재사용, 재조합, 다목적용 개발
- 새로운 가치 창출
- 분석 기술 발전



#### 빅데이터의 위기 요인과 사생활 침해

빅데이터 시대의 위기요인과 통제방안

- 사생활 침해
    - 여행 사실 트위터를 통해 빈집 강도 (익명화 기술 발전 필요)
    - 동의제에서 책임으로 : '개인정보 제공자의 동의' > '개인정보 사용자의 책임'. (사용주체의 적극적인 보호장치 강구)
- 책임 원칙 훼손
    - 마이너리티 리포트 처럼 범행 저지르기 전에 체포. 민주주의 국가의 형사 처벌은 잠재적 위협이 아닌 행동 결과에 대해 책임을 물음
    - 결과 기반 책임 원칙 고수 : 잘못된 예측 알고리즘을 통한 근거로 불이익을 줄 수 없으며, 이에 따른 피해 최소화 장치 마련
- 데이터 오용
    - 잘못된 지표를 사용하는 것도 빅데이터의 폐해. 적군 사망자 수를 전쟁의 진척 상황 지표로 활용해 전국 사망자 수가 과장돼 보고되는 경향
    - 알고리즘 접근 허용 : '알고리즘에 대한 접근권'을 제공하여 알고리즘의 부당함을 반증할 수 있는 방법 명시해 공개할 것을 주문. 알고리즈미스트가 필요하게 됨.



#### 데이터사이언스

- 의미 : 데이터 공학, 수학, 통계학, 컴퓨터공학, 시각화, 해커의 사고방식, 해당 분야의 전문지식을 종합한 학문이다. 데이터로부터 의미있는 정보를 추출해내는 학문
- 역할
    - 강력한 호기심으로 문제의 이면을 파고들고, 질문들을 찾고, 검증 가능한 가설을 세우는 능력을 의미
    - 스토리텔링, 커뮤니케이션, 창의력, 열정, 직관련, 비판적 시각, 글쓰기 능력, 대화능력 등을 갖춰야 함
- 요구역량
    - Hard Skill
        -   빅데이터에 대한 이론적 지식
        -   분석 기술에 대한 숙련
    - Soft Skill
        -   통찰력 있는 분석 : 창의적 사고, 호기심, 논리적 비판
        -   설득력 있는 전달 : 스토리텔링, 비주얼라이제이션
        -   다분양간 협력 : 커뮤니케이션



#### 빅데이터 활용의 3요소

-   데이터 : 모든 것의 데이터화
-   기술 : 진화하는 알고리즘, 인공지능
-   인력 : 데이터 사이언티스트, 알고리즈미스트



#### 사회기반구조로서의 데이터베이스

-   물류부문
    -   CVO (Commercial Vehicle Operation System, 화물운송정보)
    -   PORT-MIS (항만운영정보시스템)
    -   KROIS (철도운영정보시스템)
-   지리/교통부문
    -   GIS (Geographic Information System, 지리정보시스템)
    -   RS (Remote Sensing, 원격탐사)
    -   GPS (Global Positioning System, 범지구위치결정시스템)
    -   ITS (Intelligent Transport System, 지능형교통시스템)
    -   LBS (Location Based Service, 위치기반서비스)
    -   SIM (Spatial Information Management, 공간정보관리)
-   의료부문
    -   PACS (Picture Archiving and Communication System)
    -   U헬스 (Ubiquitous-Health)
-   교육부문
    -   NEIS (National Education Information System, 교육행정정보시스템)





#### 산업별 일차원적 분석 애플리케이션

- **금융 서비스 : 신용점수 산정, 사기 탐지, 가격 책정, 프로그램트레이딩, 클레임분석, 고객 수익성 분석**
- **병원 : 가격 책정, 고객 로열티, 수이 관리**
- **에너지 : 트레이딩, 공급/수요 예측**
- **정부 : 사기 탐지, 사례 관리, 범죄 방지, 수익 최적화**
- 제조업 : 공급사슬 최적화, 수요예측, 재고 보충, 보증서 분석, 맞춤형 상품 개발, 신상품 개발
- 소매업 : 판촉, 매대 관리, 수요 예측, 재고 보충, 가격 및 제조 최적화



#### 빅데이터가 만들어 내는 과거에서 현재로의 변화

-   사전처리 > 사후처리 : 가능한 많은 데이터를 모으고, 다양하게 조합해 숨은 정보를 찾아냄
-   표본조사 > 전수조사 : 데이터 수집비용 감소와 클라우드 컴퓨팅 발전으로 전수조사를 통해 데이터를 활용
-   질 > 양 : 데이터가 지속적으로 추가될 경우 양질의 정보가 오류 정보보다 많아 전체적으로 좋은 결과를 산출
-   인과관계 > 상관관계 : 상관관계를 통해 특정 현상의 발생 가능성이 포착됨



#### 지식경영의 암묵지와 형식지

-   암묵지 (tacit knowledge)
    -   의미 : 학습과 경험을 통해 개인에게 체화되어 있지만 겉으로 드러나지 않는 지식
    -   특징 : 사회적으로 중요하지만 다른 사람에게 공유되기 어려움
    -   예 : 김장 김치 담그기, 자전거 타기
    -   상호작용 : 공통화, 내면화 (개인에게 축적된 **내면화**된 지식 > 조직의 지식으로 **공통화**)
-   형식지 (explicit knowledge)
    -   의미 : 문서나 메뉴얼 처럼 형상화된 지식
    -   특징 : 전달과 공유가 용이함
    -   예 : 교과서, 비디오, DB
    -   상호작용 : 표출화, 연결화 (언어, 숫자, 기호로 **표출화**된 지식 > 개인의 지식으로 **연결화**)
-   분석 수행 프로세스와 노하우 등의 암묵지가 형식지화 되는 과정을 거쳐 분석방법론으로 발전하는 과정
    -   형식화 > 체계화 > 내재화



### 2과목 데이터 처리 기술의 이해

#### 데이터베이스 클러스터에 대한 설명

-   MySQL 클러스터는 비공유형으로서 메모리(최근 디스크도 제공) 기반 데이터베이스의 클러스터링을 지원



####  ETL에 대한 설명 

-   ETL은 배치 프로세스 중심이며, MPP(Massive Parallel Processing)을 지원한다.



####  VMWare 관련 메모리 가상화 기법에 대한 설명

-   Transparent page sharing : 하나의 물리저인 머신에 여러 개의 가상머신이 운영되는 경우, 각 가상머신에 할당된 메모리 중 동일한 내용을 담고 있는 페이지는 물리적인 메모리 영역에 하나만 존재시키고 모든 가상 머신이 공유하도록 하는 것



#### CDC (Change Data Capture)의 구현 기법에 관한 설명

-   개념 및 특징
    -   데이터에 대한 변경을 식별해 필요한 후속처리를 자동화 하는 기술 또는 구조
    -   **실시간** 또는 **실시간 근접** 데이터 통합을 기반으로 DW 및 기타 데이터 저장소 구축에 활용
    -   스토리지 HW 계층부터 애플리케이션 계층에 이르기 까지 다양한 계층에서 구현

-   Log scanner on database : 로그에 대한 스캐닝 및 변경 내역에 대한 해석을 통해 CDC 메커니즘을 구현하는 기법으로 데이터베이스 스키마의 변경을 필요로 하진 않는다.
-   Version Numbers on Rows : 버전을 기록하는 컬럼을 두고 기 식별된 레코드 버전보다 더 높은 버전을 보유한 레코드를 변경된 것으로 인식.
-   Status on Rows : 타임스탬프 및 버전 넘버 기업에 대한 보완 용도로 활용. 레코드에 대한 변경 여부를 사람이 직접 판단할 수 있도록 유보하는 업무 규칙을 정할 수 있다. 데이터 변경 여부를 True / False 불린 값으로 저장
-   Triggers on Tables : 데이터베이스 트리거를 활용해 사전에 등록된 다수의 대상 시스템에 변경 데이터를 배포하는 형태로 CDC를 구현하는 기법이다.
-   Time / Version / Status on Rows : 타임스탬프, 버전넘버, 상태 값의 세가지 특성을 모두 활용. 정교한 쿼리 생성에 활용할 수 있다.
-   Event Programming : 데이터 변경 식별 기능을 애플리케이션에 구현



#### HBase는 하둡 분산파일 시스템을 사용하며, SQL을 지원하지 않는다.



#### MapReduce 절차 (중요 **)

- Input > Split > Map > Combine > Shuffle&Sort > Reduce > Output

- Mapper 의 중간 output 은 reduce의 input으로 사용

    ![map-reduce-word-chart-process](https://user-images.githubusercontent.com/291782/156508689-850f108f-de71-4437-8d77-b7259f638420.png)

    



#### ODS (Operation Data Store) 개념 및 특징

-   ODS (Operation Data Store)는 데이터에 대한 추가 작업을 위해 다양한 데이터 원천(Source)들로부터 데이터를 추출.통합한 데이터베이스다.
-   ODS 내의 데이터는 향후 비즈니스 지원을 위해 타 정보시스템으로 이관되거나, 다양한 보고서 생성을 위해 데이터 웨어하우스로 이관된다.
-   ODS를 위한 데이터 통합은 일반적으로 데이터 클렌징, 중복제거, 비즈니스 룰 대비 데이터 무결성 점검 등의 작업들을 포함한다.
-   실시간(Real Time) 또는 실시간 근접(Near Real Time) 트랜잭션 데이터 혹은 가격 등의 원자성(개별성)을 지닌 하위 수준 데이터들을 저장하기 위해 설계된다.
-   ODS 구성 단계 : Interface > Staging > Profiling > Cleansing > Integration > Export



#### 데이터 웨어 하우스 특징

- 주제 중심성 : 최종사용자(end user)도 이해하기 쉬운 형태를 지닌다
- 영속성, 비휘발성 : 데이터는 최초 저장 이후에는 읽기 전용 (read only)의 속성을 가지며, 삭제되지 않는다.
- 통합성 : 데이터는 기관. 조직이 보유한 대부분의 운영 시스템들에 의해 생성된 데이터들의 통합본이다
- 시계열성 : 운영 시스템들은 최신 데이터를 보유하고 있지만. 데이터 웨어하우스는 시간 순에 의한 이력 데이터를 보유한다.



#### 맵리듀스(MapReduce) 크기

320MB 파일을 별도 옵션을 지정하지 않고 작업 수행하면 몇개의 맵 태스크(Task)가 생성되는가?

- 맵리듀스 블록 크기 기본값 : 64MB
- 맵태스크 하나가 하나의 블록을 대상으로 연산을 수행
- 320 / 64 = 5



#### 하둡분산파일시스템 (HDFS) 특징

- 데이터의 랜덤 접근 방식을 지원하지 않음. 순차 접근 방식을 지원
- 구글 파일 시스템의 아키텍처와 사상을 그대로 구현한 클로닝(Cloning) 프로젝트
- 파일 데이터를 블록이나 청크 단위로 저장함
- 기본적으로 파일은 한 번 쓰이면 변경되지 않는다고 가정함
- 파일 데이터는 블록(또는 청크) 단위로 나뉘어 여러 데이터노드에 분산.복제.저장된다.
- 낮은 데이터 접근지연시간보다는 높은 데이터 처리량에 중점을 두고 있음
- 하나의 네임도드(NameNode), 다수의 데이터노드(DataNode)로 구성됨
    - 네임노드 : 모든 메타데이터를 관리, 마스터/슬레이브 구조에서 마스터 역할. 클라이언트로 부터 파일 접근 요청을 처리
    - 데이터노드 : 슬레이브 노드, 데이터 입출력 요청을 처리. 데이터 유실 방지를 위해 3중 복제하여 저장.
    - 보조네임노드 : HDFS 상태 모니터링을 보조. 주기적으로 네임 노드의 이미지를 스냅샷해 저장



#### 아파치 하이브(hive) 특징

- 페이스북에서 개발한 데이터 웨어하우징 인프라로 아파치 내의 하둡 서브 프로젝트로 등록돼 개발되고 있다.
- Pig와 마찬가지로 하둡 플랫폼 위에서 동작
- SQL 기반의 쿼리 언어와 JDBC를 지원
- 하둡 병렬처리 기능인 Hadoop-Streaming을 쿼리 내부에 삽입해 사용할 수 있다
- 아파치 하이브는 **맵리듀스의 모든 기능을 지원**



#### SQL on 하둡

- 실시간 SQL 질의 분석 기술이다.
- 하둡 프레임워크의 맵리듀스를 이용하지 않고, 새로운 분산 처리 모델과 프레임워크를 기반으로 구현되어 있다.
- SQL on 하둡의 한 종류인 샤크(Shark)는 인메모리 기반의 대용량 데이터웨어하우스 시스템이다.
- 실시간 SQL 질의 분석 기술이다
- 클라우데라 임팔라, 아파치 타조는 대표적인 상용 SQL on 하둡 솔루션이다.



#### 임팔라 (SQL on Hadoop(하둡)의 질의 엔진)

- 분석과 트랜잭션 처리를 모두 지원하는 것을 목표로 만들어진 SQL 질의 엔진

- 하둡과 HBase에 저장된 데이터를 대상으로 SQL 질의 가능

- 고성능을 위해 C++언어를 사용하였음

- 맵리듀스를 사용하지 않고 실행 중에 최적화된 코드를 생성해 데이터를 처리

- 하이브(Hive)가 하둡에 저장된 다양한 형태의 비정형 데이터를 처리하는 표준 SQL솔루션으로 사용되고 있지만, 더 빠른 처리가 필요한 비즈니스 요구사항 때문에 임팔라와 같은 기술이 대두되고 있다.

    ![impala-arch](https://user-images.githubusercontent.com/291782/168068690-c75b20ee-e8a4-4191-8dc7-09a3c6d0b799.png)





#### 데이터베이스 클러스터

- 하나의 DB를 여러 개의 서버(또는 가상 서버) 상에 구축하는 것을 의미
- DB 파티셔닝은 DB를 여러 부분으로 분할하는 것을 의미하며, 분할된 각 요소는 파티션이라고 한다.
- 각 파티션은 여러 노드로 분할 배치되어 여러 사용자가 각 노드에서 트랜잭션을 수행할 수 있다.
- 데이터를 통합할 때, **성능과 가용성의 향상을 위해 DB 차원의 파티셔닝 또는 클러스터링을 이용**한다.
- Oracle RAC(Real Application Cluster)를 제외한 대부분의 데이터베이스 클러스터가 무공유 방식을 채택하고 있다. Oracle RAC만 공유방식 채택
- MySQL 클러스터 : 클러스터에 참여하는 노드(SQL노드, 데이터 노드, 매니저 포함)수는 255개로 제한. 데이터 노드는 최대 48개 까지만



#### DB 파티셔닝 구현 효과

- 병렬처리, 고가용성, 성능향상



#### NoSQL 개념 및 특징

- 분산 데이터베이스 기술로 확장성, 가용성, 높은 성능을 제공
- 비관계형 데이터베이스 관리 시스템. (Not Only SQL)
- 저장되는 데이터 구종에 따라 key-value 모델, Document 모델(JSON, XML 데이터 구조 채택), Graph 모델, Column 모델로 구분됨
- NoSQL은 key value 형태로 자료를 저장하고 빠르게 조회할 수 있는 구조를 제공
- 스키마 없이 동작, 복잡한 join 연산은 지원하지 않음
- 종류는 구글 BigTable, 아파치 HBase, 아마존 SimpleDB, MS의 SSDS 등

#### Oracle RAC 데이터베이스 서버의 장점

- 가용성 : 높은 수준의 폴트 톨러런스(Fault tolerance)를 제공하므로, 하나의 노드만 살아 있어도 서비스가 가능
- 확장성 : 추가 성능이 필요하면 응용 프로그램이나 테이터베이스를 수정할 필요 없이 새 노드를 클러스터에 쉽게 추가할 수 있다. 10g R2 RAC 기준 최대 100개 노드 지원
- 비용 절감 : 저가형 상용 HW 클러스터에서도 고가의 SMP 시스템 만큼 효율적으로 응용 프로그램을 실행함
- 단, 도입 비용 때문에 확장성이 중요한 데이터보다는 고가용성을 요구하는 데이터에 많이 사용



#### 하둡(hadoop) 에코시스템에 사용되는 기술의 내용과 종류

- 정형 데이터 수집 : Sqoop, hiho
- 비정형 데이터 수집 : Flume-NG, kafka, Chuckwa, Scribe
- 대용량 SQL 질의 기술 : Hive, Pig
- 워크플로우 관리 : Oozie, Azkaban
- 실시간 SQL 질의 기술 : Impla, Tajo

![hadoop-eco](https://user-images.githubusercontent.com/291782/168072330-40a88b9b-d978-40d8-a979-d7d1cc932851.png)





#### 클라우드 컴퓨팅의 개념 및 특징

- 동적으로 확장할 수 있는 가상화 자원들을 인터넷으로 서비스하는 기술을 의미

- IaaS (Infrastructure as a Service), SaaS (Sw as a Service), Paas (Platform as a Service) 3유형으로 나뉨

    > IaaS : 네트워크 장비, 서버, 스토리지 등 IT 인프라 자원을 빌려주는 클라우드 서비스
    >
    > SaaS : 소프트웨어를 웹에서 사용할 수 있게 해주는 서비스
    >
    > Paas : 애플리케이션이나 SW 개발 및 구현 시 필요한 플랫폼을 제공하는 서비스

- 서버 가상화 기술 : VMware, Xen, KVM 등의 서버 가상화 기술은 IaaS에 주로 활용

- 아마존의 EMR (Electric Map Reduce)은 하둡을 온디맨드로 이용할 수 있는 플랫폼 가상화 서비스



#### 하이퍼바이저(Hypervisor) 개념 및 특징 (CPU 가상화)

- 호스트 컴퓨터에서 다수의 운영 체제를 동시에 실행하도록 하기 위한 논리적인 플랫폼을 의미
- 일반적으로 가상머신을 하이퍼바이저라고 할 수 있음



#### CPU 가상화 방식의 분류

- 완전 가상화
    - VMware ESX server, MS Virtual Server 등이 완전 가상화 솔루션
    - 장점
        - CPU, 메모리, 네트워크 장치 등 모든 자원을 하이퍼바이저가 직접 제어.관리
        - 어떠한 운영체제라도 수정하지 않고 설치 가능
    - 단점
        - 하이퍼바이저가 자원을 직접 제어하기 때문에 성능에 영향을 미침
        - 운영중인 게스트 OS에서 할당된 CPU, 메모리 등의 자원에 대한 동적변경이 단일 서버내에서는 어려움
        - 자원 동적변경을 위해서는 VMware의 VMotion 과 같은 솔루션의 도움이 필요
- 반가상화
    - privileged 명령어를 게스트 OS에서 hypercall로 하이퍼바이저에 전달하고, 하이퍼바이저는 hypercall에 대해서 privileged 레벨에 상관없이 HW로 명령을 수행 시킴
    - CPU, 메모리 등의 자원에 대한 동적 변경이 서비스 중단 없이 가능
    - 완전 가상화에 비해 성능이 뛰어남
    - 반가상화는 커널 변경이 필요하고, 완전가상화는 커널 변경이 필요없다



#### 메모리 가상화 : VMware 기법

개념 및 특징

-   메모리 관리를 위해 물리주소(Physical Address)와 가상주소 (Virtual Address)를 사용함
    -   물리주소 : 0부터 시작해 실제 물리적인 메모리 크기까지를 나타냄
    -   가상주소 : 하나의 프로세스가 가리킬 수 있는 최대 크기를 의미하며 32비트 OS에서는 4GB 까지 가능
-   프로그램에서의 주소는 물리적인 주소가 아닌 가상주소 값
-   따라서 가상주소값의 위치(VPN, Virtual Page Number)를 실제 물리적인 주소 값 위치 (MPN, Machine Page Number)로 매핑하는 과정이 피료하며 page table을 이용
-   매핑 연산을 하드웨어적으로 도와주는 것을 TLB (Translation Lookaside Buffer)라고 함
-   VMware 하이퍼바이저의 핵심 모듈은 VMkernel

가상머신 메모리 할당의 문제 해결을 위한 방법

-   memory ballooning : 가상머신 메모리 영역을 빈 값으로 강제로 채워 가상머신 OS가 자체적으로 swapping 하도록 함
-   transparent page sharing : 동일한 내용을 담고 있는 페이지는 물리적인 메모리 영역에 하나만 존재시키고 모든 가상머신이 공유하도록 함
-   memory overcommitment : 위의 두 가지 기법을 이용하여 가능하지만, 심각한 성능저하 때문에 권장하지 않음



#### I/O 가상화

-   가상 이더넷, 공유 이더넷 어댑터, 가상 디스크 어댑터



#### EAI 와 ESB 비교

EAI (Enterprise Application Integration), ESB (Enterprise Service Bus)

-   기능
    -   EAI : 미들웨어(Hub)를 이용하여 비즈니스 로직을 중심으로 Application을 통합, 연계
    -   ESB : 미들웨어(Bus)를 이용하여 서비스 중심으로 시스템을 유기적으로 연계
-   통합관점
    -   EAI : Application
    -   ESB : Process
-   로직연동
    -   EAI : 개별 Application에서 수행
    -   ESB : ESB에서 수행
-   아키텍처
    -   EAI : 단일 접점인 허브시스템을 이용한 중앙집중식 연결구조
    -   ESB : 버스(Bus)형태의 느슨하고 유연한 연결구조 (point to point)



### 3과목 데이터 분석 기획

#### 분석과제 발굴 방법론

-   하향식 접근법
    -   문제탐색 > 문제정의 > 해결방안 탐색 > 타당성 검토로 전개
-   상향식 접근법
    -   답을 미리 내는 것이 아니라 사물을 있는 그대로 인식하는 "What" 관점에서 봄
    -   비지도 학습과 지도학습
    -   프로토타이핑 접근법

 



#### 고객니즈 변화에 대항하는 것 : 고객, 채널, 영향자들에 의해 진행



#### 분석 프로젝트 영역별 주요 관리 항목

-   범위, 시간, 원가, 품질, 통합, 조달, 자원, 리스트, 의사소통, 이해관계자 등



#### 분석과제 관리 프로세스에 대한 설명

-   분석과제 중에 발생된 시사점과 분석 결과물이 풀(pool)로 관리되고 공유
-   확정된 분석과제는 풀(pool)로 관리하지 않는다.



#### 데이터 분석 3가지 조직 구조

![anal-part](https://user-images.githubusercontent.com/291782/161255029-34d00daa-7141-43ba-a159-b2745a087e2f.png)

-   집중구조, 기능구조, 분산구조



#### CRISP-DM 분석 방법론

- 개요 : 1996년 유럽연합의 **주요한 5개의 업체들**의 주도하에  4개 레벨로 구성된 **계층적 프로세스 모델**로 만들어짐

- ![crisp-dm-4level](https://user-images.githubusercontent.com/291782/160156808-1e170816-8be3-4f5e-a5ae-e3b41c2d692c.png)

- 최상위 레벨은 여러 개의 단계(Phase)로 구성되고 각 단계는 일반화 태스크(Generic Tasks)를 포함

- 일반화 태스크는 데이터마이닝의 단일 프로세스를 완전하게 수행하는 단위이며, 이는 다시 구체적인 수행 레벨인 세분화 태스크(Specialized Tasks)로 구성된다.

- 예를 들어 데이터 정체 (Data Clensing)라는 일반화 태스크는 범주형 데이터 정제와 연속형 데이터 정제와 같은 세분화 태스크로 구성

- 마지막 레벨인 프로세스 실행 (Process instances)은 데이터마이닝을 위한 구체적인 실행을 포함한다.

- 6단계 프로세스 : 각 단계는 단방향 구성이 아닌, **단계 간 피드백**을 통해 단계별 완성도를 높이게 되어 있음

    ![crisp-dm-6phase](https://user-images.githubusercontent.com/291782/160157935-b0f7e9cc-c7db-4a58-92f1-aadb6b18b98f.png)

    -   1단계 : 업무이해 : 프로젝트의 목적과 요구사항을 이해하는 단계. 업무 목적 파악, 상황 파악, 목표 설정, 프로젝트 계획 수립
    -   2단계 : 데이터 이해 : 데이터를 수집하고, 데이터 속성을 이해하기 위한 단계. 데이터 수집, 데이터 기술분석, 데이터 품질 확인
    -   3단계 : 데이터 준비 : 분석 기법에 적합한 데이터를 편성하는 단계. 데이터셋 선택, 데이터 정제, 데이터 통합, 데이터 포맷팅
    -   4단계 : 모델링 : 모델링 기법과 알고리즘 선택하고 파라미터를 최적화 하는 단계. 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가
    -   5단계 : 평가 : 모델링 결과가 프로젝트 목적에 부합하는지 평가하는 단계. 분석결과 평가, 모델링 과정 평가, 모델 적용성 평가
    -   6단계 : 전개 : 모델을 실 업무에 적용하기 위한 계획 수립 단계. 전개 계획 수립, 유지보수 계획 수립, 종료보고서 작성 및 프로젝트 리뷰



####  KDD 와 CRISP-DM 의 비교

|                         KDD                          |              CRISP-DM              |
| :--------------------------------------------------: | :--------------------------------: |
|                분석대상 비즈니스 이해                | 업무이해 (Business Understanding)  |
|            데이터셋 선택 (Data Selection)            | 데이터의 이해 (Data Understanding) |
|            데이터 전처리 (Preprocessing)             | 데이터의 이해 (Data Understanding) |
|             데이터 변환 (Transformation)             |   데이터 준비 (Data Preparation)   |
|             데이터 마이닝 (Data Mining)              |         모델링 (Modeling)          |
| 데이터 마이닝 결과 평가 (Interpretation/ Evaluation) |         평가 (Evaluation)          |
|                  데이터 마이닝 활용                  |         전개 (Deployment)          |



#### KDD(Knowledge Discovery in DB) 분석 절차

1. 데이터셋 선택(Selection)
    - 분석 대상의 **비즈니스 도메인에 대한 이해**와 **프로젝트 목표 설정**이 필수
    - 분석에 필요한 데이터를 선택하는 단계
    - **목표 데이터**(target data)를 구성하여 분석에 활용
2. 데이터 전처리 (preprocessing)
    - **잡음(noise), 이상치(outlier), 결측치(missing value)를 식별**하거나 **제거**하여 데이터셋을 정제하는 단계
    - **추가로 요구되는 데이터 셋**이 필요한 경우 선택 프로세스를 재실행 함
3. 데이터 변환 (transformation)
    - 분석 목적에 맞게 변수를 생성, 선택하고 **데이터의 차원을 축소**하여 효율적으로 데이터마이닝을 할 수 있도록 변경하는 단계
    - **학습용 데이터**(training data)와 **검증용 데이터**(test data)로 분리하는 단계
4. 데이터 마이닝 (data mining)
    - 학습용 데이터를 이용하여 **데이터 마이닝 기법을 선택**하고, 적절한 알고리즘을 적용하는 단계
    - 필요에 따라 **전처리**와 **변환** 단계를 **추가로 실행**하여 최적의 결과를 산출



#### 분석과제 관리를 위한 5가지 주요 영역

-   Data Size : **분석하고자 하는 데이터의 양**을 고려한 관리 방안 수립이 필요. 하둡 환경에서의 엄청난 데이터를 분석하는 것과, 기존 정형 데이터베이스에 있는 시간 당 생성되는 데이터를 분석할 떄의 관리 방식은 차이가 날 수 밖에 없다.
-   Data Complexity : 정형 데이터 분석과 달리, 텍스트, 오디오, 비디오 등의 비정형 데이터 분석 프로젝트를 진행 할때는 초기 데이터의 확보와 통합뿐 아니라 해당 데이터에 **잘 적용될 수 있는 분석 모델의 선정** 등에 대한 사전 고려가 필요
-   Speed : 분석 결과가 도출되었을 때 이를 활용하는 **시나리오 측면에서의 속도**를 고려해야 한다. 사기(Fraud) 및 고객에게 개인화된 상품 서비스를 추천하는 경우에는 분석 모델의 적용 및 계산이 실시간으로 수행되어야 하기 때문에 프로젝트 수행 시 **분석 모델의 성능 및 속도를 고려한 개발** 및 테스트가 수행되어야 한다.
-   Analytic Complexity : 분석 모델의 정확도와 복잡도는 트레이드 오프 관계가 존재한다. **해석이 가능하면서도 정확도를 올릴 수 있는 최적모델**을 찾는 방안을 사전에 모색해야 한다.
-   Accuracy & Precision : **Accuracy**(정확)는 모델과 실제 값 사이의 차이가 적다는 **정확도**를 의미하고, **Precision**(정확, 정밀)은 모델을 지속적으로 반복했을 때의 편차의 수준으로써 **일관적**으로 동일한 결과를 제시한다는 것을 의미한다. 분석의 활용 측면에서는 Accuracy가 중요, 안정성 측면에서는 Precision이 중요하다.둘은 트레이드 오프 관계라 해석 및 적용 시 사전에 고려해야 함.

![accuracy-precision](https://user-images.githubusercontent.com/291782/160412734-09e83402-8824-470b-811d-b97e7f62f477.png)

-   위 이미지로 보면 accuracy 가 편향(bias)이고, precision이 분산(variance)이다.

#### 거버넌스 체계의 구성요소

-   분석기획 및 관리 수행 조직 (Organization)
-   과제 기획 및 운영 프로세스 (Process)
-   분석관련 시스템 (System)
-   데이터 (Data)
-   분석교육 / 마인드 육성체계 (Human Resource)



#### 데이터 거버넌스 구성요소

-   원칙(Principle) : 데이터 유지, 관리하기 위한 지침 가이드, 보안, 품질기준
-   조직(Organization) : 데이터 관리할 조직의 역할과 책임. 데이터 관리자, DB 관리자, DA
-   프로세스(Process) : 데이터 관리를 위한 활동과 체계



#### 데이터 거버넌스 체계

-   데이터 표준화 : 데이터 표준 용어 설정, 명명규칙, 메타 데이터, 데이터 사전 구축 등 업무
-   데이터 관리 체계 : 정합성 및 활용의 효율성, 메타 데이터와 데이터 사전의 관리 원칙을 수립. 항목별 상세한 프로세스 만들기. 데이터 생명주기 관리
-   데이터 저장소 관리(repository) : 메타 데이터 및 데이터를 관리하기 위한 전사 차원의 저장소를 구성. 워크플로우 및 관리용 응용 소프트웨어(application)를 지원
-   표준화 활동 : 데이터 거버넌스 체계 구축 후 표준 준수 여부를 주기적으로 모니터링. 계속적인 변화관리 및 주기적인 교육 진행. 지속적인 데이터 표준화 개선 활동







#### 데이터 분석 수준 진단

![analysis-quardrant](https://user-images.githubusercontent.com/291782/160861368-444b90ba-c7a6-4477-bb12-c2d946373ce7.png)



#### 분석 과제 발굴 방법

![topdown-bottomup](https://user-images.githubusercontent.com/291782/160286214-b1aa8be8-4dc3-4f86-9e24-1df4d492593c.png)



#### 비즈니스 모델 캔버스를 활용한 과제 발굴 방법 5가지

-   업무 (Operation) : 내부 프로세스 및 주요자원 (Resource)관련 주제 도출 (예. 생상 공정 최적화, 재고량 최소화)
-   제품 (Product) : 제품, 서비스를 개선하기 위한 관련 주제 도출 (예. 제품의 주요기능 개선, 서비스 모니터링 지표 도출)
-   고객 (Customer) : 제공하는 채널의 관점에서 관련 주제 도출 (예. 고객 Call 대기 시간 최소화, 영업점 위치 최적화)
-   규제와 감사 (Regualtion & Audit) : 규제 및 보안의 관점에서 주제 도출 (예. 제공 서비스 품질의 이상 징후 관리, 새로운 환경 규제 시 예상 되는 제품 추출 등)
-   지원 인프라 (IT & Human Resources) : 시스템 영역 및 이를 운영 관리하는 **인력의 관점**에서 주제 도출 (예. EDW 최적화, 적정 운영 인력 도출 등)



#### 데이터에 기반한 의사결정 방해 요소

- 고정 관념, 편향된 생각, 프레이밍 효과



#### 목표시점별 분석기 기획방안

- 과제 중심적인 접근 방식 : 당면한 과제를 빠르게 해결

- 장기적인 마스터 플랜 방식 : 지속적인 분석 내재화를 위함

- |             | 당면한 분석 주제의 해결(과제 단위) | 지속적 분석 문화 내재화(마스터 플랜) |
    | ----------- | ---------------------------------- | ------------------------------------ |
    | 1차 목표    | Speed & Test                       | Accuracy & Deploy                    |
    | 과제의 유형 | Quick & Win                        | Long Term View                       |
    | 접근 방식   | Problem Solving                    | Problem Definition                   |



#### ROI 관점에서 빅데이터의 핵심 특징

- 3V (Volume, Variety, Velocity) 난이도 : 데이터 규모/양, 데이터 종류/유형, 데이터 생성속도/처리속도 => 투자비용 요소 (Investment)

- 4V (3V + Value(가치)) 시급성 : 분석 결과 활용 및 실행을 통한 비즈니스 가치 => 비즈니스 효과 (Return)

- 시급성 : 시급성의 판단 기준은 전략적 중요도가 핵심

- 포트폴리오 사분면 분석을 통한 과제 우선순위 선정

    ![pflo_4_layer](https://user-images.githubusercontent.com/291782/160415872-abf3a83b-f41d-42ae-9a34-b4f0d357843d.png)

    -   분석과제의 우선순위를 '**시급성**'에 둔다면 3 > 4 > 2 영역 순, '**난이도**'에 둔다면 3 > 1 > 2 영역순으로 의사결정



#### 분석기회발굴의 범위확장

-   거시적 관점의 메가 트랜드 : Social(사회), Technological(기술), Economic(경제), Environment(환경), Political(정치)
-   경쟁자 확대 관점 : 대체재(substitute), 경쟁자(competitor), 신규진입자(New entrant)
-   시장의 니즈 탐색 관점 : 고객(customer), 채널(channel), 영향자(Influencer)
-   역량의 재해석 관점 : 내부 역량, 파트너와 네트워크



### 4과목 데이터 분석

#### 34. 모분산의 추론에 대한 설명

-   F-분포 : 이표본에 의한 분산비 검정은 두 표본의 **분산이 동일한지를 비교**하는 검정
-   모분산이 추론의 대상이 되는 경우는 모집단의 변동성 또는 퍼짐의 정도에 관심이 있을 때이다.
-   모집단이 정규분포를 따르지 않더라도 중심극한 정리를 통해 정규 모집단으로부터의 모분산에 대한 검정을 유사하게 시행할 수 있다.
-   X<sup>2</sup>(카이제곱) : 평균모집단에서 n개를 단순임의 추출한 **표본의 분산**은 자유도가 n-1인 카이제곱 분포를 따른다. **두 집단간의 동질성 검정에 활용**
-   t-분포 : **두 집단의 평균이 동일**한지를 알고자 할 때 검정통계량으로 활용



#### 이산형 확률분포, 연속형 확률분포

가. 이산형 확률분포

- 확률 변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포, 확률값은 확률질량함수를 이용하여 계산

> $P(X_i) > 0$      $  i=1,2,...,k$      $\displaystyle \sum_{i=1}^kP(X_i)=1$

- 이산형 확류변수 예시 : 동전 2개를 던져서 앞/뒷면이 나오는 경우의 수(H:앞, T:뒤)

    | 표본공간(&Omega;) | HH(사건) | HT   | TH   | TT   | 합계 |
    | ----------------- | -------- | ---- | ---- | ---- | ---- |
    | P(x)              | 1/4      | 1/4  | 1/4  | 1/4  | 1    |

    

1) 베르누이  확률분포(Bernoulli distribution)

- 결과가 2개만 나오는 경우 (ex. 동전 던지기, 시험의 합/불합격 등)

> $P(X=x)=p^x(1-p)^{1-x}$  (x=1 or 0), E(x) = p,  var(x)=p(1-p)
>
> 예) 메이저리거인 추추가 안타를 칠 확률은 베르누이 분포를 따름. (안타를 치는 사건을 x=1이라고 할 때 안타를 칠 확률은 타율로 적용 가능)



2) 이항분포(Binomial distribution)

- 베르누이 시행을 n번 반복했을 때 k번 성공할 확률
- $P(X = k) = _nC_kP^k(1-p)^{n-k}, \quad  _nC_k = \dfrac {n!} {k!(n-k)!}$
- 한 축구 선수가 페널티킥을 차면 5번 중 4번은 성공한다고 한다. 그럼 이 선수가 10번의 페널티킥을 차서 7번 성공할 확률은?
- 5번 중 4번 성공하기에 성공확률은 4/5 = 0.8, 실패확률은 1-0.8 = 0.2
- $이항분포 = \begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} =  \begin{pmatrix} 10 \\ 7 \end{pmatrix} 0.8^70.2^3$
- $ = \dfrac {10!} {7! \times 3!} 0.8^7 0.2^3 = 0.2013 = 20.13\%$



3) 기하분포(Geometric distribution)

- 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- $p(x) = p(1-p)^{x-1}$
- 어느 야구선수가 홈런을 칠 확률은 0.05라고한다. 이 선수가 6번째 타석에서 홈런을 칠 확률은?
- 성공확률 p=0.05, 실패확률은 1 - 0.05 = 0.95, 6번째 타석에서 성공할 확률이기에 x-1 = 6-1
- $p(1 - p)^{x-1} = 0.05 \times 0.95^{6-1} = 0.0387 = 3.87\%$



4) 다항분포(Multinomial distribution)

- 이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포

- 각 상황의 확률과 각 상황의 횟수를 잘 파악해야 함

- $p(x) = \dfrac {n!} {x_1!x_2! ...x_k!}p1^{x_1}p_2^{x_2}... p_k^{x_k} $

- 국내 인터넷 포털 사이트의 점유율은 아래와 같다.  12명을 임의로 뽑아 사용 사이트를 알아보았을 때, 네이버 7명, 구글 3명, 다음과 ZUM이 각 1명, 기타는 0명이 사용할 확률을 구하시오

    네이버 : 61%, 구글 : 30%, 다음 : 7%, ZUM: 1%, 기타 : 1%

- $\dfrac {12!} {7! \times 3! \times 1! \times 1! \times 0!} \times 0.61^7 \times 0.3^3 \times 0.07^1 \times 0.01^1 \times 0.01^0 = 0.0094$

    ​        

    ​        

5) 포아송분포 (Poisson distribution)

- 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 (예. 가게에 손님이 1시간에 20명씩 방문한다고 할 때, 10분에 손님이 5명씩 방문할 확률)
- 확률을 구하기 위해서는 **평균(&lambda;)과 발생횟수(x)**를 잘 파악해야 함
- $p(x) = \dfrac {e^{-\lambda} \lambda^x} {x!} , \quad e=2.718281...$
- 전공 책 5페이지를 검사했는데, 오타가 총 10개가 발견되었다고 한다. 그럼 이 책에서 어느 한 페이지를 검사하였을 때, 오타가 3개 나올 확률을 구하시오?
- 해설) 포아송 분포는 평균을 잘 구해야함. 문제에 말장난이 섞여 있음. 일단 5페이지에 총 10개의 오타이므로, 1페이지에 평균 2개의 오타가 발견된 셈. 그래서 평균 (&lambda;)=2 이다. 그리고 발생횟수 x=3 이므로...
- $\dfrac {2.718281^{-2} \times 2^3} {3!} = 0.1804$



##### 나. 연속형 확률분포

- 확률 변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포이며, 확률값은 확률밀도함수를 이용하여 계산한다.

1) 균일분포 (일양분포, Uniform distribution)

- 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)



2) 정규분포 (Normal distribution)

- 평균이 &mu;이고, 표준편차가 &sigma;인 X의 확률밀도함수
- 표준편차가 클 경우 퍼져보이는 그래프가 나타난다.



3) 지수분포 (Exponential distribution)

- 어떤 사건이 발생할 때까지 경과한 시간에 대한 연속확률분포이다. (예. 전자렌지의 수명시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간)



4) t-분포 (t-distiribution)

- 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
- 표본의 크기가 적을때는 표준정규분포를 위에서 눌러 놓은 것과 같은 형태를 보이지만 표본이 커져서(30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
- 데이터가 연속형일 경우 활용한다.
- **두 집단의 평균이 동일**한지 알고자 할 때 검정통계량으로 활용된다.
- 표준정규분포와 같이 평균 값이 0이며, 자유도에 따라 분포의 모양이 변화한다.
- 자유도가 30미만인 경우, 표준정규분포에 비해 양쪽 끝이 평평하고 두터운 꼬리 모양을 가진다.



5) X<sup>2</sup>-분포 (X<sup>2</sup>-distribution) (카이제곱분포)

- 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포이다.
- **두 집단 간의 동질성 검정에 활용**된다.
- 확률변수 X가 표준정규분포(Z)를 따를 때, 자유도가 k인 카이제곱분포를 따른다. 자유도는 표본 자료 중 모집단에 대한 정보를 주는 독립적인 표본 자료의 수와 같으며, 분할표에서의 행과 열의 개수를 통해 구할 수 있다. (자유도(df) = (r-1)(c-1), r=행의 개수, c=열의 개수)



6) F-분포 (F-distribution)

- **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량의 분포이다.
- 확률변수는 항상 양의 값만을 갖고 카이제곱분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.



#### R에서 사용 가능한 데이터 오브젝트

-   벡터 : 모든 원소는 같은 모드여야 한다.
-   리스트 : 원소들은 다른 모드여도 상관 없다
-   행렬 : 차원을 가진 벡터
-   데이터프레임 : 테이블로 된 데이터 구조로써 리스트 구조로 구현된다.



#### 벡터 및 sequence 표현 방법

```R
> c(1, 10)
[1]  1 10

> c(1:10)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10, 1)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10, 2)
[1] 1 3 5 7 9

> 1:10
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(10, 100, 10)/10
 [1]  1  2  3  4  5  6  7  8  9 10
```



#### 종속변수를 설명하는 가장 중요한 독립변수

-   추정한 계수가 클수록 종속변수에 가장 많은 영향을 미친다.
-   특히 &beta;0 가 없는 표준화된 추정식을 만들게 되면 각 계수의 크기를 더욱 정확히 알 수 있게 된다.



#### 확률분포 종류

-   이산형 확률변수 : 베르누이 확률분포, 이항분포, 기하분포, 다항분포, 포아송분포
-   연속형 확률변수 : 균일분포(일양분포, Uniform distribution), 정규분포, 지수분포, t-분포, X2 분포, F-분포

1. 이산형 확률변수

    -   베르누이 확률분포 (Bernoulli distribution)

        -   결과가 2개만 나오는 경우 **성공 또는 실패** (예. 동전 던기지, 시험의 합격/불합격 등)
        -   $P(X = x) = P^x . (1-p)$<sup>1-x</sup> 
        -   (x= 1 or 0), 기댓값: $E(x) = p$, 분산 :$var(x) = p(1-p)$
        -   예) 추신수가 안타를 칠 확률은 베르누이 분포를 따른다.
    -   이항분포 (Binomial distribution)

        -   베르누이 시행을 n번 반복했을 때 k번 성공할 확률
        -   n번 시행 중에 각 시행의 확률이 p일 때, k번 성공할 확률분포
        -   $P(X = k) = _nC_kP^k(1-p)$<sup>n-k</sup> , $_nC_k = \dfrac {n!}{k!(n-k)!}$
        -   기댓값 : $E(X) = np$, 분산 : $V(X) = np(1-p) $  (단, n과 k가 1이면 베르누이 시행)
        -   추신수가 오늘 경기에서 5번 타석에 들어와서 3번 안타를 칠 확률은 이항분포를 따른다. (n=5, k=3, 안타를 칠 확률 P(x) = 타율로 적용 가능)
        -   성공할 확률 p가 0이나 1에 가깝지 않고 n이 충분히 크면 이항분포는 정규분포에 가까워 진다. 성공할 확률 p가 1/2에 가까우면 종모양이 된다.
    -   기하분포 (Geometric distribution)
        -   성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 X번 실패할 확률
        -   예) 추신수가 오늘 경기에서 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률은 기하분포를 따른다.
    -   다항분포 (Multinomial distribution)
        -   이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포
    -   포아송분포 (Poisson distiribution)
        -   시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
        -   예) 책에 오타가 5page 당 10개씩 나온다고 할 떄, 한 페이지에 오타가 3개 나올 확률, 추신수가 최근 5경기에서 홈런을 쳤을 경우, 오늘 경기에서 홈런을 못 칠 확률은 포아송 분포
        -   &lambda; (람다) = 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값, y= 사건이 일어난 수
        -   $P = \dfrac {\lambda^ne^{-\lambda}} {n!}$ (e는 자연상수)
        -   기댓값 : $E(X) = \lambda$, 분산 : $V(X) = \lambda $

2. 연속형 확률변수

    - 가능한 값이 실수의 어느 특정구간 전체에 해당하는 확률변수 (확률밀도함수)

    - $ f(x)\ge 0 $     $\int_{-\infty}^{\infty}f(x)dx = 1$

    - 균일분포 (일양분포, Uniform distiribution)

        - 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
        - $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
        - ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)

    - 정규분포 (Normal distribution)

        - 평균이 &mu; (뮤) 이고, 표준편차가 &sigma; (시그마) 인 X의 확률밀도 함수
        - 표준편차가 클 경우 퍼져보이는 그래프가 나타남
        - 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
        - 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
        - ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)

    - 지수분포 (Exponential distribution)

        - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포이다.
        - 예) 전자렌지의 수명시간, 콜센터에 전화가 걸려올때 까지의 시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간
        - ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)

    - t분포 (t-distribution)

        - 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
        - 표본이 커져서 (30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
        - 데이터가 연속형일 경우 활용한다.
        - **두 집단의 평균이 동일한지** 알고자 할 때 검정통계량으로 활용된다.
        - ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)

    - X<sup>2</sup>-분포 (chi-square distribution, 카이제곱분포)

        - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포
        - **두 집단 간의 동질성 검정에 활용**된다. (범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용)
        - ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)

    - F-분포 (F-distribution)

        - **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량 분포
        - 확률변수는 항상 양의 값만을 갖고 X<sup>2</sup> 분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
        - ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)

        

#### 분해시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

    > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
    >
    > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
    >
    > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
    >
    > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인





#### 데이터마이닝의 분석방법

- 지도학습 (Supervisied Learning)
    - 의사결정나무 (DT), 인공신경망 (Artifician Neural Network), 일반화 선형 모형 (GLM, Generalized Linear Model)
    - 회귀분석 (regression analysis), 로지스틱 회귀분석 (logistic regression analysis), 사례기반 추론 (case-based reasoning), 최근접이웃법 (KNN)
- 비지도학습 (Unsupervised Learning)
    - OLAP (On-Line Analytical Processing), 연관성 규칙발견 (Association Rule Discovery, Market Basket)
    - 군집분석 (K-Means Clustering), SOM (Slef Organizing Map)



#### 정규화 선형회귀 (Regularized Linear Regression)

-   선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상(과적합, Overfitting)을 막는 방법
-   **계수의 크기를 제한**하는 방법으로 제약조건을 추가한다.
-   제약 조건의 종류에 따라 Ridge회귀, LASSO회귀, Elastic Net 회귀모형
    1.   릿지회귀 (Ridge Regression)
         -   가중치들의 제곱합(Squared Sum of weight)을 최소화하는 것을 제약조건으로 추가하는 기법
         -   가중치의 모든 원소가 0에 가까워지는 것을 원하며, 규제 방식을 L2 규제(Penalty)라고 한다.
         -   &lambda;는 제약조건의 비중을 조절하기 위한 하이퍼 모수(Hyper parameter)에 해당하며, 람다가 커지면 가중치의 값들이 작아지며, 정규화 정도가 커진다. 람다가 작아지면 정규화 정도가 작아지고, 람다가 0이 되면 일반적인 선형회귀 모형이 된다.
    2.   라쏘회귀 (LASSO Regression)
         -   라쏘 (Least Absolute Shrinkage and Selection Operator)는 가중치 절대값의 합을 최소화하는 것을 제약조건으로 추가하는 기법
         -   가중치가 0에 가까워질 뿐, 실제 0이 되지는 않는다. 하지만 중요하지 않은 가중치는 0이 될 수도 있다.
         -   라쏘에서 사용하는 규제방식을 L1 규제라고 한다.
    3.   엘라스틱넷(Elastic Net)
         -   릿지와 라쏘를 결합한 모델
         -   &lambda;<sub>1</sub> 와 &lambda;<sub>2</sub> 두 개의 하이퍼 모수를 가짐



#### 선형회귀분석의 가정

- **선형성** : 입력변수와 출력변수의 관계가 선형이다
- 등분산성 : 오차의 분산이 입력변수와 무관하게 일정하다.
- 정상성 (정규성) : 오차의 분포가 정규분포를 따른다. **Q-Q plot**, Kolmogolov-Smirnov 검정, **Shaprio-Wilk** 검정 등을 활용하여 정규성을 확인
- 독립성 : 입력변수와 오차는 관련이 없다. 독립성을 알아보기 위해 **Dubrin-Watson** 통계량 사용. 주로 시계열 데이터에서 많이 활용
- 비상관성 : 오차들끼리 상관이 없다.



#### 그래프를 활용한 선형회귀분석의 가정 검토

| 선형성                                                       | 등분산성                                                     | 정규성                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![linear](https://user-images.githubusercontent.com/291782/162400779-89b3cc2b-e948-4f2e-a217-8251396f5225.png) | ![residuals](https://user-images.githubusercontent.com/291782/162400366-c11a9068-d55f-4f77-aebb-3432abe8678f.png) | ![normal-qqplot](https://user-images.githubusercontent.com/291782/162400224-35785847-68a1-4025-8ac9-8b9dca8fc9f0.png) |



#### 다중선형회귀분석 (다변량회귀분석)

-   다중회귀식 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$

-   모형의 통계적 유의성

    -   **모형의 통계적 유의성은 F-통계량**으로 확인

    -   유의수준 5% 하에서 **F-통계량의 p-value가 0.05보다 작으면 통계적으로 유의**함

    -   F-통계량이 크면 p-value가 0.05보다 작아지고 이렇게 되면 귀무가설을 기각한다.

        >   귀무가설 : $H_0: \beta_1 = \beta_2 = ... \beta_k = 0$ vs 대립가설 : $H_1 : \beta_1 \neq \beta_2 \neq ... \neq \beta_k \neq 0$

    -   **모형의 설명력은 결정계수**(R<sup>2</sup>)나 수정결정계수(R<sub>a</sub><sup>2</sup>)를 확인

    -   모형의 적합성 : 잔차와 종속변수의 산점도로 확인

    -   데이터가 전제하는 가정을 만족하는가? **선형성, 독립성, 등분산성,** 비상관성, **정상성**

    -   다중공선성 (multicollinearity)

        -   다중회귀분석에서 설명변수들 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란
        -   다중공선성 검사방법
            -   분산팽창요인 (VIF) : 4보다 크면 다중공선성이 존재, 10보다 크면 심각한 문제가 있는것으로 해석
            -   상태지수 : 10이상이면 문제가 있음, 30보다 크면 심각한 문제가 있음



#### 회귀분석의 종류

-   단순회귀 : $Y = \beta_0 + \beta_1X + \epsilon$ : 독립변수가 1개이며 종속변수와의 관계가 직선
-   다중회귀 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon $ : 독립변수가 k개 이며 종속변수와의 관계가 선형 (1차함수)
-   로지스틱회귀 : $P(y) = \dfrac {1} {1 + exp[-(\beta_0 + \beta_1X_1 + ... + \beta_kX_k + \epsilon)]}$  : 종속변수가 범주형(2진변수)인 경우에 적용되며, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장될 수 있음
-   다항회귀 : K=2이고 2차 함수인 경우



#### 로지스틱 회귀분석

- 반응변수가 범주형인 경우 적용되는 회귀분석모형

- 신규 설명변수 추정 및 기준치에 따라 분류하는 목적(분류모형)으로 활용

- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)라고 함

    > 오즈비(odds ratio) : 오즈(odds)는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 확률
    >
    > ex) 16강에 한국과 브라질이 진출을 성공/실패할 확률과 각각의 오즈와 오즈비는 아래와 같음
    >
    > |  구분  | 16강 성공확률 | 16강 실패확률 |
    > | :----: | :-----------: | :-----------: |
    > | 브라질 |      0.8      |      0.2      |
    > |  한국  |      0.1      |      0.9      |
    >
    > odds (브라질) : $\dfrac {0.8} {1 - (0.8)} = \dfrac {0.8} {0.2} = 4$
    >
    > odds (한국) : $\dfrac {0.1} {1-0.1} = \dfrac {1} {9}$
    >
    > Odds ratio : $\dfrac {odds(브라질)} {odds(한국)} = \dfrac {4}{\dfrac {1}{9}} = 36$
    >
    > 오즈비가 36 이 나타나 브라질이 16강에 진출할 확률이 한국의 16강 진출 확률보다 36배 높다고 볼 수 있다.

- 선형회귀분석과 로지스틱 회귀분석 비교

    |    목적     |  선형회귀분석  |         로지스틱 회귀분석          |
    | :---------: | :------------: | :--------------------------------: |
    |  종속변수   |  연속형 변수   |               (0, 1)               |
    | 계수 추정법 |   최소제곱법   |           최대우도추정법           |
    |  모형 검정  | F-검정, T-검정 | 카이제곱 검정 (X<sup>2</sup>-test) |

    > 최대우도추정법 (MLE : Maximum Likelihood Estimation) : 모수가 미지의 &theta; (theta)인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 &theta;를 추정하는 기법

- glm() 함수를 활용하여 로지스틱 회귀분석 실행

- R코드 : glm(종속변수 ~ 독립변수1 +...+ 독립변수k, family=binomial, data=데이터셋명)



#### 다중회귀분석의 모형의 설명(적절함)

- F-검정 통계량과 유의확률

- t-통계량과 유의확류

- R<sup>2</sup> (결정계수) 값 검정

    >   r(상관계수)는 회귀분석 이전의 단계에서 실행하여 설명력 확인



#### 회귀분석의 영향력 진단

-   영향력 진단이란 **적합된 회귀모형의 안전성을 평가하는 통계적인 방법**
-   선형회귀분석에서 **회귀직선의 기울기에 영향을 크게 주는 점을 영향점**이라고 함
-   영향력 진단 방법에는 Cook's distance, DFBETAS, DFFITS, Leverage H 등이 있다.
    -   Cook's distance : 쿡의 거리가 **기준값인 1보다 클 경우 영향치**로 간주
    -   DFBETAS : 값이 크지면 영향치 또는 이상치일 가능성 높음. **기준값은 2나 sqrt(n)**(표본을 고려한 경우), DFBETAS 값이 **기준값보다 클 경우 영향치**일 가능성이 높다.
    -   DFFITS : **기준값인 $2\sqrt{((p+1)/n)}$ 보다 클수록 영향치**일 가능성이 높다
    -   Leverage H : 관측치가 다른 관측치 집단으로부터 떨어진 정도를 의미하며, **2 x (p+1)/n** 보다 크면 영향치 이거나 이상치라고 본다.



#### 의사결정나무의 특징

의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형**으로 분류나무와 회귀나무 모형이 있다.

- 장점
    - 결과를 누구에게나 설명하기 용이
    - 만드는 방법이 계산적으로 복잡하지 않음
    - 대용량 데이터에서도 빠르게 만들 수 있음
    - 비정상 잡음 데이터에 대해서도 민감함 없이 분류 가능
    - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않음
    - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다.
    - 모형 분류 정확도가 높다.
- 단점
    - 새로운 자료에 대한 과적합 발생할 가능성이 높다.
    - 분류 경계선 부근의 자료값에 대해서 오차가 크다.
    - 설명변수 간의 중요도를 판단하기 쉽지 않다.
- **교호작용** 효과의 파악 : 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
- 알고리즘
    - CART (Classification and Regression Tree) : 불순도의 측도 지니지수, 이진분리
    - C4.5와 C5.0 : CART와 다르게 다지분리 가능. 불순도 측도 엔트로피 지수
    - CHAID(CHi-Squared Automatic Interaction Detection) : 가지치기 않함. 적당한 크기에서 나무 성장을 중지. 입력변수는 반드시 범주형. 불순도 측도 카이제곱 통계량 사용



#### 비모수 검정

모딥단의 모수에 대한 검정은 모수적 검정과 비모수적 검정으로 구분한다.

-   모수적 검정
    -   모집단의 **분포에 대한 가정을 하고**, 그 가정하에서 검정통계량과 **검정통계량의 분포를 유도해 검정을 실시**하는 방법

-   비모수적 검정
    -   자료가 추출된 **모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시**하는 방법
    -   관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우에 이용
    -   관측된 **자료의 수가 많지 않거나** (30개 미만), 자료가 개체간의 **서열관계를 나타내는 경우**에 이용
-   모수적 검정과 비모수적검정의 차이점
    -   가설의 설정
        -   모수적 검정 : 가정된 **모수의 분포에 대해 가설을 설정**
        -   비모수적 검정 : 가정된 분포가 없으므로 가설은 단지 '분포의 형태가 동일하다' 또는 '분포의 형태가 동일하지 않다'와 같이 **분포의 형태에 대해 설정**한다.
    -   검정 방법
        -   모수적 검정 : 관측된 자료를 이용해 구한 **표본평균, 표본분산** 등을 이용해 검정을 실시
        -   비모수적 검정 : 절대적인 크기에 의존하지 않고 **관측값들의 순위**(rank)나 **두 관측값 차이의 부호** 등을 이용해 검정
-   비모수검정의 예
    -   부호검정 (sign test), 윌콕슨의 순위합검정 (rank sum test), 윌콕슨의 부호순위합검정 (Wilcoxon signed rank test), 만-위트니의 U 검정, 런검정 (run test), 스피어만의 순위상관계수



#### TermDocumentMatrix 에서 sparsity(희박성)의 구하는 법

| Term   | 1    | 2    | 3    | 4    | 5    |
| ------ | ---- | ---- | ---- | ---- | ---- |
| 사과   | 0    | 0    | 1    | 0    | 1    |
| 바나나 | 0    | 1    | 0    | 0    | 0    |

-   sparsity(희박성)은 매트릭스 안에 0인 원소가 있는 %를 의미
-   sparsity = 7/10 = 70%



#### SOM (Self Organizing Map)

가. 개요

- 자기조직화지도 (SOM) 알고리즘은 코호넨 (Kohonen)에 의해 제시, 개발되었으며 코호넨 맵(kohonen maps)이라고도 알려져 있다.

- **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬**하여 지도의 형태로 형상화 한다.

    <img width="466" alt="image" src="https://user-images.githubusercontent.com/291782/163676024-410958e8-d1fa-46b2-abe2-26a2e42e6fb6.png">



나. 구성

- SOM 모델은 위 그림과 같이 두 개의 인공신경망 층으로 구성되어 있다.
    1. 입력층 (input layer: 입력벡터를 받는 층)
        - **입력 변수의 개수와 동일하게 뉴런 수가 존재**한다.
        - 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도 (map)라 부른다.
        - 입력층에 있는 뉴런은 경쟁층에 있는 뉴런들과 연결되어 있으며, 이때 완전연결 (fully connected)되어 있다.
    2. 경쟁층 (competitive layer : 2차원 격자(grid)로 구성된 층)
        - 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
        - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하며 학습한다.
        - 입력 층의 표본 벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best Matching Unit)라고 하며, **코호넨의 승자 독점의 학습 규칙**에 따라 위상학적 이웃 (topological neighbors)에 대한 연결 강도를 조정한다.
    3. BMU (Best-Matching Unit) : 표본 벡터와 거리가 가장 가까운 프로토타입 벡터를 선택하는데, 선택된 벡터를 나타내는 용어가 BMU



다. 특징

- 고차원의 데이터를 저차원의 **지도 형태로 형상화**하기 때문에 시각적으로 이해가 쉽다.
- 실제 데이터가 유사하면 지도상에서 가깝게 표현된다. 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보인다.
- 역전파 (Back Propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 **하나의 전방 패스 (feed-forward flow)를 사용함으로써 속도가 매우 빠른다**. 실시간 학습처리를 할 수 있는 모형이다.



라. SOM과 신경망 모형의 차이점

|         구분          |          신경망 모형          |                  SOM                  |
| :-------------------: | :---------------------------: | :-----------------------------------: |
|       학습방법        |         오차역전파법          |             경쟁학습방법              |
|         구성          |    입력층, 은닉층, 출력층     | 입력층, 2차원 격자(grid)형태의 경쟁층 |
| 기계 학습 방법의 분류 | 지도학습(Supervised learning) |   비지도학습(Unsupervised learning)   |





#### 사회연결망분석의 중심성

- 연결정도 중심성 (Degree centrality) : 한 점에 직접적으로 연결된 점들의 합
    - 인디그리중심성(In-Degree) : 한 점이 다른점으로부터 화살표를 받는 관계의 정도(영향을 받는 관계)
    - 아웃디그리중심성(Out-Degree) : 어떤점이 다른점에 화살표를 주는 정도 (영향을 주는 관계)
- 근접 중심성 (Closeness centrality) : 한 노드로부터 다른 노드에 도달하기까지 필요한 최소 단계의 합
- 매개 중심성 (Betweenness centrality) : 네트워크 내에서 한 점이 담당하는 매개자 혹은 중재자 역할의 정도
- 위세 중심성 (Eigenvector centrality) : 보나시치(Bonacich) 권련지수 : 위에 중심성의 일반적인 형태로, 연결된 노드의 중요성에 가중치를 둬 노드의 중심성을 측정하는 방법





#### 표본추출방법

1. 단순랜덤 추출법 (simple random sampling)

    -   각 샘플에 번호를 부여하여 n개를 추출하는 방법으로 각 샘플은 선택될 확률이 동일하다. (복원, 비복원 추출)

2. 계통추출법 (systematic sampling)

    -   단순랜덤 추출법의 변형된 방식으로 샘플을 나열하여 K개씩 n개의 구간으로 나누고 첫 구간에서 하나를 임의로 선택한 후에 K개식 띄어서 n 개의 표본을 선택
    -   ![systematic-sampling](https://user-images.githubusercontent.com/291782/161560760-0d60d365-a300-4262-8b09-5e9d64125e21.png)

3. 집락추출법 (cluster random sampling)

    -   군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링하는 방법
    -   ![cluster-random-sampling](https://user-images.githubusercontent.com/291782/161560900-294b7296-b204-41f2-9170-0f60ae4d9fc4.png)

4. 층화추출법 (stratified random sampling)

    -   이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출하는 방법으로, 유사한 원소끼리 몇 개의 층(stratum)으로 나누어서 각 층에서 랜덤 추출하는 방법
    -   ![stratified-random-sampling](https://user-images.githubusercontent.com/291782/161561325-a774c94f-ce60-4430-a2fb-f91dcdf969c2.png)

    

    

#### 측정방법 (아주중요)

-   질적척도 : 범주형 자료, 숫자들의 크기 차이가 계산되지 않는 척도
    -   명목척도 : 측정 대상이 어느 **집단**에 속하는지 분류할 때 사용 (성별, 출생지 구분)
    -   순서척도 : 측정 대상의 **서열관계**를 관측하는 척도 (만족도, 선호도, 학년, 신용등급)
-   양적척도 : 수치형자료, 숫자들의 크기 차이를 계산할 수 있는 척도
    -   구간척도(등간척도) : 측정 대상이 갖고 있는 **속성의 양**을 측정하는 것으로 구간이나 구간 사이의 **간격이 의미가 있는** 자료 (온도, 지수)
    -   비율척도 : 간격(차이)에 대한 비율이 의미를 가지는 자료, **절대적인 기준인 0이 존재**하고 **사칙연산이 가능**하며 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리)

순서척도는 명목척도와 달리 매겨진 숫자의 크기를 의미있게 활용 가능 (예: 1등이 2등보다 성적이 높다)

구간척도는 절대적 크기는 측정할 수 없기 때문에 사칙연산 중 더하기와 빼기는 가능. 곱하기나 나눗셈은 불가능





#### 오분류표 (Confusion matrix)

![precision_recall](https://user-images.githubusercontent.com/291782/150641056-4425fc9d-36be-4369-9c35-f76b1522c204.png)

- 참긍정률(TPR)  = $\dfrac{TP}{TP+FN}$ = 재현율(Recall) = 민감도(Sensitive) = ROC의 세로축
- 거짓긍정률(FPR) = $\dfrac {FP}{FP+TN}$ = (1 - 특이도(Specificity)) = ROC의 가로축
- 정확도(Accuracy, 정분류율) = $\dfrac {TP+TN}{TP+TN+FP+FN}$
- 오분류율(Error Rate) : $1 - Accuracy = \dfrac {FN + FP} {P + N}$
- 정밀도(Precision) = $\dfrac {TP}{TP+FP}$
- 재현도(Recall) = 민감도(Sensitive) = $\dfrac {TP}{TP+FN}$ = TPR(참긍정률)
- 특이도(Specificity, TNR, True Negative Rate) = $\dfrac{TN}{TN+FP}$
- F1-Score 에 들어가는 지표는? 정밀도(Precision) 와 재현율(Recall, 민감도)
    - 식 = $2 × \dfrac {Precision × Recall}{Precision + Recall}  $ 
    - 재현율과 정밀도 값이 모두 클 때 F1-Score도 큰 값을 가진다
    - F1-Score는 민감도와 정밀도를 합한 **성능평가지표**로 0~1 사이의 값을 가진다. 1이 좋음





#### 연관규칙

-   개념
    -   연관성 분석은 흔히 장바구니분석(market basket analysis) 또는 서열분석 (sequence analysis) 이라고 불린다.
    -   기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용
    -   장바구니 분석 : 장바구니에 무엇이 같이 들어 있는지에 대한 분석
    -   서열분석 : A를 산 다음에 B를 산다.

-   연관규칙의 측도 : 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 한다.

1. 지지도 (support)

    - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의
    - 지지도(support) = $P(A \cap B) = \dfrac {A와 B가 동시에 포함된 거래수} {전체 거래수} = \dfrac {A \cap B} {전체}$

2. 신뢰도 (confidence)

    - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률. 연관성의 정도를 파악 가능
    - 신뢰도(confidence) = $\dfrac {P(A \cap B)} {P(A)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수} = \dfrac {지지도} {P(A)}$

3. 향상도 (Lift)

    - A가 구매되지 않았을 떄 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가 비이다. 연관규칙 A&rarr;B는 품목 A와 품목 B의 구매가 서로 **관련이 없는 경우**에 **향상도가 1**이 된다.

    - 향상도(lift) = $\dfrac {P(B|A)} {P(B)} = \dfrac {P(A \cap B)} {P(A)P(B)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수 \; X \; B를 포함하는 거래수} = \dfrac {신뢰도} {P(B)}$ 

        <img width="935" alt="support-confidence-lift" src="https://user-images.githubusercontent.com/291782/163676755-3b1a3876-f2d0-4ce5-b748-1bf5f4038e19.png">





#### 결정계수(R<sup>2</sup>)

- 설명력 : R<sup>2</sup>는 전체 데이터를 회귀모형이 설명할 수 있는 설명력을 의미

- SSR / SST (전체 제곱합에서 회귀제곱합의 비율), $0 \le R^2 \le 1$

    > $SST = \displaystyle \sum(y_i - \overline{y})^2 , \quad SSE = \displaystyle \sum (y_i - \hat{y_i})^2, \quad SSR = SST - SSE$

- 단순선형회귀분석에서는 결정계수는 **상관계수 r ( -1 < r < 1)**의 제곱과 같다.

- 결정계수(R<sup>2</sup>)를 통해 추정된 회귀식이 얼마나 타당한지 검토한다. (결정계수가 1에 가까울수록 회귀모형이 자료를 잘 설명함)

- 독립변수가 종속변수 변동의 몇%를 설명하는지 나타내는 지표이다.

- 다변량 회귀분석에서는 독립변수가 많아지면 결정계수가 높아지므로 독립변수가 유의하든, 그렇지 않든 독립변수의 수가 많아지면 결정계수가 높아지는 단점이 있다.

- 이러한 결정계수의 단점을 보완하기 위해 수정 결정계수(R<sup>2</sup>: adjusted R<sup>2</sup>)를 활용한다. 수정결정계수는 결정계수보다 작은 값으로 산출되는 특징이 있다.

- 수정 결정계수 (adjusted R<sup>2</sup>) 식

    $R_a^2 = 1 - \dfrac {(n-1)(1-R^2)} {n - k - 1} \newline = 1- \dfrac {(n-1) \times (\dfrac {SSE} {SST})} {n-k-1} \newline = 1 - (n-1)\dfrac {MSE} {SST} \newline (k:독립변수 개수, n: 데이터 개수)$

    



#### 잔차(Residuals)

- 예측값과 실제값의 차이

- ```R
    Residuals:
      Min        1Q       Median     3Q       Max
    -29.056    -9.525    -2.272    9.215    43.201
    ```

- 잔차의 최솟값(Min), 사분위수(1Q, Median, 3Q), 최대값(Max)을 보여줌

- 중앙값이 0에 가깝고, 1, 3사분위 수가 거의 대칭을 이루고 있으므로, 잔차가 정규분포에서 것의 벗어나지 않았다고 볼수 있음

> 오차(error)와 잔차(residual)의 차이
>
> - 오차 : 모집단에서 실제값이 회귀선과 비교해 볼 때 나타나는 차이(정확치와 관측치의 차이)
> - 잔차 : 표본에서 나온 관측값이 회귀선과 비교해볼 때 나타나는 차이



#### 회귀계수(Coefficients)

```R
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
```



- Estimate는 데이터로 부터 얻은 계수의 추정치(estimate)를 말함
- 절편(Intercept)의 추정치는 -17.5791로,  ```speed```가 0 일때 ```dist```의 값이다
- ```speed```의 계수 추정치는 3.9324로 ```speed```가 1 증가할 때마다 ```dist```가 3.9324 증가한다는 것을 의미함
- 이를 수식으로 정리하면 $dist = -17.5791 + 3.9324 \times speed$
- 추정치 오른쪽 끝의 ```Pr(>|t|)```는 모집단에서 계수가 0 일때, 현재와 같은 크기의 표본에서 이런한 계수가 추정될 확률인 p 값을 나타낸다. 이확률이 매우 작다는 것은, 모집단에서 ```speed```의 계수가 정확히 3.9324가 아니더라도 현재의 표본과 비슷하게 0보다 큰 어떤 범위에 있을 가능성이 높다는 것을 의미한다. 보통 5%와 같은 유의수준을 정하여 p값이 그 보다 작으면 (p < 0.05), ```"통계적으로 유의하다"``` 라고 한다.



#### 모형적합도

```R
Multiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

Multiple R-squared, Adjusted R-squared, F-statistic, p-value는 모형이 데이터에 잘 맞는 정도를 보여주는 지표들이다.

- Multiple R-squared: 0.6511
    - 모형 적합도(혹은 설명력)
    - `dist`의 분산을 `speed`가 약 65%를 설명한다
    - 각 사례마다 `dist`에 차이가 있다.
- Adjusted R-squared: 0.6438
    - 독립변수가 여러 개인 다중회귀분석에서 사용
    - 독립변수의 개수와 표본의 크기를 고려하여 R-squared를 보정
    - 서로 다른 모형을 비교할 때는 이 지표가 높은 쪽은 선택한다
- F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12
    - 회귀모형에 대한 (통계적) 유의미성 검증 결과, 유의미함 (p < 0.05)
    - 즉, 이 모형은 주어진 표본 뿐 아니라 모집단에서도 의미있는 모형이라 할 수 있음



#### 결과 보고

논문 등에서 회귀분석의 결과는 다음 순서대로 보고한다.

먼저 모형적합도를 보고한다. F 분포의 파라미터 2개와 그 때의 F 값, p-value와 유의수준의 비교를 적시한다.

> dist에 대하여 speed로 예측하는 회귀분석을 실시한 결과, 이 회귀모형은 통계적으로 유의미하였다(F(1,48) = 89.57, p < 0.05).

다음으로 독립변수에 대해 보고한다.

> speed의 회귀계수는 3.9324로, dist에 대하여 유의미한 예측변인인 것으로 나타났다(t(48) = 9.464, p < 0.05).



#### 결측값 처리 방법

-   단순 대치법
    -   단순 삭제
    -   평균 대치법 : 비조건부 평균 대치법 (단순 평균), 조건부 평균 대치법 (회귀분석을 활용하여 대치)
    -   단순확률 대치법 : 어떤 적절한 확률값을 부여한 후 대치하는 방법. Hot-deck 방법, nearest neighbor 방법
-   다중 대치법 : 단순 대치법을 여러번
    -   **m번의 대치를 통한 m개의 가상적 완전한 자료**를 만들어서 분석하는 방법
    -   **대치**(imputation step) > **분석** (analysis step) > **결합** (combination step) 의 **3단계**를 거침



#### 정상성

- 정상성은 **평균이 일정, 분산이 일정, 공분산**도 단지 시차에만 의존하고 실제 특정 시점 t, s에는 의존하지 않을 떄 만족함

- 평균이 일정할 경우

    -   모든 시점에 대해 일정한 평균을 가짐
    -   평균이 일정하지 않은 **시계열은 차분(difference)**을 통해 정상화 할 수 있다.

    > 차분이란?
    >
    > - 현시점 자료에서 전시점 자료를 빼는 것
    > - 일반차분 (regular difference) : 바로 전 시점의 자료를 빼는 방법
    > - 계절차분 (seasonal difference) : 여러 시점 전의 자료를 뺴는 방법, 주로 계절성을 갖는 자료를 정상화 하는데 사용

- 분산이 일정

    -   분산도 시점에 의존하지 않고 일정해야함
    -   분산이 일정하지 않을 경우 **변환 (transformation)을 통해 정상화** 할 수 있다.

- 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않는다.     



#### 시계열 자료 분석방법

##### 가. 분석방법

- 회귀분석 (계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등이 있다.



##### 나. 자료 형태에 따른 분석 방법

- 일변량 시계열 분석
    - Box-Jenkins (ARMA), 지수 평활법, 시계열 분해법 등이 있다.
    - 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심을 갖는 경우의 시계열 분석
- 다중 시계열
    - 계량경제 모형, 전이함수 모형, 개입분석, 상태공간 분석, 다변량 ARIMA 등
    - 여러개의 시간(t)에 따른 변수들을 활용하는 시계열 분석
    - 예) 이자율, 인플레이션이 환율에 미치는 요인



##### 다. 이동평균법

- 개념
    - 추세를 파악하여 다음 기간을 예측하는 방법
    - n개의 시계열 데이터를 m기간으로 이동평균하면 n-m+1개의 이동평균 데이터가 생성된다.
- 특징
    - 간단하고 쉽게 미래를 예측가능, 자료의 수가 많고 안정된 패턴을 보이는 경우 예측의 품질이 높음
    - 특정 기간안에 속하는 시계열에 대해서는 동일한 가중치를 부여
    - 불규칙변동이 심하지 않은 경우에는 짧은 기간(m의 개수가 적음), 반대로 불규칙변동이 심한 경우 긴 기간 (m의 개수가 많음)의 평균을 사용
    - 이동평균에서 가장 중요한 것은 적절한 기간을 사용하는 것. 즉, 적절한 n의 개수를 결정하는 것.



##### 라. 지수평활법 (Exponential Smoothing)

- 개념
    - 일정기간의 평균을 이용하는 이동평균법과 달리 모든 시계열 자료를 사용하여 평균을 구하며, 시간의 흐름에 따라 최근 시계열에 더 많은 가중치를 부여하여 미래를 예측하는 방법
- 특징
    - 단기간에 발생하는 불규칙변동을 평활하는 방법
    - 자료의 수가 많고, 안정된 패턴을 보이는 경우일수록 예측 품질이 높음
    - 지수평활법에서 가중치의 역할을 하는 것은 지수평활계수(&alpha;)이며, 불규칙변동이 큰 시계열의 경우 지수평활계수는 작은 값을, 불규칙변동이 작은 시계열의 경우, 큰 값의 지수평활계수를 적용(generally, &alpha; is between 0.05 and 0.3)
    - 지수평활계수는 예측오차 (실제 관측치와 예측치 사이의 잔차제곱합)를 비교하여 예측오차가 가장 작은 값을 선택하는 것이 바람직함
    - 지수평활계수는 과거로 갈수록 지속적으로 감소함
    - 지수평활법은 불규칙변동의 영향을 제거하는 효과가 있으며, 중기 예측 이상에 주로 사용됨
    - 단, 단순지수 평활법의 경우, 장기추세나 계절변동이 포함된 시계열의 예측에는 적합하지 않음



#### 시계열모형

##### 가. 자기회귀 모형 (AR모형, autoregressive model)

- p 시점 전의 자료가 현재 자료에 영향을 주는 모형

- $Z_t = \Phi_1Z_{t-1} + \Phi_2Z_{t-2} + ... + \Phi_pZ_{t-p} + \alpha_t$

- >- Z<sub>t</sub> : 현재 시점의 시계열 자료
    >- Z<sub>t-1</sub>, Z<sub>t-2</sub> ..., Z<sub>p</sub> : 이전, 그 이전 시점 p의 시계열 자료
    >- &Phi;<sub>p</sub> : p 시점이 현재에 어느 정도 영향을 주는지를 나타내는 모수
    >- &alpha;<sub>t</sub> : 백색잡음과정 (white noise process) : 시계열분석에서 오차항을 의미
    >- 평균이 0, 분산이 &sigma;<sup>2</sup>, 자기공분산이 0인 경우를 뜻하며, 시계열간 확률적 독립인 경우 강(strictly) 백색잡음 과정이라고 한다. 백색잡음 과정이 정규분포를 따를 경우 이를 가우시안(Gaussian) 백색잡음과정이라고 한다.

- AR(1) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &alpha;<sub>t</sub>, 직전 시점 데이터로만 분석

- AR(2) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &Phi;<sub>2</sub>Z<sub>t-2</sub> + &alpha;<sub>t</sub>, 연속된 2시점 정도의 데이터로 분석

- AR(2) 모형의 자기상관함수(ACF)와 편자기상관함수(PACF)

- <img width="584" alt="acf-pacf" src="https://user-images.githubusercontent.com/291782/162600723-a762b3b5-6461-4b8c-972e-c100914b3da5.png">



##### 나. 이동평균 모형 (MA 모형, Moving Average model)

- 유한한 개수의 백색잡음의 결합이므로 언제나 정상성을 만족
- 1차 이동평균모형 (MA1 모형)은 이동평균모형 중에서 가장 간단한 모형으로 시계열이 같은 시점의 백색잡음과 바로 전 시점의 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub> -  &phi;<sub>2</sub>&alpha;<sub>t-2</sub> - ... -  &phi;<sub>p</sub>&alpha;<sub>t-p</sub> 
- 2차 이동평균모형 (MA2 모형)은 바로 전 시점의 백색잡음과 시차가 2인 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub>
- AR모형과 반대로 ACF에서 절단점을 갖고, PACF가 빠르게 감소
- $Z_t = \alpha_t - \phi1\alpha_{t-1} - \phi_2\alpha_{t-2}$



##### 다. 자기회귀누적이동평균 모형 (ARIMA(p,d,q) 모형, autoregressive integrated moving average model)

- ARIMA 모형은 비정상시계열 모형이다

- ARIMA 모형은 차분이나 변환을 통해 AR모형이나 MA모형, 이 둘을 합친 ARMA 모형으로 정상화 할 수 있다.

- p는 AR모형, q는 MA모형과 관련이 있는 차수

- 시계열 {Z<sub>t</sub>}의 d번 차분한 시계열이 ARMA (p, q) 모형이면, 시계열 {Z<sub>t</sub>}는 차수가 p,d,q인 ARIMA 모형, 즉 ARIMA(p,d,q) 모형을 갖는다고 한다.

- d=0이면 ARMA(p, q) 모형이라 부르고, 이 모형은 정상성을 만족한다. (ARMA (0 , 0)일 경우 정상화가 불필요)

- p=0 이면 IMA (d, q) 모형이라 부르고, d번 차분하면 MA(q) 모형을 따른다.

- q = 0이면 ARI (p, d) 모형이라 부르며, d번 차분한 시계열이 AR (p) 모형을 따른다.

    > ARIMA (0, 1, 1)의 경우에는 1차분 후 MA(1) 활용
    >
    > ARIMA (1, 1, 0)의 경우에는 1차분후 AR(1) 활용
    >
    > ARIMA (1, ,1 2)의 경우에는 1차분 후 AR(1), MA(2), ARMA (1, 2) 선택 활용
    >
    > => 이런 경우 가장 간단한 모형을 선택하거나 AIC 를 적용하여 점수가 가장 낮은 모형을 선정



##### 라. 분해 시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

    > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
    >
    > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
    >
    > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
    >
    > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인





#### 인공신경망 활성화함수

- 뉴런의 활성화 함수

    - **시그모이드 함수**의 경우 로지스틱 회귀분석과 유사하며, 0 ~ 1의 확률값을 가진다.

        <img width="878" alt="sigmoid-fn" src="https://user-images.githubusercontent.com/291782/163423927-052bd171-81cc-45b6-97d8-70ba67c1a4e5.png">

    - softmax 함수 : 표준화지수 함수로도 불리며, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수

        - $y_i = \dfrac {exp(z_j)} {\displaystyle \sum_{i=1}^Lexp(z_i)}, j = 1, ... ,L$

    - Relu함수 : 입력값이 0 이하는 0, 0 초과는 x값을 가지는 함수. 최근 딥러닝에서 많이 활용하는 활성화 함수

        - $Y^{relu} = \begin{cases} 0,\; if \quad x \le 0 \\ x, \; if \quad x \gt 0  \end{cases}$

    - 단일 뉴런의 학습 (단층 퍼셉트론)

        - 퍼셉트론은 선형 결합기와 하드 리미터로 구성된다.

        - 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눈다.

            <img width="818" alt="hyperplane" src="https://user-images.githubusercontent.com/291782/163426523-78c739e8-fe97-499d-8d72-f111438f3988.png">



#### 군집분석의 유사도 측도

-   맨하튼 거리 : 각 방향 직각의 이동거리 합으로 계산
-   유클리드 거리 : 두 점을 잇는 가장 짧은 직선거리
-   마할라노비스거리 : 통계적 개념이 포함된 거리. 변수들의 산포를 고려하여 표준화한 거리
-   표준화 거리 : 해당 변수의 표준편차로 척도 변환 후 유클리디안 거리를 계산



#### 군집화 기법의 종류

- 밀도기반 군집분석
    - 어느 점을 기준으로 주어진 반경 내에 최소 개수만큼의 데이터들은 가질 수 있도록 함으로써 특정 밀도함수 혹은 밀도에 의해 군집을 형성해 나가는 기법
    - 종류
        - DBSCAN : 밀도 한계점에 따라 군집을 형성해 나가는 대표적인 밀도기반 군집화 기법
        - OPTICS : 순서를 생성하는 밀도기반 기법
        - DENCLUE : 밀도 분포함수에 기초한 군집화 방법
- 격자기반 군집분석
    - 데이터 포인트 대신 셀을 이용해 군집화 과정을 수행하는 기법. 빠른 처리시간. 셀의 수에만 의존
    - 종류
        - STING : 격자 셀에 저장되어 있는 통계정보를 탐색
        - WaveCluster : Wavelet 변환 기법을 사용
        - CLIQUE : 고차원 데이터 공간의 군집화를 위한 격자 및 밀도기반 기법



#### K-평균군집분석의 특징 (k-means, k평균)

- 거리 계산을 통해 군집화가 이루어지므로 **연속형 변수에 활용이 가능**
- K개의 **초기 중심값은 임의로 선택이 가능**하며 가급적이면 멀리 떨어지는 것이 바람직하다.
- 초기 중심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집 혼합되지 않고 층으로 나누어질 수 있어 주의하여야 한다. **초기 중심값의 선정에 따라 결과가 달라**질 수 있다.
- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 **탐욕적(greedy) 알고리즘**이므로 안정된 군집은 보장하나 최적이라는 보장은 없다.
- 장점
    - 알고리즘이 단순하며, 빠르게 수행되어 분석 방법 적용이 용이
    - 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다.
    - 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다.
    - 다양한 형태의 데이터에 적용이 가능
- 단점
    - 군집의 수, 가중치와 거리 정의가 어렵다.
    - 사전에 주어진 목적이 없으므로 결과 해석이 어렵다.
    - 잡음이나 이상값의 영향을 많이 받는다.
    - 볼록한 형태가 아닌 (non-convex) 군집이 (예를 들어 U형태의 군집) 존재할 경우에는 성능이 떨어진다.
    - 초기 군집수 결정에 어려움이 있다.



####  Corpus (비정형데이터마이닝 중 텍스트 마이닝)

-   데이터마이닝의 절차 중 데이터의 정체, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 더 이상 추가적인 절차 없이 데이터 마이닝 알고리즘 실험에 활용될 수 있는 상태이다.

#### Term-Document Matrix (비정형데이터마이닝 중 텍스트 마이닝)

-   텍스트 마이닝을 불러온 문서에 대해 plain text로 전환, 공백 제거, lowercase로 변환, 불용어(stopward) 처리, 어간추출(stemming) 등의 작업을 수행한 다음에 문서 번호와 단어 간의 사용 여부 또는 빈도수를 이용해 matrix를 만드는 작업이 term document matrix이다.

#### Dictionary (비정형데이터마이닝 중 텍스트 마이닝)

-   텍스트 마이닝 분석 시 사용하고자 하는 단어들의 집합



#### 상관분석

-   정의 : 두 변수의 상관관계를 알아보기 위해 상관계수 (correlation coefficient)를 이용하며, 그 공식은 아래와 같다.
-   $r = \dfrac {cov(x, y)} {S_x ⅹ S_y} = \dfrac {\displaystyle \sum_{i=1}^n [(x - \overline{x})(y - \overline{y})]} {n(S_x ⅹ S_y)}$
-   특성 : $0.7 \lt  r \le 1$ : 강한 양(+)의 상관관계, $0 \lt  r \le 0.3$ : 거의 상관 음다. ,r = 0 : 상관관계(선형, 직선)가 없다.
-   상관분석의 유형
    -   피어슨 : 등간척도, 연속형 변수, 정규성 가정, 대부분 많이 사용, 피어슨 
    -   스피어만 : 서열척도, 순서형 변수, 비모수적 방법, 순위를 기준으로 상관관계 측정. 순위상관계수



#### 1종 오류, 2종 오류

-   1종 오류 : 귀무가설(H0)가 실제로 참이어서 채택해야 함에도 불구하고 표본의 오차 때문에 이를 채택하지 않는 오류. 보통 &alpha;로 표기하고 유의수준이라고 부름

-   2종 오류 : 귀무가설(H0)가 거짓이라 채택하지 말아야 하는데 표본의 오차때문에 이를 채택하는 오류. 보통 &beta;로 표기

    ![type1-type2-error](https://user-images.githubusercontent.com/291782/169569549-6abd1668-32ee-40b3-8434-8db458d2c278.png)



### 5과목 데이터 시각화

#### 시각화 인사이트 프로세스는?

-   탐색(1단계) > 분석(2단계) > 활용(3단계)



#### 데이터클라우드(워들) 사용하는 단계에 대한 설명

-   데이터클라우드는 탐색 단계에서 비정형 데이터(텍스트 데이터) 측정값에서 관계를 탐색하기 위해 사용하는 시각화이다.



#### 벤프라이(Ben Fry) 7단계

-   정보획득, 분석(분해), 선별, 마이닝, 표현, 정제, 상호작용



#### facet_grid : 집단별 구분

-   facet_grid(Type ~.) # 가로 (라벨이 오른쪽에 존재)
-   facet_grid(. ~ Type) # 세로 (라벨이 위쪽에 존재)
-   facet_grid(Type ~ Origin) # Type은 오른쪽, Origin은 윗쪽에 라벨 존재



#### D3 라이브러리

-   d3.layout.pie() : 이렇게 만드는건 파이 차트만
-   d3.scale.linear() : 막대차트, 스캐터플롯



#### 시각화를 위한 그래픽 디자인 기본 원리

1.   아이소타이프 (ISOTYPE, International System Of TYpographic Picture Education)

     -   많은 양의 데이터를 쉽게 지각할 수 있도록 도와주는 시각표현 방법

     -   정보, 자료, 개념, 의미 등을 나타내기 위해 문자와 숫자 대신 상징적 도형이나 정해진 기호를 조합해 시각적이고 집접적으로 나타내는 방식

     -   ![isotype](https://user-images.githubusercontent.com/291782/158065420-24a0c418-cf69-453e-879c-1b9f758b8b1f.png)

2.   타이포그래피
     -   가장 어려운 일이 서체를 선택하는 것



#### 시각적 이해의 위계

-   데이터는 시각화
-   정보는 디자인
-   지식은 매핑
-   지혜는 정의되지 않은 것으로 표시됨
-   데이터 : 분리된 요소(=개별적 요소 하나 하나), 단어, 숫자, 암호, 도표, 차트 등. 근거나 되는 사실이나 참고 자료를 의미. 원자재. 데이터는 불완전하고 비연속적
-   정보 : 연관된 요소들. 의견, 단락, 균형, 개념, 생각, 질문. 데이터와 달리 그 자체만으로 의미가 있다.
-   지식 : 조직화된 정보. 화제, 이론, 이치, 개념상의 구성. 경험을 통해 **다른 관점과 방법으로 해석할 수 있다.**
-   지혜 : 적용된 지식. 책, 범례, 체계. 종교 철학. **명시적인 언어로 상대에게 전달하기 어렵다.**



#### 시각화 라이브러리 와 인포그래픽스

-   시각화 라이브러리
    -   종류:  PolyMaps, D3.js, Google Chart
    -   라이브러리 설치 필요, 라이브러리가 제공하는 API로 코드 작성해 시각화
-   인포그래픽스
    -   웹서비스 형태로 제공, 회원가입 필요, 제공되는 템플릿으로 구현 가능
    -   종류 : iCharts, Visualize Free, Visual.ly



#### 데이터 구성원리

1.   이벤트 기록으로서 접근
     -   원본 데이터 (raw data, log data)는 명세화의 기본 대상이 된다.
     -   원본 데이터는 특정 이벤트가 발생했을 때 생성된다.
     -   데이터로 부터 통찰을 이끌어 내기 위해서는 **데이터가 어떤 원리로 생성, 구성되었는지**를 항상 염두에 두고 있어야 한다.
2.   객체지향 관점에서의 접근
     -   데이터의 구성과 생성 배경에 대해 고민함으로써 어떤 식으로 시각화할 지에 대한 닶을 찾아갈 수 있다.
     -   데이터의 범위가 주어지면, 데이터의 구조 자체를 설계, 생성하여 이를 토대로 통찰을 뽑아볼 수 있다.
     -   기본적으로 대상을 객체화 한다.
     -   **모든 객체들은 행위와 고유 속성 값**을 갖게 된다.



#### 시각화 방법에 따른 그래프의 종류 (중요**)

-   시간시각화 : 막대 그래프, 누적막대 그래프, 점그래프
-   분포시각화 : 파이차트, 도넛차트, **트리맵**, **누적연속그래프**
-   관계시각화 : **스케터플롯**(산점도, XY그래프), **버블차트**, 히스토그램
-   비교시각화 : **히트맵**, 체르노프 페이스, 스타차트, 평행좌표계, 다차원 척도법
-   공간시각화 : 지도 매핑



- 산점도

    ![scatterplot](https://user-images.githubusercontent.com/291782/158017483-7c785a0c-2e4c-4840-97f0-7dab7b8d36ff.png)



- 점 그래프

    ![point-graph](https://user-images.githubusercontent.com/291782/168115035-f7be41ff-7380-462e-9960-73eb863da672.png)

    

- 평행좌표계

    ![parallel-graph](https://user-images.githubusercontent.com/291782/158017799-206cfc73-ef4f-4a2e-b161-44727750d9a1.png)

    



- 다차원 척도법

    ![mds-graph](https://user-images.githubusercontent.com/291782/168115621-282a7350-ff8e-423e-a6a9-974f54d60089.png)



- 지도 매핑

    ![map-mapping](https://user-images.githubusercontent.com/291782/168115803-af72e984-3b60-46a1-b49b-6b011b886013.png)

    



#### D3.js 설명

1.   특징
     -   JS 기반의 데이터 시각화 라이브러리
     -   HTML5, SVG, CSS로 데이터 시각화
     -   SVG 객체, canvas 객체 등을 기반으로 동작
     -   CSS를 통해 레이아웃과 속성 변경을 통해 디자인적 요소 조작 가능
     -   cross browser 지원
2.   기본 개념
     -   SVG
         -   그림을 그리기 위한 html 태그
         -   rect, circle, line, path, ellipse, polyline 등의 객체를 사용하여 그림
         -   시각화 구현을 위해 HTML5의 SVG 객체가 필요함
     -   SCALE
         -   시각화 그림들이 화면에 부자연스럽게 표현되는 것을 방지하기 위해 사용
         -   **시각화의 최적화**를 도움
         -   domain() : scale **입력** 값의 범위 지정
         -   range() : scale **출력** 값의 범위 지정

-   JS 기반의 데이터 시각화 라이브러리
-   HTML5, SVG, CSS로 데이터 시각화
-   SVG 객체, canvas 객체 등을 기반으로 동작
-   CSS를 통해 레이아웃과 속성 변경을 통해 디자인적 요소 조작 가능
-   모든 브라우저에 동일한 코드에 대한 일관적인 결과 얻을 수 있음





#### 타이포그래피

-   서체
    -   글의 형태를 총칭하는 말로 얼굴에 해당. 타이포 그래피에서 **가장 어려운 일이 서체를 선택**하는 일
    -   세리프 서체 : 돌기가 있음. 가독성이 높아 본문용 (함초롱 바탕체)
    -   산세리프 서체 : 돌기가 없음. 주목성이 높아 제목용 서체 (맑은 고딕)
-   무게
    -   획의 두께를 의미하며, 굵기라고도 한다.
-   크기
    -   글자 크기는 **실제 글자의 크기가 아니**라 글자가 배치되는 금속 활자판의 높이를 의미
    -   같은 크기라도 서체에 따라 실제 글자 크기가 달라짐
-   스타일
    -   각도에 따라 글자 스타일이 달라짐
    -   이탤릭체와 같이 기울이거나 장체, 평체처럼 글자의 폭을 좁히거나 넓힘
-   색채
    -   명도, 채도, 색상의 색채 속성을 활용해 정보 분류 가능
    -   **정보의 중요도나 종속의 관계표현이 가능**
-   간격 (글자 사이, 낱말 사이, 글줄 사이)
    -   가독성에 큰 영향을 미침
    -   **읽어야 할 다음 글자가 다른 글자보다 근접**해 있어야 하며, 이 때문에 글자 사이보다 낱말 사이, 낱말 사이보다 글줄 사이가 넓어야 함



#### TABLEAU (태블로, 시각화 플랫폼 제품)

- MS 데이터 소스, MySQL, ORACLE, IBM OLAP 서버, csv 파일 등 **다양한 데이터**로부터 실시간으로 크로스 테이블을 시각적으로 보여줌
- **VizQLTM** (비주얼 쿼리 언어)을 개발해 사용자가 DB와 상호작용하면서 그래픽 / 시각적인 결과를 얻을 수 있다.
- 태블로 실행 후 데이터 소스에 연결하면, 자동으로 데이터 소스의 필드들을 **디맨션이나 measure로 분할**한다. 필드들을 shelves에 끌어다 두는 것으로 쓸 수 있다. 매우 직관적이며, 분석에 빠르게 적용할 수 있다.
- 태블로는 **크로스탭과 피벗 테이블 기능**을 단 몇번의 클릭만으로 가능하다.



#### 래치(LATCH) 방법

- 위치(Location), 알파벳(Alphabet), 시간(Time), 카테고리(Category), 위계(Hierarchy) 이상 5가지가 정보를 정리 또는 조직화하는 기준

- 위치 : 정보를 공간적인 위치에 배열하는 방법으로 지리적인 것만이 아니라 공간적으로 구분하는 것 모두를 포괄한다.

    ![latch-location](https://user-images.githubusercontent.com/291782/168120447-af159c41-e9fe-4d94-8a00-2cf37f6b7d1e.png)

- 알파벳 : 알파벳 또는 가나다순으로 정렬하는 방법이 흔히 사용됨

    ![latch-alphabet](https://user-images.githubusercontent.com/291782/168120675-1fe8feda-6be0-4031-b0a5-d64c0fa164e1.png)

- 시간 : **연도별 시간 순서**에 따라 강아지를 분류

    ![latch-time](https://user-images.githubusercontent.com/291782/168120883-d4f7f5c3-b178-4c2a-ac05-7618b4c7a098.png)

- 카테고리 : 정보의 속성에 따라 분류할 떄 적합. 상점의 상품분류, 도서관의 서적 분류 등

    ![latch-category](https://user-images.githubusercontent.com/291782/168121082-3f162705-36ce-41f4-a3b5-24b5c07e36bf.png)

- 위계(가중치) : 고도의 변화(낮음에서 높음), 가격의 변화(싼 것에서 비싼것) 등 **정보의 변화에 따라 데이터의 값이나 중요도의 순서로 정보를 조직화 하는 것**

    ![latch-hierarchy](https://user-images.githubusercontent.com/291782/168121378-1652c57f-40bc-48ae-9455-66ff33ae53eb.png)



#### 시각화 방법의 단계

- 정보 구조화 > 정보 시각화 > 정보 시각표현
- 정보구조화 : 데이터 수집 및 탐색, 데이터 분류하기, 데이터 배열하기, 데이터 재배열
- 정보 시각화 : 시간 시각화, 분포 시각화, 비교 시각화, 여러 변수 비교, 공간 시각화
- 정보 시각표현 : 그래픽 7요소(위치, 크기, 모양, 색, 명도, 기울기, 질감), 그래픽 디자인 기본원리, 인터랙션, 시각정보 디자인 7원칙
    - 시각정보 디자인 7원칙
    - 시각적 비교를 강화하라
    - 인과관계를 제시하라
    - 다중변수를 표시하라
    - 텍스트, 그래픽, 데이터를 한 화면에 조화롭게 표시하라
    - 콘텐츠의 질과 연관성, 진실성을 분명히 하라
    - **시간순이 아닌 공간순으로 나열하라**
    - 정량적 자료의 정량성을 제거하지 마라







### 서술형

#### PCA 분석

1. 각 라면에 대하여 면발, 라면 그릇의 모양, 국물맛에 대한 소비자의 점수를 측정하여 3개의 변수(면, 그릇, 국물)를 사용하여 주성분 분석을 시행한 결과 Biplot이 아래와 같다.

    ![pca-image](https://user-images.githubusercontent.com/291782/169244885-8b117a20-3802-49ae-97f9-8e47f40719d9.png)

    

    1-1. PC1과 PC2의 의미를 원변수와의 관계를 통해 유추하시오

    > 화살표는 원변수와 주성분(PC)의 상관계수를 나타내며, PC와 평행할수록 해당 PC에 큰 영향을 끼친다. 또, 화살표가 같은 방향으로 인접해 있을수록 같은 주성분으로 생성될 수 있다. PC1은 국물, 면 변수가 하나로 묶여 생성되었다고 판단할 수 있으며, PC1과 두 변수는 강한 양의 상관관계가 있을 것이라고 해석이 가능하다. PC2는 그릇 변수가 하나로 묶여 생성이 되었다고 판단할 수 있다. 그리고 3가지 변수 중 가장 영향을 많이 끼치는 변수는 PC1과 수평을 이루고 있는 국물 이라고 판단된다.

    2-2. 이상치가 있다면 어떤 특징을 가지는지 서술하시오

    > 그래프에서 이상치로 판단되는 라면은 **얼큰라면, 해물라면과 된장라면**으로 판단할 수 있다. **얼큰라면의 특징은 면 변수에 영향**을 많이 받고 있는 라면이라고 해석할 수 있다. 또 **해물라면의 특징은 그릇 변수에 영향**을 많이 받고 있는 라면으로 알 수 있으며, **된장라면은 3가지 변수에 모두 영향을 받지 않는** 라면으로 해석할 수 있다.

    

#### 군집분석

아래는 세계 유명 도시(11개 도시)들의 주거 환경을 평가한 자료를 군집분석 한 결과이다.

1-1. 아래 1은 계층적 군집분석을 한 결과를 덴드로그램으로 표현한 것이다. 2개의 군집으로 나눌 경우와 3개의 군집으로 나눌 경우 각 군집에 포함되는 도시들을 나열하시오

![dendrogram-1](https://user-images.githubusercontent.com/291782/169271557-92fee835-9879-4731-80a1-52f6a3aecc9c.png)

> 덴드로그램 시각화 결과에서 Height 값 (y축)을 기준으로 하여 하위 군집을 구성하는 방법은 해당 Height 값에서 수평으로 선을 그어 나뉘는 그룹을 하나의 군집으로 구성한다.
>
> 2개의 군집으로 나눌 경우 Height 60을 기준으로 나누면 (베이징, 상하이, 모스크바, 두바이)와 (파리, 스톡홀롬, 도쿄, 뉴욕, 하노이, 서울, 런던)으로 나눌 수 있다.
>
> 3개의 군집으로 나눌 경우 Height 50을 기준으로 나누면 (베이징, 상항이, 모스크바, 두바이), (파리, 스톡홀롬), (도쿄, 뉴욕, 하노이, 서울, 런던)으로 나눌 수 있다.



1-2.  아래 2는 비계층적 군집분석인 kmeans의 결과이다. 조사 자료에 대한 군집의 수를 3, 4, 5개로 군집분석을 한 결과이다. 전체 변동에서 군집 간 변동이 차지하는 비율에 대한 검토를 통해 최적 군집의 수를 정하는 방법에 대해 구체적으로 설명하시오.

![kmeans-1](https://user-images.githubusercontent.com/291782/169273073-572cbf85-4fd3-4de0-9d40-2cddca3bec82.png)

![kmeans-2](https://user-images.githubusercontent.com/291782/169273169-e12f83df-c87d-4761-aba8-edd013330519.png)

> 전체 변동에서 군집 간 변동이 차지하는 비율이 1에 가까울수록 잘 분류되었고 좋은 모델임을 나타낸다. 군집간 변동이 차지하는 비율은 `betweenss`(군집과 군집 간 중심의 거리 제곱합) / `totss`(제곱합의 총합)으로 구할 수 있다. 해당 분석 결과에 대해 확인했을 때, 3, 4개의 군집으로 나누었을 때 보다 5개의 군집으로 나누었을 때 `80.6%`로 가장 군집이 잘 되었다고 판단할 수 있다. 또 , `kmeans clustering`으로  5개의 군집으로 나눈 결과에서도 `[between_SS / total_SS = 80.6%]`로 나타나 전체 변동에서 군집 간 변동이 차지하는 비율을 확인 할 수 있다.
>
> 또, 최적 군집의 수를 정하는 방법으로는 군집 수에 따른 집단 내 제곱합 그래프를 통해 `scree plot` 형태로 그래프를 그려 급격히 감소하는 지점까지만 군집으로 설정하여 최적의 군집 개수를 지정하거나 R 프로그램에서 `Nbclust` 패키지의 `Nbclust` 함수를 활용하여 최적의 군집을 정하는 방법도 있다.



#### 회귀분석

아래 Default 데이터는 10,000명의 신용카드 고객의 체납 여부(default)와 학생여부(student), 카드잔고(balance), 연봉(income)을 포함하고 있다.

![test-01-1](https://user-images.githubusercontent.com/291782/169275268-bf45637a-c85a-4627-a104-c27fb1b81af5.png)

1-1. 신용카드를 사용하는 고객의 체납 확률을 예측하기 위한 방법을 제시하시오.

> `str` 함수를 통해 데이터의 구조와 `summary` 함수를 통해 데이터에 대한 기초 통계량을 확인했을 때, default(체납여부)와 student(학생여부)는 범주형 변수이며, balance, income은 수치형 변수임을 확인할 수 있다. 
>
> 고객의 체납 확률을 예측하기 전, 종속변수와 독립변수를 나누어 보면 종속변수는 default이며, 설명변수는 student, balance, income 이다. 체납 여부인 default를 분류하기 위해서는 정형 데이터 마이닝 중 분류분석을 사용하여 체납 확률을 예측해야 한다. 분류 분석의 방법으로는 로지스틱 회귀뷘석, 의사결정나무, 앙상블기법(배깅, 부스팅, 랜덤포레스트), 인공신경망, 나이브베이지안, K-NN, SVM 등이 있다.
>
> 이 중 로지스틱 회귀분석 방법을 활용한다면 R 프로그램에서 `glm` 함수 등을 사용하여 모델을 구축한다. 예를 들어 `glm(default~student + balance + income, data=Default, family="binomial")` 라는 코드로 모델을 구축하는 것은 일반화 선형 모델 구축 함수인 `glm` 함수를 이용해 Default 데이터를 사용하여 종속변수 default에 대해 모든 설명변수 (student, balance, income)를 사용하여 모델을 구축할 수 있다. 분석결과가 나타난다면 먼저 설명변수에 대해 통계적 타당성을 가설검정하여 설명변수가 모두 유의한지를 파악한다. 유의하지 않은 변수가 포함이 될 수도 있으므로 `step` 함수를 활용하여 변수선택법(전진선택법, 후진제거법, 단계선택법)으로 최적의 모형을 찾을 수 있다. 
>
> 마지막으로 최종적으로 로지스틱 회귀분석의 결과를 종합하여 로지스틱 회귀식을 산출한다. 이때, 일반회귀분석과 다르므로 
>
> $P(X)=\dfrac {1} {1 + exp(x)}$ 의 식에 맞게 회귀식을 도출하여야 한다.
>
> 로지스틱 회귀분석을 진해할 때 주의사항은 분류분석이기 때문에 종속변수가 연속형 변수값이면 예측에 해당하지 않으므로 해당 종속변수를 구간화 등을 통해 범주형 변수로 변환하여 분석에 적용해야 한다.



1-2. 분석을 통해 얻은 결과물로 부터 발견할 수 있는 인사이트를 예시로 설명하시오.

> 예를 들어 단계적 선택법을 활용하여 로지스틱 회귀분석 결과를 얻었을 때, income(수입)의 변수는 유의하지 않아 제거되고 balance(카드잔고)의 계수는 양수, studentYes(학생여부 중 학생일때)의 계수가 음수로 나타났다고 가정하자.
>
> 이때, 로지스틱 회귀식은 
>
> $P(X) = \dfrac {1} {1 + exp(- (상수 + a*balance - b*studentYes))}$로 나타날 수 있으며 다른 설명변수의 조건이 동일할 떄, studentYes이 1 증가할수록 졸업할 확률은 0보다 작은 값이 나와 0.xx배 증가, 즉 학생일수록 체납확률이 낮아진다고 볼 수 있다.
>
> 로지스틱 회귀식을 통해 카드잔고가 증가할수록 체납여부는 증가하고 학생일수록 체납확률이 낮아질 것이라고 예측할 수 있다. 
>
> 마지막으로 studentYes 처럼 해당 변수의 특징을 파악해보면 학생여부를 0과 1로 나타낸 변수이므로 결과해석을 유의해서 해야된다.
>
> 이러한 인사이트를 통해 신용카드 개설을 위한 조건 강화를 통해 카드사의 손해를 줄일 수 있는 방법을 강구해야 할 거싱라고 인사이트를 도출할 수 있다. 또, 카드 잔고가 많은 고객들이 소비할 수 있도록 마케팅 전략 등을 강화하는 방안도 마련하는 인사이트도 도출할 수 있다.



#### 다변량회귀분석

MASS 패키지의 Cars93 이라는 데이터셋의 가격(Price)을 종속변수로 선정하고, 엔진크기(EngineSize), RPM, 무게(Weight)를 이용해서 다중회귀분석을 실시했다. 아래의 결과를 해석하시오

```R
> library(MASS)
> attach(Cars93)
> lm(Price~EngineSize+RPM+Weight, data=Cars93)

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Coefficients:
(Intercept)   EngineSize          RPM       Weight  
 -51.793292     4.305387     0.007096     0.007271  

> summary(lm(Price~EngineSize+RPM+Weight, data=Cars93))

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Residuals:
    Min      1Q  Median      3Q     Max 
-10.511  -3.806  -0.300   1.447  35.255 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -51.793292   9.106309  -5.688 1.62e-07 ***
EngineSize    4.305387   1.324961   3.249  0.00163 ** 
RPM           0.007096   0.001363   5.208 1.22e-06 ***
Weight        0.007271   0.002157   3.372  0.00111 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.504 on 89 degrees of freedom
Multiple R-squared:  0.5614,	Adjusted R-squared:  0.5467 
F-statistic: 37.98 on 3 and 89 DF,  p-value: 6.746e-16
```

- 해석방법

- > 다변량회귀분석은 아래와 같은 단계로 분석할 수 있다.
    >
    > 1단계 : 다변량 모형에 대한 가설검정 실시
    >
    > 2단계 : 각 변수에 대한 가설검정 실시
    >
    > 3단계 : 결정계수를 통한 모형에 대한 설명력 확인
    >
    > 4단계 : 다중공선성의 확인을 통한 모형의 안정성 확인
    >
    > 5단계 : 잔차분석을 통한 다변량 회귀분석의 가정 확인

- 해석결과

    -   위의 분석은 1~3단계 까지 해석 가능
    -   모형의 구조는 formula를 통해 확인할 수 있으며, 가격(Price)을 종속변수로 예측하기 위해 엔진사이즈(EngineSize), 회전수(RPM), 무게(Weight)라는 3가지 설명변수를 활용하여 모형이 설계되었음을 확인할 수 있다.
    -   1단계 : 다변량 회귀분석에서 종속변수인 가격(Price)에 대한 설명변수들 간의 모형에 대한 통계적 타당성을 가설 검정한다.
        -   귀무가설(H<sub>0</sub>) : EngineSize = RPM = Weight = 0
        -   대립가설(H<sub>1</sub>) : 적어도 하나의 설명변수는 0이 아니다.
        -   F-통계량은 37.98이며 p-value 가 6.746e-16 로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형은 통계적으로 매우 유의함을 알 수 있다.
    -   2단계 : 다변량 회귀분석에 활용된 각 설명변수들의 계수들에 대한 통계적 타당성을 가설 검정한다.
        -   **첫 번째 설명변수인 엔지사이즈(EngineSize)**에 대한 통계적 가설검정을 실시
            -   귀무가설(H<sub>0</sub>) : EngineSize = 0
            -   대립가설(H<sub>1</sub>): EngineSize $\neq$ 0
            -   t-통계량은 3.249 이며 p-value 값이 0.00163이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 EngineSize는 통계적으로 유의함을 알 수 있다.
        -   **두 번째 설명변수인 RPM**의 경우, t-통계량은 5.208이며 p-value 가 1.22e-06이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다.그러므로 RPM은 통계적으로 유의함을 알 수 있다.
        -   **세 번째 설명변수인 Weigh**t의 경우, t-통계량은 3.372이며 p-value가 0.00111이므로 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 모든 설명변수는 통계적으로 유의함을 알 수 있다.
    -   3단계 : 통계적으로 유의성을 확인한 다변량 회귀모형이 전체 데이터를 얼마나 **잘 설명하는지** 확인하기 위해 **결정계수(R<sup>2</sup>)**를 확인한다.
        -   결정계수를 확인하기 위해 Multiple R-squared와 R-squared를 확인결과 0.5614 와 0.5467 로 나타났으며, 이는 전체 데이터를 설계된 다변량 회귀모형이 56.14%, 54.67%를 설명하고 있다고 해석할 수 있다.
    -   최종적으로 결과를 종합해보면 **추정된 다변량 회귀식**은 아래와 같다.
    -   ```Price = -51.79 + 4.31*EngineSize + 0.007*RPM + 0.007*Weight ```
    -   회귀식을 통해 EngineSize, RPM, Weight 가 모두 증가할수록 가격(Price)도 증가하는 것을 확인할 수 있으며, **Price에 가장 영향을 많이 끼치는 변수는 EngineSize이며, 차량의 가격에서 EngineSize를 가장 많이 신경 써야한다고 결론을 도출**할 수 있다.





#### 회귀분석2

Q. swiss 데이터는 프랑스어를 사용하는 스위스 내 지역의 출산율과 관련된 자료이다. 아래는 각 변수의 내용과 출산율을 농업종사자 비율 등 5개의 변수로 설명하기 위한 모형을 추정한 결과이다. 아래의 결과를 해석하시오

```R
> # 서술형 회귀분석 (swiss 데이터 p.154)
> # 설명변수 : Agriculture : 농업, Infant.Mortality (영아사망률)
> # 종속변수 : Fertility 출산
> summary(step(lm(Fertility~1, data=swiss),
+             scope = list(lower = ~1, upper=~Agriculture+Examination+Education
+                           +Catholic+Infant.Mortality),
+             direction="both"))
Start:  AIC=238.35
Fertility ~ 1

                   Df Sum of Sq    RSS    AIC
+ Education         1    3162.7 4015.2 213.04
+ Examination       1    2994.4 4183.6 214.97
+ Catholic          1    1543.3 5634.7 228.97
+ Infant.Mortality  1    1245.5 5932.4 231.39
+ Agriculture       1     894.8 6283.1 234.09
<none>                          7178.0 238.34

Step:  AIC=213.04
Fertility ~ Education

                   Df Sum of Sq    RSS    AIC
+ Catholic          1     961.1 3054.2 202.18
+ Infant.Mortality  1     891.2 3124.0 203.25
+ Examination       1     465.6 3549.6 209.25
<none>                          4015.2 213.04
+ Agriculture       1      62.0 3953.3 214.31
- Education         1    3162.7 7178.0 238.34

Step:  AIC=202.18
Fertility ~ Education + Catholic

                   Df Sum of Sq    RSS    AIC
+ Infant.Mortality  1    631.92 2422.2 193.29
+ Agriculture       1    486.28 2567.9 196.03
<none>                          3054.2 202.18
+ Examination       1      2.46 3051.7 204.15
- Catholic          1    961.07 4015.2 213.04
- Education         1   2580.50 5634.7 228.97

Step:  AIC=193.29
Fertility ~ Education + Catholic + Infant.Mortality

                   Df Sum of Sq    RSS    AIC
+ Agriculture       1    264.18 2158.1 189.86
<none>                          2422.2 193.29
+ Examination       1      9.49 2412.8 195.10
- Infant.Mortality  1    631.92 3054.2 202.18
- Catholic          1    701.74 3124.0 203.25
- Education         1   2380.38 4802.6 223.46

Step:  AIC=189.86
Fertility ~ Education + Catholic + Infant.Mortality + Agriculture

                   Df Sum of Sq    RSS    AIC
<none>                          2158.1 189.86
+ Examination       1     53.03 2105.0 190.69
- Agriculture       1    264.18 2422.2 193.29
- Infant.Mortality  1    409.81 2567.9 196.03
- Catholic          1    956.57 3114.6 205.10
- Education         1   2249.97 4408.0 221.43

Call:
lm(formula = Fertility ~ Education + Catholic + Infant.Mortality + 
    Agriculture, data = swiss)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.6765  -6.0522   0.7514   3.1664  16.1422 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)      62.10131    9.60489   6.466 8.49e-08 ***
Education        -0.98026    0.14814  -6.617 5.14e-08 ***
Catholic          0.12467    0.02889   4.315 9.50e-05 ***
Infant.Mortality  1.07844    0.38187   2.824  0.00722 ** 
Agriculture      -0.15462    0.06819  -2.267  0.02857 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.168 on 42 degrees of freedom
Multiple R-squared:  0.6993,	Adjusted R-squared:  0.6707 
F-statistic: 24.42 on 4 and 42 DF,  p-value: 1.717e-10
```

1. 해석 방법

    -   벌점화 방식(AIC)의 변수선택법을 활용한 다변량 회귀분석은 아래와 같은 단계로 분석할 수 있다.
    -   1단계 : 변수선택법을 결정하고, 초기모형을 세팅
    -   2단계 : 선택된 최적 모형의 AIC를 계산
    -   3단계 : 선택된 모형에서 변수를 추가/삭제 할 경우의 각 모형에서 AIC를 계산
    -   4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정
    -   5단계 : 2 ~ 4단계를 반복하고 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적모형으로 선정
    -   6단계 : 다변량 모형에 대한 F-test를 통해 가설검정 실시
    -   7단계 : 각 변수의 계수에 대한 t-test를 통해 가설검정 실시
    -   8단계 : 결정계수를 통한 모형에 대한 설명력 확인
    -   9단계 : 다중공선성의 확인을 통한 모형의 안정성 확인
    -   10단계 : 잔차분석을 통한 다변량 회귀분석의 가정 확인

2. 해석 결과

    - 위의 분석 결과는 해석방법 10단계 중 1~8단계까지 해석이 가능. 모형의 구조는 formula를 통해 확인 가능하며, step 함수를 통해 종속변수에 대해 설명변수가 없을 경우부터 모든 설명변수가 포함될 때의 회귀모형을 비교해 최적의 회귀방정식을 도출할 수 있다. 또, direction에서 ```both```는 단계적 선택법, ```forward```는 전진선택법, ```backward```는 후진제거법을 의미

    - 출산율(Fertility)을 종속변수로 설정하고, 설명변수가 없을 때 부터 최대 모든 설명변수가 포함된 회귀식까지 설정하여 최적의 회귀식을 도출한다

    - 1단계 : 변수선택법을 결정하고, 초기 모형을 설정

        위의 분석결과에서 direction이 ```both```로 설정되어 변수선택법을 단계적 선택법으로 선정했음을 확인할 수 있다. 또, 초기 모형은 ```Fertility~1``` 로 설명변수가 하나도 없는 상태에서부터 시작함을 의미한다.```scope```의 경우 모형 선정 중 최소 설명변수가 아무것도 없는것부터 설명변수가 모두 있는 것까지를 비교하여 모형을 선정한다는 것이다.

    - 2단계 : 선택된 최적 모형의 AIC를 계산

        분석 결과에서 시작 모혀은 ```Fertility~1```이 **최적모형**로 설정되어 있으며 start에서 **AIC 값이 238.35**로 계산되어 있다.

    - 3단계 : 선택된 모형에서 변수를 추가/삭제 할 경우의 각 모형의 AIC를 계산한다.

        ```Fertility~1``` 모형에 대해 설명변수 5개에 대한 각각의 AIC값을 계산하여 자유도 등과 함께 나타낸다. **Education의 AIC 값이 213.04, Examination의 AIC 값은 214.97등으로 나타나 있다.**

    - 4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정한다.

        계산된 AIC 값을 비교하여 가장 작은 설명변수인 **Education을 추가하여 최적 모형으로 선정**한다.

    - 5단계 : 2 ~ 4단계를 반복하여 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적의 모형으로 선정한다.

        위의 과정을 반복하여 ```Fertility~Education + Catholic + Infant.Mortality + Agriculture```이 최적의 모형으로 선정되고 마지막 **Step에서 AIC가 189.86으로 계산되고 추가되지 않은 설명변수 Examination의 AIC 값이 190.96**로 나타나 해당 변수를 모형에 추가하지 않고 **최적의 모형을 ```Fertility~Education + Catholic + Infant.Mortality + Agriculture```으로 선정**했다.

    - 6단계 : 다변량 회귀분석에서 종속변수인 출산율(Fertility)에 대한 설명변수들 간의 모형에 대한 통계적 타당성을 가설 검정한다.

        귀무가설 (H<sub>0</sub>) : Education = Catholic = Infant.Mortality = Agriculture = 0

        대립가설 (H<sub>1</sub>) : 적어도 하나의 설명변수는 0이 아니다.

        F-통계량은 24.42이며 p-value 값이 1.717e-10로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형은 통계적으로 매우 유의함을 알 수 있다.

    - 7단계 : 다변량 회귀분석에서 활용된 각 설명변수들의 계수들에 대한 통계적 타당성을 가설 검정한다.

        - **첫 번째 설명변수인 Education에 대한 통계적 가설검정을 실시**한다.

            귀무가설 (H<sub>0</sub>) : Education = 0

            대립가설 (H<sub>1</sub>) : Education $\neq$ 0 

            t-통계량은 -6.617이며 p-value 값이 5.14e-08 이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 첫 번째 설명변수인 Education은 통계적으로 유의함을 알 수 있다.

        - **두 번째 설명변수인 Catholic의 경우,** t-통계량은 4.315 이며 p-value 값이 9.50e-05이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 두 번째 설명변수인 Catholic은 통계적으로 유의함을 알 수 있다.

        - **세 번째 설명변수인 Weight의 경우**, t-통계량은 2.824 이며, p-value 값이 0.00722 이므로 유의수준 5%하에서 대립가설을 채택하고, **네번째 설명변수인 Agriculture의 경우**, t-통계량은 -2.267이며, p-value 값이 0.02857 이므로 유의수준 5%하에서 대립가설을 채택하게된다. 그러므로 모든 설명변수는 통계적으로 유의함을 알 수 있다.

    - 8단계 : 통계적으로 유의성을 확인한 다변량 회귀모형이 전체 데이터를 얼마나 잘 설명하는지 확인하기 위해 결정계수 (R<sup>2</sup>)를 확인한다.

        결정계수를 확인하기 위해 Multiple R-squared:  0.6993,	Adjusted R-squared:  0.6707 로 나타났으며, 이는 전체 데이터를 설계된 다변량 회귀모형이 69.93%, 67.07%를 설명하고 있다고 해석할 수 있다.

    - 최종적으로 다변량 회귀분석 결과를 종합해 보면 추정된 다변량 회귀식은 Fertility = 62.1 - 0.98*Education + 0.13 * Catholic  + 1.08 * Infant.Mortality - 0.16 * Agriculture 이다. **Fertility 에 가장 영향을 많이 끼치는 변수는 Infant.Mortality이며 , 출산율에서 Infant.Mortality 를 가장 많이 신경 써야한다고 결론을 도출** 할 수 있다.

    



#### 주성분분석

- 주성분 분석은 주성분의 선택과 관련하여 출제가 되는 파트로 주성분 분석에 대한 R 프로그램 결과 중 **주성분 분석의 누적 기여율(cumulative proportion)과 시각화 자료인 scree plot, biplot에 대한 해석**은 정확히 숙지

- ##### 예상문제 (USArrests)

    USArrests 데이터는 미국 50개 주에서 폭행, 살인 및 강간에 대한 10만명의 주민에 대한 체포 통계 포함하는 여러개의 변수가 있다. 아래의 결과를 해석하시오

    ```R
    > # pca
    > # install.packages("factoextra")
    > # install.packages("FactoMineR")
    > library(factoextra)
    > library(FactoMineR)
         
    > pca_rslt = PCA(USArrests, graph = FALSE)
    > summary(pca_rslt)
    
    Call:
    PCA(X = USArrests, graph = FALSE) 
    
    
    Eigenvalues
                           Dim.1   Dim.2   Dim.3   Dim.4
    Variance               2.480   0.990   0.357   0.173
    % of var.             62.006  24.744   8.914   4.336
    Cumulative % of var.  62.006  86.750  95.664 100.000
    
    Individuals (the 10 first)
                    Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  
    Alabama     |  1.574 |  0.986  0.783  0.392 | -1.133  2.596  0.518 |  0.444  1.107  0.080 |
    Alaska      |  3.051 |  1.950  3.067  0.409 | -1.073  2.327  0.124 | -2.040 23.343  0.447 |
    Arizona     |  2.089 |  1.763  2.507  0.712 |  0.746  1.124  0.127 | -0.055  0.017  0.001 |
    Arkansas    |  1.149 | -0.141  0.016  0.015 | -1.120  2.534  0.950 | -0.115  0.074  0.010 |
    California  |  3.037 |  2.524  5.137  0.690 |  1.543  4.811  0.258 | -0.599  2.010  0.039 |
    Colorado    |  2.114 |  1.515  1.850  0.513 |  0.988  1.971  0.218 | -1.095  6.726  0.268 |
    Connecticut |  1.860 | -1.359  1.489  0.534 |  1.089  2.396  0.343 |  0.643  2.321  0.120 |
    Delaware    |  1.184 |  0.048  0.002  0.002 |  0.325  0.214  0.075 |  0.719  2.897  0.368 |
    Florida     |  3.070 |  3.013  7.321  0.964 | -0.039  0.003  0.000 |  0.577  1.866  0.035 |
    Georgia     |  2.366 |  1.639  2.167  0.480 | -1.279  3.305  0.292 |  0.342  0.658  0.021 |
    
    Variables
                   Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  
    Murder      |  0.844 28.719  0.712 | -0.416 17.488  0.173 |  0.204 11.644  0.042 |
    Assault     |  0.918 34.010  0.844 | -0.187  3.534  0.035 |  0.160  7.190  0.026 |
    UrbanPop    |  0.438  7.739  0.192 |  0.868 76.179  0.754 |  0.226 14.290  0.051 |
    Rape        |  0.856 29.532  0.732 |  0.166  2.800  0.028 | -0.488 66.876  0.238 |
    > # scree plot 
    > fviz_screeplot(pca_rslt, addlabels = TRUE, ylim = c(0,80))
    ```

    ![scree-plot](https://user-images.githubusercontent.com/291782/164728563-68df7929-d200-427e-a6a8-d8f9c07f96ca.png)

    

    ##### 다. 해석결과

    -   Proportion of Variance (% of var.)는 분산 비율로서 각 주성분이 차지하는 비율을 말하므로 클수록 영향도가 높다는 것을 의미
    -   Cumulative Proportion (Cumulative % of var.)는 분산의 누적합계이며, 누적기여율이 **85% 이상이면 주성분의 수로 결정**할 수 있다.
    -   위의 분석결과에서 누적기여율이 85% 이상인 **Comp.2 의 누적기여율이 87%이므로 4차원을 2차원으로 축소**할 수 있다.

    

    ##### 라. Scree Plot 해석방법

    -   scree plot이란 2차원 그래프에서 x축에는 주성분의 개수, y축에는 분산이나 고유값(eigenvalue)을 두어 **주성분 분석에서 요인의 수를 결정하기 위해 사용**된다. 이 외에도 군집분석 등의 분석에서 최적의 군집 설정 등에 사용할 수 있다. 해석방법은 y축의 값이 수평을 유지하기 전 단계로 주성분 개수를 선택한다.
    -   위의 그래프로 보면 주성분 3개째에서 그래프의 기울기가 줄어드는 형태를 보이므로 한 개를 뺀 **(3-1=2) 2개 주성분이 적합**하다.

    

    ##### 마. biplot  해석방법

    ```R
    fviz_pca_biplot(pca_rslt, reple=FALSE)
    ```

    

    ![pca-biplot](https://user-images.githubusercontent.com/291782/164730920-6e145901-c1a9-485c-b701-29e0364a4b2b.png)

    -   biplot 은 원 변수와 성분 (Comp1, 2) 간의 관계를 그래프로 표현한 것으로 그래프를 통해 각 주성분의 의미를 해석하고 각 객체들의 특성을 파악할 수 있다.
    -   **화살표는 원 변수와 Comp의 상관계수를 의미**하며, **화살표가 Comp와 평행할수록 상관관계가 크므로** 해당 **Comp에 큰 영향**을 끼친다. 그리고 **화살표가 같은 방향으로 인접해 있을수록 같은 주성분으로 생성될 수도 있음**을 알 수 있다.
    -   위의 biplot 에서 Comp1(가로축)을 기준으로 **Murder, Assault, Rape는 같은 방향으로 인접**해 있는 것을 확인할 수 있꼬, Comp2(세로축)을 기준으로 **Urbanpop은 다른 3개와 방향이 다르므로 상관 관계가 낮다.**

    -   여기서 이상치는 **Vermont, West Virginia 등은 변수 방향, 상관관계가 동떨어져 이상치**로 판정될 수 있는 데이터이다.
    -   이상치의 특성을 파악하라는 문제가 출제된다면, 위 결과에서 이상치라고 판단되는 값들 중 West Virginia는 **범죄율과 도시인구비율이 적으므로 '범죄가 없는 시골'이라고 해석하여 분석 결과를 본 미국 시민들이 그 도시로 몰릴 수**도 있다고 판단할 수 있다.



#### 시계열분석

- 시계열 분석에 대한 R 프로그램 분석 결과 중 **자기상관함수(ACF), 편자기상관함수(PACF)에 따른 모형 선택방법에 대한 해석은 정확히 숙지**해야 된다

- ##### 예상문제 (king)

    -   king 데이터는 영국왕 42명의 사망 시 나이 예제의 데이터로 비계절성을 띄는 시계열 자료이며, 트렌드 요소, 불규칙 요소로 구성되어 있다.

    ```R
    > # 시계열분석
    > king <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat", skip=3)
    Read 42 items
    > head(king)
    [1] 60 43 67 50 56 42
    > summary(king)
       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      13.00   44.00   56.00   55.29   67.75   86.00 
    > # ts를 이용하여 king 데이터를 시계열 데이터로 변환 후 king.ts 에 저장
    > king.ts <- ts(king)
    > # diff 함수를 활용해 king 데이터를 1차 차분하여 분석에 사용
    > king.ff1 <- diff(king.ts, differences = 1)
    > # acf 함수에서 
    > # lag.max는 최대 lag를 몇까지 나타낼 것인가를 지정하는 옵션
    > # plot은 ACF 그래프를 나타낼 것인가를 묻는 옵션
    > # 분석 결과에서 각 시점의 자기상관계수 값을 확인 가능
    > # ACF 그래프로 절단점과 모형을 선정 가능
    > king.acf <- acf(king.ff1, lag.max = 20, plot=TRUE)
    > king.acf
    
    Autocorrelations of series ‘king.ff1’, by lag
    
         0      1      2      3      4      5      6      7      8      9     10     11     12     13 
     1.000 -0.360 -0.162 -0.050  0.227 -0.042 -0.181  0.095  0.064 -0.116 -0.071  0.206 -0.017 -0.212 
        14     15     16     17     18     19     20 
     0.130  0.114 -0.009 -0.192  0.072  0.113 -0.093 
    ```

    ![ts-acf](https://user-images.githubusercontent.com/291782/164735304-1ee76849-4633-427e-8782-846e0abe3d85.png)

    

    ##### 다. 해석결과

    -   ACF 그래프는 자기상관함수로 계산된 시차 (1, 2, 3..)에서 **자기상관값과 시차 상관그림**을 함께 그린것으로 **이동평균(MA)모형을 선정**하기 위해 사용하는 그래프이다.
    -   ACF 그래프 확인결과, **lag 1인 지점까지는 점선 구간을 초과**하고 나머지는 점선 구간 안에 있으므로 **lag2에서 절단점**을 가지므로 **MA(1) 모형을 생성**할 수 있다.

    

    ##### 라. 예상문제 (king pacf)

    -   PACF를 통한 적합한 ARIMA 모델 결정을 위한 시계열 분석을 시행한 결과이다. 아래 결과를 해석하시오

    ```R
    > # PACF 결과해석
    > king.pacf <- pacf(king.ff1, lag.max = 20, plot=TRUE)
    > king.pacf
    
    Partial autocorrelations of series ‘king.ff1’, by lag
    
         1      2      3      4      5      6      7      8      9     10     11     12     13     14 
    -0.360 -0.335 -0.321  0.005  0.025 -0.144 -0.022 -0.007 -0.143 -0.167  0.065  0.034 -0.161  0.036 
        15     16     17     18     19     20 
     0.066  0.081 -0.005 -0.027 -0.006 -0.037 
    ```

    ![ts-pacf](https://user-images.githubusercontent.com/291782/164737335-25596406-7d9f-453f-bb35-200b79223990.png)

    

    ##### 마. 해석결과

    -   pacf 분석 결과에서 **각 시점의 편자기상관계수**(Partial autocorrelations) 값을 확인할 수 있고 PACF 그래프로 **절단점과 모형을 선정**할 수 있다.
    -   PACF 그래프는 서로 다른 두 시점 사이의 관계를 분석할 때 중간에 존재하는 값을 제외하고 나타낸 상관계수를 구하여 그린것으로 **자기회귀(AR)모형을 선정하기 위해 사용하는 그래프**이다.
    -   PACF 그래프 확인 결과, lag 3 지점까지 점선을 초과하고 나머지는 점선안에 있으므로, **lag 4에서 절단점**을 가지므로 **AR(3) 모형을 생성**할 수 있다.

    

    ##### 바. 예상문제 (king auto.arima)

    - 아래 결과를 해석하시오

        ```R
        > # auto.arima
        > # install.packages("forecast")
        > library(forecast)
        
        # auto.arima 함수 활용 시 자동으로 ARIMA 모형선정 해줌
        > auto.arima(king.ts)
        Series: king.ts 
        ARIMA(0,1,1) 
        
        Coefficients:
                  ma1
              -0.7218
        s.e.   0.1208
        
        sigma^2 = 236.2:  log likelihood = -170.06
        AIC=344.13   AICc=344.44   BIC=347.56
        
        
        > king.arima <- arima(king, order=c(0, 1, 1))
        > king.forecasts <- forecast(king.arima)
        > king.forecasts
           Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95
        43       67.75063 48.29647 87.20479 37.99806  97.50319
        44       67.75063 47.55748 87.94377 36.86788  98.63338
        45       67.75063 46.84460 88.65665 35.77762  99.72363
        46       67.75063 46.15524 89.34601 34.72333 100.77792
        47       67.75063 45.48722 90.01404 33.70168 101.79958
        48       67.75063 44.83866 90.66260 32.70979 102.79146
        49       67.75063 44.20796 91.29330 31.74523 103.75603
        50       67.75063 43.59372 91.90753 30.80583 104.69543
        51       67.75063 42.99472 92.50653 29.88974 105.61152
        52       67.75063 42.40988 93.09138 28.99529 106.50596
        ```

    

    ##### 사. 해석결과

    -   1단계 auto.arima 결과 해석
        -   auto.arima 분석결과 모형이 ARIMA(0, 1, 1)이 선정되었음
        -   시계열 모형식은 Z<sub>t</sub> = &alpha;<sub>t</sub> + 0.7218 &alpha;<sub>t-1</sub>  이다.
    -   2단계 : forecast 함수 결과 예측
        -   ARIMA(0, 1, 1) 모형 예측했을 때, 43번 ~ 52번째 왕의 사망 시 나이 예측결과는 67.75살로 추정
        -   신뢰구간은 80 ~ 95% 사이
        -   h 인자는 예측하고자 하는 개수를 'h=5' 처럼 5개로 정할 수 있다. ```forecast(king.arima, h=5)```
    -   최종적으로 분석 결과를 종합하면
        -   ACF 그래프를 통해 MA(1) 모형 생성
        -   PCAF 그래프를 통해 **AR(3) 모형** 생성
        -   auto.arima 활용하여 **ARIMA(0, 1, 1) 모형 선정**
        -   ARIMA(0, 1, 1)로 예측 결과, **43 ~ 52번째 왕의 사망 시 나이는 67.65살로 예측한다는 결론을 도출**

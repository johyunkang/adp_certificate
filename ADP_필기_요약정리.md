### 1과목 데이터의 이해

#### 데이터베이스의 특징 4가지

-   통합된 데이터, 저장된 데이터, 공용 데이터, 운영 데이터

####  

#### 빅데이터 분석에 경제성을 제공해 준 기술

-   클라우드 컴퓨팅의 보편화는 처리 비용을 획기적으로 낮춰 경제성을 제공했다.



#### 사물인터넷의 의미로 가장 적절한 것

-   모든 것의 데이터화(datafication)



#### 빅데이터 가치 산정이 어려운 이유

- 데이터 활용방식 : 재사용, 재조합, 다목적용 개발
- 새로운 가치 창출
- 분석 기술 발전



#### 빅데이터의 위기 요인과 사생활 침해

빅데이터 시대의 위기요인과 통제방안

- 사생활 침해
    - 여행 사실 트위터를 통해 빈집 강도 (익명화 기술 발전 필요)
    - 동의제에서 책임으로 : '개인정보 제공자의 동의' > '개인정보 사용자의 책임'. (사용주체의 적극적인 보호장치 강구)
- 책임 원칙 훼손
    - 마이너리티 리포트 처럼 범행 저지르기 전에 체포. 민주주의 국가의 형사 처벌은 잠재적 위협이 아닌 행동 결과에 대해 책임을 물음
    - 결과 기반 책임 원칙 고수 : 잘못된 예측 알고리즘을 통한 근거로 불이익을 줄 수 없으며, 이에 따른 피해 최소화 장치 마련
- 데이터 오용
    - 잘못된 지표를 사용하는 것도 빅데이터의 폐해. 적군 사망자 수를 전쟁의 진척 상황 지표로 활용해 전국 사망자 수가 과장돼 보고되는 경향
    - 알고리즘 접근 허용 : '알고리즘에 대한 접근권'을 제공하여 알고리즘의 부당함을 반증할 수 있는 방법 명시해 공개할 것을 주문. 알고리즈미스트가 필요하게 됨.



#### 데이터사이언스

- 의미 : 데이터 공학, 수학, 통계학, 컴퓨터공학, 시각화, 해커의 사고방식, 해당 분야의 전문지식을 종합한 학문이다. 데이터로부터 의미있는 정보를 추출해내는 학문
- 역할
    - 강력한 호기심으로 문제의 이면을 파고들고, 질문들을 찾고, 검증 가능한 가설을 세우는 능력을 의미
    - 스토리텔링, 커뮤니케이션, 창의력, 열정, 직관련, 비판적 시각, 글쓰기 능력, 대화능력 등을 갖춰야 함
- 요구역량
    - Hard Skill
        -   빅데이터에 대한 이론적 지식
        -   분석 기술에 대한 숙련
    - Soft Skill
        -   통찰력 있는 분석 : 창의적 사고, 호기심, 논리적 비판
        -   설득력 있는 전달 : 스토리텔링, 비주얼라이제이션
        -   다분양간 협력 : 커뮤니케이션



#### 빅데이터 활용의 3요소

-   데이터 : 모든 것의 데이터화
-   기술 : 진화하는 알고리즘, 인공지능
-   인력 : 데이터 사이언티스트, 알고리즈미스트



#### 사회기반구조로서의 데이터베이스

-   물류부문
    -   CVO (Commercial Vehicle Operation System, 화물운송정보)
    -   PORT-MIS (항만운영정보시스템)
    -   KROIS (철도운영정보시스템)
-   지리/교통부문
    -   GIS (Geographic Information System, 지리정보시스템)
    -   RS (Remote Sensing, 원격탐사)
    -   GPS (Global Positioning System, 범지구위치결정시스템)
    -   ITS (Intelligent Transport System, 지능형교통시스템)
    -   LBS (Location Based Service, 위치기반서비스)
    -   SIM (Spatial Information Management, 공간정보관리)
-   의료부문
    -   PACS (Picture Archiving and Communication System)
    -   U헬스 (Ubiquitous-Health)
-   교육부문
    -   NEIS (National Education Information System, 교육행정정보시스템)





#### 산업별 일차원적 분석 애플리케이션

- **금융 서비스 : 신용점수 산정, 사기 탐지, 가격 책정, 프로그램트레이딩, 클레임분석, 고객 수익성 분석**
- **병원 : 가격 책정, 고객 로열티, 수이 관리**
- **에너지 : 트레이딩, 공급/수요 예측**
- **정부 : 사기 탐지, 사례 관리, 범죄 방지, 수익 최적화**
- 제조업 : 공급사슬 최적화, 수요예측, 재고 보충, 보증서 분석, 맞춤형 상품 개발, 신상품 개발
- 소매업 : 판촉, 매대 관리, 수요 예측, 재고 보충, 가격 및 제조 최적화



### 2과목 데이터 처리 기술의 이해

#### 데이터베이스 클러스터에 대한 설명

-   MySQL 클러스터는 비공유형으로서 메모리(최근 디스크도 제공) 기반 데이터베이스의 클러스터링을 지원



####  ETL에 대한 설명 

-   ETL은 배치 프로세스 중심이며, MPP(Massive Parallel Processing)을 지원한다.



####  VMWare 관련 메모리 가상화 기법에 대한 설명

-   Transparent page sharing : 하나의 물리저인 머신에 여러 개의 가상머신이 운영되는 경우, 각 가상머신에 할당된 메모리 중 동일한 내용을 담고 있는 페이지는 물리적인 메모리 영역에 하나만 존재시키고 모든 가상 머신이 공유하도록 하는 것



#### CDC (Change Data Capture)의 구현 기법에 관한 설명

-   Log scanner on database : 로그에 대한 스캐닝 및 변경 내역에 대한 해석을 통해 CDC 메커니즘을 구현하는 기법으로 데이터베이스 스키마의 변경을 필요로 하진 않는다.
-   Version on Rows : 데이터 변경 여부에 대해 True/False의 논리값으로 표현하는 컬럼을 두는 기법으로, 레코드에 대한 변경 여부는 사람이 직접 판단할 수 없다.
-   Status on Rows : 레코드에 대한 변경 여부를 사람이 직접 판단할 수 있도록 유보하는 업무 규칙을 정할 수 있다.
-   Triggers on Tables : 데이터베이스 트리거를 활용해 사전에 등록된 다수의 대상 시스템에 변경 데이터를 배포하는 형태로 CDC를 구현하는 기법이다.
-   Time / Version / Status on Rows : 정교한 쿼리 생성에 활용할 수 있다.



#### HBase는 하둡 분산파일 시스템을 사용하며, SQL을 지원하지 않는다.



#### MapReduce 절차 (중요 **)

- Input > Split > Map > Combine > Shuffle&Sort > Reduce > Output

    ![map-reduce-word-chart-process](https://user-images.githubusercontent.com/291782/156508689-850f108f-de71-4437-8d77-b7259f638420.png)

    



#### ODS (Operation Data Store) 개념 및 특징

-   ODS (Operation Data Store)는 데이터에 대한 추가 작업을 위해 다양한 데이터 원천(Source)들로부터 데이터를 추출.통합한 데이터베이스다.
-   ODS 내의 데이터는 향후 비즈니스 지원을 위해 타 정보시스템으로 이관되거나, 다양한 보고서 생성을 위해 데이터 웨어하우스로 이관된다.
-   ODS를 위한 데이터 통합은 일반적으로 데이터 클렌징, 중복제거, 비즈니스 룰 대비 데이터 무결성 점검 등의 작업들을 포함한다.
-   실시간(Real Time) 또는 실시간 근접(Near Real Time) 트랜잭션 데이터 혹은 가격 등의 원자성(개별성)을 지닌 하위 수준 데이터들을 저장하기 위해 설계된다.
-   ODS 구성 단계 : Interface > Staging > Profiling > Cleansing > Integration > Export



#### 데이터 웨어 하우스 특징

- 주제 중심성 : 최종사용자(end user)도 이해하기 쉬운 형태를 지닌다
- 영속성, 비휘발성 : 데이터는 최초 저장 이후에는 읽기 전용 (read only)의 속성을 가지며, 삭제되지 않는다.
- 통합성 : 데이터는 기관. 조직이 보유한 대부분의 운영 시스템들에 의해 생성된 데이터들의 통합본이다
- 시계열성 : 운영 시스템들은 최신 데이터를 보유하고 있지만. 데이터 웨어하우스는 시간 순에 의한 이력 데이터를 보유한다.



#### 맵리듀스(MapReduce) 크기

320MB 파일을 별도 옵션을 지정하지 않고 작업 수행하면 몇개의 맵 태스크(Task)가 생성되는가?

- 맵리듀스 블록 크기 기본값 : 64MB
- 맵태스크 하나가 하나의 블록을 대상으로 연산을 수행
- 320 / 64 = 5



#### 하둡분산파일시스템 (HDFS) 특징

- 데이터의 랜덤 접근 방식을 지원하지 않음. 순차 접근 방식을 지원
- 구글 파일 시스템의 아키텍처와 사상을 그대로 구현한 클로닝(Cloning) 프로젝트
- 파일 데이터를 블록이나 청크 단위로 저장함
- 기본적으로 파일은 한 번 쓰이면 변경되지 않는다고 가정함
- 
- 파일 데이터는 블록(또는 청크) 단위로 나뉘어 여러 데이터노드에 분산.복제.저장된다.
- 낮은 데이터 접근지연시간보다는 높은 데이터 처리량에 중점을 두고 있음
- 하나의 네임도드(NameNode), 다수의 데이터노드(DataNode)로 구성됨
    - 네임노드 : 모든 메타데이터를 관리, 마스터/슬레이브 구조에서 마스터 역할. 클라이언트로 부터 파일 접근 요청을 처리
    - 데이터노드 : 슬레이브 노드, 데이터 입출력 요청을 처리. 데이터 유실 방지를 위해 3중 복제하여 저장.
    - 보조네임노드 : HDFS 상태 모니터링을 보조. 주기적으로 네임 노드의 이미지를 스냅샷해 저장



#### 아파치 하이브(hive) 특징

- 페이스북에서 개발한 데이터 웨어하우징 인프라로 아파치 내의 하둡 서브 프로젝트로 등록돼 개발되고 있다.
- Pig와 마찬가지로 하둡 플랫폼 위에서 동작
- SQL 기반의 쿼리 언어와 JDBC를 지원
- 하둡 병렬처리 기능인 Hadoop-Streaming을 쿼리 내부에 삽입해 사용할 수 있다
- 아파치 하이브는 **맵리듀스의 모든 기능을 지원**



#### SQL on 하둡

- 실시간 SQL 질의 분석 기술이다.
- 하둡 프레임워크의 맵리듀스를 이용하지 않고, 새로운 분산 처리 모델과 프레임워크를 기반으로 구현되어 있다.
- SQL on 하둡의 한 종류인 샤크(Shark)는 인메모리 기반의 대용량 데이터웨어하우스 시스템이다.
- 실시간 SQL 질의 분석 기술이다
- 클라우데라 임팔라, 아파치 타조는 대표적인 상용 SQL on 하둡 솔루션이다.



#### 임팔라 (SQL on Hadoop(하둡)의 질의 엔진)

- 분석과 트랜잭션 처리를 모두 지원하는 것을 목표로 만들어진 SQL 질의 엔진

- 하둡과 HBase에 저장된 데이터를 대상으로 SQL 질의 가능

- 고성능을 위해 C++언어를 사용하였음

- 맵리듀스를 사용하지 않고 실행 중에 최적화된 코드를 생성해 데이터를 처리

- 하이브(Hive)가 하둡에 저장된 다양한 형태의 비정형 데이터를 처리하는 표준 SQL솔루션으로 사용되고 있지만, 더 빠른 처리가 필요한 비즈니스 요구사항 때문에 임팔라와 같은 기술이 대두되고 있다.

    ![impala-arch](https://user-images.githubusercontent.com/291782/168068690-c75b20ee-e8a4-4191-8dc7-09a3c6d0b799.png)





#### 데이터베이스 클러스터

- 하나의 DB를 여러 개의 서버(또는 가상 서버) 상에 구축하는 것을 의미
- DB 파티셔닝은 DB를 여러 부분으로 분할하는 것을 의미하며, 분할된 각 요소는 파티션이라고 한다.
- 각 파티션은 여러 노드로 분할 배치되어 여러 사용자가 각 노드에서 트랜잭션을 수행할 수 있다.
- 데이터를 통합할 때, **성능과 가용성의 향상을 위해 DB 차원의 파티셔닝 또는 클러스터링을 이용**한다.
- Oracle RAC(Real Application Cluster)를 제외한 대부분의 데이터베이스 클러스터가 무공유 방식을 채택하고 있다. Oracle RAC만 공유방식 채택
- MySQL 클러스터 : 클러스터에 참여하는 노드(SQL노드, 데이터 노드, 매니저 포함)수는 255개로 제한. 데이터 노드는 최대 48개 까지만



#### DB 파티셔닝 구현 효과

- 병렬처리, 고가용성, 성능향상



#### NoSQL 개념 및 특징

- 분산 데이터베이스 기술로 확장성, 가용성, 높은 성능을 제공
- 비관계형 데이터베이스 관리 시스템. (Not Only SQL)
- 저장되는 데이터 구종에 따라 key-value 모델, Document 모델(JSON, XML 데이터 구조 채택), Graph 모델, Column 모델로 구분됨
- NoSQL은 key value 형태로 자료를 저장하고 빠르게 조회할 수 있는 구조를 제공
- 스키마 없이 동작, 복잡한 join 연산은 지원하지 않음
- 종류는 구글 BigTable, 아파치 HBase, 아마존 SimpleDB, MS의 SSDS 등

#### Oracle RAC 데이터베이스 서버의 장점

- 가용성 : 높은 수준의 폴트 톨러런스(Fault tolerance)를 제공하므로, 하나의 노드만 살아 있어도 서비스가 가능
- 확장성 : 추가 성능이 필요하면 응용 프로그램이나 테이터베이스를 수정할 필요 없이 새 노드를 클러스터에 쉽게 추가할 수 있다. 10g R2 RAC 기준 최대 100개 노드 지원
- 비용 절감 : 저가형 상용 HW 클러스터에서도 고가의 SMP 시스템 만큼 효율적으로 응용 프로그램을 실행함
- 단, 도입 비용 때문에 확장성이 중요한 데이터보다는 고가용성을 요구하는 데이터에 많이 사용



#### 하둡(hadoop) 에코시스템에 사용되는 기술의 내용과 종류

- 정형 데이터 수집 : Sqoop, hiho
- 비정형 데이터 수집 : Flume-NG, kafka, Chuckwa, Scribe
- 대용량 SQL 질의 기술 : Hive, Pig
- 워크플로우 관리 : Oozie, Azkaban
- 실시간 SQL 질의 기술 : Impla, Tajo

![hadoop-eco](https://user-images.githubusercontent.com/291782/168072330-40a88b9b-d978-40d8-a979-d7d1cc932851.png)





#### 클라우드 컴퓨팅의 개념 및 특징

- 동적으로 확장할 수 있는 가상화 자원들을 인터넷으로 서비스하는 기술을 의미

- IaaS (Infrastructure as a Service), SaaS (Sw as a Service), Paas (Platform as a Service) 3유형으로 나뉨

    > IaaS : 네트워크 장비, 서버, 스토리지 등 IT 인프라 자원을 빌려주는 클라우드 서비스
    >
    > SaaS : 소프트웨어를 웹에서 사용할 수 있게 해주는 서비스
    >
    > Paas : 애플리케이션이나 SW 개발 및 구현 시 필요한 플랫폼을 제공하는 서비스

- 서버 가상화 기술 : VMware, Xen, KVM 등의 서버 가상화 기술은 IaaS에 주로 활용

- 아마존의 EMR (Electric Map Reduce)은 하둡을 온디맨드로 이용할 수 있는 플랫폼 가상화 서비스



#### 하이퍼바이저(Hypervisor) 개념 및 특징 (CPU 가상화)

- 호스트 컴퓨터에서 다수의 운영 체제를 동시에 실행하도록 하기 위한 논리적인 플랫폼을 의미
- 일반적으로 가상머신을 하이퍼바이저라고 할 수 있음



#### CPU 가상화 방식의 분류

- 완전 가상화
    - VMware ESX server, MS Virtual Server 등이 완전 가상화 솔루션
    - 장점
        - CPU, 메모리, 네트워크 장치 등 모든 자원을 하이퍼바이저가 직접 제어.관리
        - 어떠한 운영체제라도 수정하지 않고 설치 가능
    - 단점
        - 하이퍼바이저가 자원을 직접 제어하기 때문에 성능에 영향을 미침
        - 운영중인 게스트 OS에서 할당된 CPU, 메모리 등의 자원에 대한 동적변경이 단일 서버내에서는 어려움
        - 자원 동적변경을 위해서는 VMware의 VMotion 과 같은 솔루션의 도움이 필요
- 반가상화
    - privileged 명령어를 게스트 OS에서 hypercall로 하이퍼바이저에 전달하고, 하이퍼바이저는 hypercall에 대해서 privileged 레벨에 상관없이 HW로 명령을 수행 시킴
    - CPU, 메모리 등의 자원에 대한 동적 변경이 서비스 중단 없이 가능
    - 완전 가상화에 비해 성능이 뛰어남
    - 반가상화는 커널 변경이 필요하고, 완전가상화는 커널 변경이 필요없다



#### 메모리 가상화 : VMware 기법

개념 및 특징

-   메모리 관리를 위해 물리주소(Physical Address)와 가상주소 (Virtual Address)를 사용함
    -   물리주소 : 0부터 시작해 실제 물리적인 메모리 크기까지를 나타냄
    -   가상주소 : 하나의 프로세스가 가리킬 수 있는 최대 크기를 의미하며 32비트 OS에서는 4GB 까지 가능
-   프로그램에서의 주소는 물리적인 주소가 아닌 가상주소 값
-   따라서 가상주소값의 위치(VPN, Virtual Page Number)를 실제 물리적인 주소 값 위치 (MPN, Machine Page Number)로 매핑하는 과정이 피료하며 page table을 이용
-   매핑 연산을 하드웨어적으로 도와주는 것을 TLB (Translation Lookaside Buffer)라고 함
-   VMware 하이퍼바이저의 핵심 모듈은 VMkernel

가상머신 메모리 할당의 문제 해결을 위한 방법

-   memory ballooning : 가상머신 메모리 영역을 빈 값으로 강제로 채워 가상머신 OS가 자체적으로 swapping 하도록 함
-   transparent page sharing : 동일한 내용을 담고 있는 페이지는 물리적인 메모리 영역에 하나만 존재시키고 모든 가상머신이 공유하도록 함
-   memory overcommitment : 위의 두 가지 기법을 이용하여 가능하지만, 심각한 성능저하 때문에 권장하지 않음



#### I/O 가상화

-   가상 이더넷, 공유 이더넷 어댑터, 가상 디스크 어댑터



#### EAI 와 ESB 비교

EAI (Enterprise Application Integration), ESB (Enterprise Service Bus)

-   기능
    -   EAI : 미들웨어(Hub)를 이용하여 비즈니스 로직을 중심으로 Application을 통합, 연계
    -   ESB : 미들웨어(Bus)를 이용하여 서비스 중심으로 시스템을 유기적으로 연계
-   통합관점
    -   EAI : Application
    -   ESB : Process
-   로직연동
    -   EAI : 개별 Application에서 수행
    -   ESB : ESB에서 수행
-   아키텍처
    -   EAI : 단일 접점인 허브시스템을 이용한 중앙집중식 연결구조
    -   ESB : 버스(Bus)형태의 느슨하고 유연한 연결구조



### 3과목 데이터 분석 기획

#### 하향식 접근법 : 문제탐색 > 문제정의 > 해결방안 탐색 > 타당성 검토로 전개



#### 고객니즈 변화에 대항하는 것 : 고객, 채널, 영향자들에 의해 진행



#### 분석 프로젝트 영역별 주요 관리 항목

-   범위, 시간, 원가, 품질, 통합, 조달, 자원, 리스트, 의사소통, 이해관계자 등



#### 분석과제 관리 프로세스에 대한 설명

-   분석과제 중에 발생된 시사점과 분석 결과물이 풀(pool)로 관리되고 공유
-   확정된 분석과제는 풀(pool)로 관리하지 않는다.



#### 데이터 분석 3가지 조직 구조

![anal-part](https://user-images.githubusercontent.com/291782/161255029-34d00daa-7141-43ba-a159-b2745a087e2f.png)

-   집중구조, 기능구조, 분산구조



#### CRISP-DM 분석 방법론

- 개요 : 1996년 유럽연합의 **주요한 5개의 업체들**의 주도하에  4개 레벨로 구성된 **계층적 프로세스 모델**로 만들어짐

- ![crisp-dm-4level](https://user-images.githubusercontent.com/291782/160156808-1e170816-8be3-4f5e-a5ae-e3b41c2d692c.png)

- 최상위 레벨은 여러 개의 단계(Phase)로 구성되고 각 단계는 일반화 태스크(Generic Tasks)를 포함

- 일반화 태스크는 데이터마이닝의 단일 프로세스를 완전하게 수행하는 단위이며, 이는 다시 구체적인 수행 레벨인 세분화 태스크(Specialized Tasks)로 구성된다.

- 예를 들어 데이터 정체 (Data Clensing)라는 일반화 태스크는 범주형 데이터 정제와 연속형 데이터 정제와 같은 세분화 태스크로 구성

- 마지막 레벨인 프로세스 실행 (Process instances)은 데이터마이닝을 위한 구체적인 실행을 포함한다.

- 6단계 프로세스 : 각 단계는 단방향 구성이 아닌, **단계 간 피드백**을 통해 단계별 완성도를 높이게 되어 있음

    ![crisp-dm-6phase](https://user-images.githubusercontent.com/291782/160157935-b0f7e9cc-c7db-4a58-92f1-aadb6b18b98f.png)

    -   1단계 : 업무이해 : 프로젝트의 목적과 요구사항을 이해하는 단계. 업무 목적 파악, 상황 파악, 목표 설정, 프로젝트 계획 수립
    -   2단계 : 데이터 이해 : 데이터를 수집하고, 데이터 속성을 이해하기 위한 단계. 데이터 수집, 데이터 기술분석, 데이터 품질 확인
    -   3단계 : 데이터 준비 : 분석 기법에 적합한 데이터를 편성하는 단계. 데이터셋 선택, 데이터 정제, 데이터 통합, 데이터 포맷팅
    -   4단계 : 모델링 : 모델링 기법과 알고리즘 선택하고 파라미터를 최적화 하는 단계. 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가
    -   5단계 : 평가 : 모델링 결과가 프로젝트 목적에 부합하는지 평가하는 단계. 분석결과 평가, 모델링 과정 평가, 모델 적용성 평가
    -   6단계 : 전개 : 모델을 실 업무에 적용하기 위한 계획 수립 단계. 전개 계획 수립, 유지보수 계획 수립, 종료보고서 작성 및 프로젝트 리뷰



####  KDD 와 CRISP-DM 의 비교

|                         KDD                          |              CRISP-DM              |
| :--------------------------------------------------: | :--------------------------------: |
|                분석대상 비즈니스 이해                | 업무이해 (Business Understanding)  |
|            데이터셋 선택 (Data Selection)            | 데이터의 이해 (Data Understanding) |
|            데이터 전처리 (Preprocessing)             | 데이터의 이해 (Data Understanding) |
|             데이터 변환 (Transformation)             |   데이터 준비 (Data Preparation)   |
|             데이터 마이닝 (Data Mining)              |         모델링 (Modeling)          |
| 데이터 마이닝 결과 평가 (Interpretation/ Evaluation) |         평가 (Evaluation)          |
|                  데이터 마이닝 활용                  |         전개 (Deployment)          |



#### KDD(Knowledge Discovery in DB) 분석 절차

1. 데이터셋 선택(Selection)
    - 분석 대상의 **비즈니스 도메인에 대한 이해**와 **프로젝트 목표 설정**이 필수
    - 분석에 필요한 데이터를 선택하는 단계
    - **목표 데이터**(target data)를 구성하여 분석에 활용
2. 데이터 전처리 (preprocessing)
    - **잡음(noise), 이상치(outlier), 결측치(missing value)를 식별**하거나 **제거**하여 데이터셋을 정제하는 단계
    - **추가로 요구되는 데이터 셋**이 필요한 경우 선택 프로세스를 재실행 함
3. 데이터 변환 (transformation)
    - 분석 목적에 맞게 변수를 생성, 선택하고 **데이터의 차원을 축소**하여 효율적으로 데이터마이닝을 할 수 있도록 변경하는 단계
    - **학습용 데이터**(training data)와 **검증용 데이터**(test data)로 분리하는 단계
4. 데이터 마이닝 (data mining)
    - 학습용 데이터를 이용하여 **데이터 마이닝 기법을 선택**하고, 적절한 알고리즘을 적용하는 단계
    - 필요에 따라 **전처리**와 **변환** 단계를 **추가로 실행**하여 최적의 결과를 산출



#### 분석과제 관리를 위한 5가지 주요 영역

-   Data Size : **분석하고자 하는 데이터의 양**을 고려한 관리 방안 수립이 필요. 하둡 환경에서의 엄청난 데이터를 분석하는 것과, 기존 정형 데이터베이스에 있는 시간 당 생성되는 데이터를 분석할 떄의 관리 방식은 차이가 날 수 밖에 없다.
-   Data Complexity : 정형 데이터 분석과 달리, 텍스트, 오디오, 비디오 등의 비정형 데이터 분석 프로젝트를 진행 할때는 초기 데이터의 확보와 통합뿐 아니라 해당 데이터에 **잘 적용될 수 있는 분석 모델의 선정** 등에 대한 사전 고려가 필요
-   Speed : 분석 결과가 도출되었을 때 이를 활용하는 **시나리오 측면에서의 속도**를 고려해야 한다. 사기(Fraud) 및 고객에게 개인화된 상품 서비스를 추천하는 경우에는 분석 모델의 적용 및 계산이 실시간으로 수행되어야 하기 때문에 프로젝트 수행 시 **분석 모델의 성능 및 속도를 고려한 개발** 및 테스트가 수행되어야 한다.
-   Analytic Complexity : 분석 모델의 정확도와 복잡도는 트레이드 오프 관계가 존재한다. **해석이 가능하면서도 정확도를 올릴 수 있는 최적모델**을 찾는 방안을 사전에 모색해야 한다.
-   Accuracy & Precision : **Accuracy**(정확)는 모델과 실제 값 사이의 차이가 적다는 **정확도**를 의미하고, **Precision**(정확, 정밀)은 모델을 지속적으로 반복했을 때의 편차의 수준으로써 **일관적**으로 동일한 결과를 제시한다는 것을 의미한다. 분석의 활용 측면에서는 Accuracy가 중요, 안정성 측면에서는 Precision이 중요하다.둘은 트레이드 오프 관계라 해석 및 적용 시 사전에 고려해야 함.

![accuracy-precision](https://user-images.githubusercontent.com/291782/160412734-09e83402-8824-470b-811d-b97e7f62f477.png)



#### 거버넌스 체계의 구성요소

-   분석기획 및 관리 수행 조직 (Organization)
-   과제 기획 및 운영 프로세스 (Process)
-   분석관련 시스템 (System)
-   데이터 (Data)
-   분석교육 / 마인드 육성체계 (Human Resource)



#### 데이터 거버넌스 구성요소

-   원칙(Principle) : 데이터 유지, 관리하기 위한 지침 가이드, 보안, 품질기준
-   조직(Organization) : 데이터 관리할 조직의 역할과 책임. 데이터 관리자, DB 관리자, DA
-   프로세스(Process) : 데이터 관리를 위한 활동과 체계



#### 데이터 거버넌스 체계

-   데이터 표준화 : 데이터 표준 용어 설정, 명명규칙, 메타 데이터, 데이터 사전 구축 등 업무
-   데이터 관리 체계 : 정합성 및 활용의 효율성, 메타 데이터와 데이터 사전의 관리 원칙을 수립. 항목별 상세한 프로세스 만들기. 데이터 생명주기 관리
-   데이터 저장소 관리(repository) : 메타 데이터 및 데이터를 관리하기 위한 전사 차원의 저장소를 구성. 워크플로우 및 관리용 응용 소프트웨어(application)를 지원
-   표준화 활동 : 데이터 거버넌스 체계 구축 후 표준 준수 여부를 주기적으로 모니터링. 계속적인 변화관리 및 주기적인 교육 진행. 지속적인 데이터 표준화 개선 활동







#### 데이터 분석 수준 진단

![analysis-quardrant](https://user-images.githubusercontent.com/291782/160861368-444b90ba-c7a6-4477-bb12-c2d946373ce7.png)



#### 분석 과제 발굴 방법

![topdown-bottomup](https://user-images.githubusercontent.com/291782/160286214-b1aa8be8-4dc3-4f86-9e24-1df4d492593c.png)



#### 비즈니스 모델 캔버스를 활용한 과제 발굴 방법 5가지

-   업무 (Operation) : 내부 프로세스 및 주요자원 (Resource)관련 주제 도출 (예. 생상 공정 최적화, 재고량 최소화)
-   제품 (Product) : 제품, 서비스를 개선하기 위한 관련 주제 도출 (예. 제품의 주요기능 개선, 서비스 모니터링 지표 도출)
-   고객 (Customer) : 제공하는 채널의 관점에서 관련 주제 도출 (예. 고객 Call 대기 시간 최소화, 영업점 위치 최적화)
-   규제와 감사 (Regualtion & Audit) : 규제 및 보안의 관점에서 주제 도출 (예. 제공 서비스 품질의 이상 징후 관리, 새로운 환경 규제 시 예상 되는 제품 추출 등)
-   지원 인프라 (IT & Human Resources) : 시스템 영역 및 이를 운영 관리하는 **인력의 관점**에서 주제 도출 (예. EDW 최적화, 적정 운영 인력 도출 등)



#### 데이터에 기반한 의사결정 방해 요소

- 고정 관념, 편향된 생각, 프레이밍 효과



#### 목표시점별 분석기 기획방안

- 과제 중심적인 접근 방식 : 당면한 과제를 빠르게 해결

- 장기적인 마스터 플랜 방식 : 지속적인 분석 내재화를 위함

- |             | 당면한 분석 주제의 해결(과제 단위) | 지속적 분석 문화 내재화(마스터 플랜) |
    | ----------- | ---------------------------------- | ------------------------------------ |
    | 1차 목표    | Speed & Test                       | Accuracy & Deploy                    |
    | 과제의 유형 | Quick & Win                        | Long Term View                       |
    | 접근 방식   | Problem Solving                    | Problem Definition                   |



#### ROI 관점에서 빅데이터의 핵심 특징

- 3V (Volume, Variety, Velocity) 난이도 : 데이터 규모/양, 데이터 종류/유형, 데이터 생성속도/처리속도 => 투자비용 요소 (Investment)

- 4V (3V + Value(가치)) 시급성 : 분석 결과 활용 및 실행을 통한 비즈니스 가치 => 비즈니스 효과 (Return)

- 시급성 : 시급성의 판단 기준은 전략적 중요도가 핵심

- 포트폴리오 사분면 분석을 통한 과제 우선순위 선정

    ![pflo_4_layer](https://user-images.githubusercontent.com/291782/160415872-abf3a83b-f41d-42ae-9a34-b4f0d357843d.png)

    -   분석과제의 우선순위를 '**시급성**'에 둔다면 3 > 4 > 2 영역 순, '**난이도**'에 둔다면 3 > 1 > 2 영역순으로 의사결정



#### 분석기회발굴의 범위확장

-   거시적 관점의 메가 트랜드 : Social(사회), Technological(기술), Economic(경제), Environment(환경), Political(정치)
-   경쟁자 확대 관점 : 대체재(substitute), 경쟁자(competitor), 신규진입자(New entrant)
-   시장의 니즈 탐색 관점 : 고객(customer), 채널(channel), 영향자(Influencer)
-   역량의 재해석 관점 : 내부 역량, 파트너와 네트워크



### 4과목 데이터 분석

#### 34. 모분산의 추론에 대한 설명

-   F-분포 : 이표본에 의한 분산비 검정은 두 표본의 **분산이 동일한지를 비교**하는 검정
-   모분산이 추론의 대상이 되는 경우는 모집단의 변동성 또는 퍼짐의 정도에 관심이 있을 때이다.
-   모집단이 정규분포를 따르지 않더라도 중심극한 정리를 통해 정규 모집단으로부터의 모분산에 대한 검정을 유사하게 시행할 수 있다.
-   X<sup>2</sup>(카이제곱) : 평균모집단에서 n개를 단순임의 추출한 **표본의 분산**은 자유도가 n-1인 카이제곱 분포를 따른다. **두 집단간의 동질성 검정에 활용**
-   t-분포 : **두 집단의 평균이 동일**한지를 알고자 할 때 검정통계량으로 활용



#### 이산형 확률분포, 연속형 확률분포

가. 이산형 확률분포

- 확률 변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포, 확률값은 확률질량함수를 이용하여 계산

> $P(X_i) > 0$      $  i=1,2,...,k$      $\displaystyle \sum_{i=1}^kP(X_i)=1$

- 이산형 확류변수 예시 : 동전 2개를 던져서 앞/뒷면이 나오는 경우의 수(H:앞, T:뒤)

    | 표본공간(&Omega;) | HH(사건) | HT   | TH   | TT   | 합계 |
    | ----------------- | -------- | ---- | ---- | ---- | ---- |
    | P(x)              | 1/4      | 1/4  | 1/4  | 1/4  | 1    |

    

1) 베르누이  확률분포(Bernoulli distribution)

- 결과가 2개만 나오는 경우 (ex. 동전 던지기, 시험의 합/불합격 등)

> $P(X=x)=p^x(1-p)^{1-x}$  (x=1 or 0), E(x) = p,  var(x)=p(1-p)
>
> 예) 메이저리거인 추추가 안타를 칠 확률은 베르누이 분포를 따름. (안타를 치는 사건을 x=1이라고 할 때 안타를 칠 확률은 타율로 적용 가능)



2) 이항분포(Binomial distribution)

- 베르누이 시행을 n번 반복했을 때 k번 성공할 확률
- $P(X = k) = _nC_kP^k(1-p)^{n-k}, \quad  _nC_k = \dfrac {n!} {k!(n-k)!}$
- 한 축구 선수가 페널티킥을 차면 5번 중 4번은 성공한다고 한다. 그럼 이 선수가 10번의 페널티킥을 차서 7번 성공할 확률은?
- 5번 중 4번 성공하기에 성공확률은 4/5 = 0.8, 실패확률은 1-0.8 = 0.2
- $이항분포 = \begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} =  \begin{pmatrix} 10 \\ 7 \end{pmatrix} 0.8^70.2^3$
- $ = \dfrac {10!} {7! \times 3!} 0.8^7 0.2^3 = 0.2013 = 20.13\%$



3) 기하분포(Geometric distribution)

- 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- $p(x) = p(1-p)^{x-1}$
- 어느 야구선수가 홈런을 칠 확률은 0.05라고한다. 이 선수가 6번째 타석에서 홈런을 칠 확률은?
- 성공확률 p=0.05, 실패확률은 1 - 0.05 = 0.95, 6번째 타석에서 성공할 확률이기에 x-1 = 6-1
- $p(1 - p)^{x-1} = 0.05 \times 0.95^{6-1} = 0.0387 = 3.87\%$



4) 다항분포(Multinomial distribution)

- 이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포

- 각 상황의 확률과 각 상황의 횟수를 잘 파악해야 함

- $p(x) = \dfrac {n!} {x_1!x_2! ...x_k!}p1^{x_1}p_2^{x_2}... p_k^{x_k} $

- 국내 인터넷 포털 사이트의 점유율은 아래와 같다.  12명을 임의로 뽑아 사용 사이트를 알아보았을 때, 네이버 7명, 구글 3명, 다음과 ZUM이 각 1명, 기타는 0명이 사용할 확률을 구하시오

    네이버 : 61%, 구글 : 30%, 다음 : 7%, ZUM: 1%, 기타 : 1%

- $\dfrac {12!} {7! \times 3! \times 1! \times 1! \times 0!} \times 0.61^7 \times 0.3^3 \times 0.07^1 \times 0.01^1 \times 0.01^0 = 0.0094$

    ​      

    ​      

5) 포아송분포 (Poisson distribution)

- 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 (예. 가게에 손님이 1시간에 20명씩 방문한다고 할 때, 10분에 손님이 5명씩 방문할 확률)
- 확률을 구하기 위해서는 **평균(&lambda;)과 발생횟수(x)**를 잘 파악해야 함
- $p(x) = \dfrac {e^{-\lambda} \lambda^x} {x!} , \quad e=2.718281...$
- 전공 책 5페이지를 검사했는데, 오타가 총 10개가 발견되었다고 한다. 그럼 이 책에서 어느 한 페이지를 검사하였을 때, 오타가 3개 나올 확률을 구하시오?
- 해설) 포아송 분포는 평균을 잘 구해야함. 문제에 말장난이 섞여 있음. 일단 5페이지에 총 10개의 오타이므로, 1페이지에 평균 2개의 오타가 발견된 셈. 그래서 평균 (&lambda;)=2 이다. 그리고 발생횟수 x=3 이므로...
- $\dfrac {2.718281^{-2} \times 2^3} {3!} = 0.1804$



##### 나. 연속형 확률분포

- 확률 변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포이며, 확률값은 확률밀도함수를 이용하여 계산한다.

1) 균일분포 (일양분포, Uniform distribution)

- 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)



2) 정규분포 (Normal distribution)

- 평균이 &mu;이고, 표준편차가 &sigma;인 X의 확률밀도함수
- 표준편차가 클 경우 퍼져보이는 그래프가 나타난다.



3) 지수분포 (Exponential distribution)

- 어떤 사건이 발생할 때까지 경과한 시간에 대한 연속확률분포이다. (예. 전자렌지의 수명시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간)



4) t-분포 (t-distiribution)

- 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
- 표본의 크기가 적을때는 표준정규분포를 위에서 눌러 놓은 것과 같은 형태를 보이지만 표본이 커져서(30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
- 데이터가 연속형일 경우 활용한다.
- **두 집단의 평균이 동일**한지 알고자 할 때 검정통계량으로 활용된다.
- 표준정규분포와 같이 평균 값이 0이며, 자유도에 따라 분포의 모양이 변화한다.
- 자유도가 30미만인 경우, 표준정규분포에 비해 양쪽 끝이 평평하고 두터운 꼬리 모양을 가진다.



5) X<sup>2</sup>-분포 (X<sup>2</sup>-distribution) (카이제곱분포)

- 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포이다.
- **두 집단 간의 동질성 검정에 활용**된다.
- 확률변수 X가 표준정규분포(Z)를 따를 때, 자유도가 k인 카이제곱분포를 따른다. 자유도는 표본 자료 중 모집단에 대한 정보를 주는 독립적인 표본 자료의 수와 같으며, 분할표에서의 행과 열의 개수를 통해 구할 수 있다. (자유도(df) = (r-1)(c-1), r=행의 개수, c=열의 개수)



6) F-분포 (F-distribution)

- **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량의 분포이다.
- 확률변수는 항상 양의 값만을 갖고 카이제곱분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.



#### R에서 사용 가능한 데이터 오브젝트

-   벡터 : 모든 원소는 같은 모드여야 한다.
-   리스트 : 원소들은 다른 모드여도 상관 없다
-   행렬 : 차원을 가진 벡터
-   데이터프레임 : 테이블로 된 데이터 구조로써 리스트 구조로 구현된다.



#### 벡터 및 sequence 표현 방법

```R
> c(1, 10)
[1]  1 10

> c(1:10)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10, 1)
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(1, 10, 2)
[1] 1 3 5 7 9

> 1:10
 [1]  1  2  3  4  5  6  7  8  9 10

> seq(10, 100, 10)/10
 [1]  1  2  3  4  5  6  7  8  9 10
```



#### 종속변수를 설명하는 가장 중요한 독립변수

-   추정한 계수가 클수록 종속변수에 가장 많은 영향을 미친다.
-   특히 &beta;0 가 없는 표준화된 추정식을 만들게 되면 각 계수의 크기를 더욱 정확히 알 수 있게 된다.



#### 확률분포 종류

-   이산형 확률변수 : 베르누이 확률분포, 이항분포, 기하분포, 다항분포, 포아송분포
-   연속형 확률변수 : 균일분포(일양분포, Uniform distribution), 정규분포, 지수분포, t-분포, X2 분포, F-분포

1. 이산형 확률변수

    -   베르누이 확률분포 (Bernoulli distribution)

        -   결과가 2개만 나오는 경우 **성공 또는 실패** (예. 동전 던기지, 시험의 합격/불합격 등)
        -   $P(X = x) = P^x . (1-p)$<sup>1-x</sup> 
        -   (x= 1 or 0), 기댓값: $E(x) = p$, 분산 :$var(x) = p(1-p)$
        -   예) 추신수가 안타를 칠 확률은 베르누이 분포를 따른다.
    -   이항분포 (Binomial distribution)

        -   베르누이 시행을 n번 반복했을 때 k번 성공할 확률
        -   n번 시행 중에 각 시행의 확률이 p일 때, k번 성공할 확률분포
        -   $P(X = k) = _nC_kP^k(1-p)$<sup>n-k</sup> , $_nC_k = \dfrac {n!}{k!(n-k)!}$
        -   기댓값 : $E(X) = np$, 분산 : $V(X) = np(1-p) $  (단, n과 k가 1이면 베르누이 시행)
        -   추신수가 오늘 경기에서 5번 타석에 들어와서 3번 안타를 칠 확률은 이항분포를 따른다. (n=5, k=3, 안타를 칠 확률 P(x) = 타율로 적용 가능)
        -   성공할 확률 p가 0이나 1에 가깝지 않고 n이 충분히 크면 이항분포는 정규분포에 가까워 진다. 성공할 확률 p가 1/2에 가까우면 종모양이 된다.
    -   기하분포 (Geometric distribution)
        -   성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 X번 실패할 확률
        -   예) 추신수가 오늘 경기에서 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률은 기하분포를 따른다.
    -   다항분포 (Multinomial distribution)
        -   이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포
    -   포아송분포 (Poisson distiribution)
        -   시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
        -   예) 책에 오타가 5page 당 10개씩 나온다고 할 떄, 한 페이지에 오타가 3개 나올 확률, 추신수가 최근 5경기에서 홈런을 쳤을 경우, 오늘 경기에서 홈런을 못 칠 확률은 포아송 분포
        -   &lambda; (람다) = 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값, y= 사건이 일어난 수
        -   $P = \dfrac {\lambda^ne^{-\lambda}} {n!}$ (e는 자연상수)
        -   기댓값 : $E(X) = \lambda$, 분산 : $V(X) = \lambda $

2. 연속형 확률변수

    - 가능한 값이 실수의 어느 특정구간 전체에 해당하는 확률변수 (확률밀도함수)

    - $ f(x)\ge 0 $     $\int_{-\infty}^{\infty}f(x)dx = 1$

    - 균일분포 (일양분포, Uniform distiribution)

        - 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
        - $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
        - ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)

    - 정규분포 (Normal distribution)

        - 평균이 &mu; (뮤) 이고, 표준편차가 &sigma; (시그마) 인 X의 확률밀도 함수
        - 표준편차가 클 경우 퍼져보이는 그래프가 나타남
        - 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
        - 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
        - ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)

    - 지수분포 (Exponential distribution)

        - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포이다.
        - 예) 전자렌지의 수명시간, 콜센터에 전화가 걸려올때 까지의 시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간
        - ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)

    - t분포 (t-distribution)

        - 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
        - 표본이 커져서 (30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
        - 데이터가 연속형일 경우 활용한다.
        - **두 집단의 평균이 동일한지** 알고자 할 때 검정통계량으로 활용된다.
        - ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)

    - X<sup>2</sup>-분포 (chi-square distribution, 카이제곱분포)

        - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포
        - **두 집단 간의 동질성 검정에 활용**된다. (범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용)
        - ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)

    - F-분포 (F-distribution)

        - **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량 분포
        - 확률변수는 항상 양의 값만을 갖고 X<sup>2</sup> 분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
        - ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)

        

#### 분해시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

    > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
    >
    > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
    >
    > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
    >
    > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인





#### 데이터마이닝의 분석방법

- 지도학습 (Supervisied Learning)
    - 의사결정나무 (DT), 인공신경망 (Artifician Neural Network), 일반화 선형 모형 (GLM, Generalized Linear Model)
    - 회귀분석 (regression analysis), 로지스틱 회귀분석 (logistic regression analysis), 사례기반 추론 (case-based reasoning), 최근접이웃법 (KNN)
- 비지도학습 (Unsupervised Learning)
    - OLAP (On-Line Analytical Processing), 연관성 규칙발견 (Association Rule Discovery, Market Basket)
    - 군집분석 (K-Means Clustering), SOM (Slef Organizing Map)



#### 로지스틱 회귀분석

- 반응변수가 범주형인 경우 적용되는 회귀분석모형

- 신규 설명변수 추정 및 기준치에 따라 분류하는 목적(분류모형)으로 활용

- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)라고 함

    > 오즈비(odds ratio) : 오즈(odds)는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 확률
    >
    > ex) 16강에 한국과 브라질이 진출을 성공/실패할 확률과 각각의 오즈와 오즈비는 아래와 같음
    >
    > |  구분  | 16강 성공확률 | 16강 실패확률 |
    > | :----: | :-----------: | :-----------: |
    > | 브라질 |      0.8      |      0.2      |
    > |  한국  |      0.1      |      0.9      |
    >
    > odds (브라질) : $\dfrac {0.8} {1 - (0.8)} = \dfrac {0.8} {0.2} = 4$
    >
    > odds (한국) : $\dfrac {0.1} {1-0.1} = \dfrac {1} {9}$
    >
    > Odds ratio : $\dfrac {odds(브라질)} {odds(한국)} = \dfrac {4}{\dfrac {1}{9}} = 36$
    >
    > 오즈비가 36 이 나타나 브라질이 16강에 진출할 확률이 한국의 16강 진출 확률보다 36배 높다고 볼 수 있다.

- 선형회귀분석과 로지스틱 회귀분석 비교

    |    목적     |  선형회귀분석  |         로지스틱 회귀분석          |
    | :---------: | :------------: | :--------------------------------: |
    |  종속변수   |  연속형 변수   |               (0, 1)               |
    | 계수 추정법 |   최소제곱법   |           최대우도추정법           |
    |  모형 검정  | F-검정, T-검정 | 카이제곱 검정 (X<sup>2</sup>-test) |

    > 최대우도추정법 (MLE : Maximum Likelihood Estimation) : 모수가 미지의 &theta; (theta)인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 &theta;를 추정하는 기법

- glm() 함수를 활용하여 로지스틱 회귀분석 실행

- R코드 : glm(종속변수 ~ 독립변수1 +...+ 독립변수k, family=binomial, data=데이터셋명)



#### 다중회귀분석의 모형의 설명(적절함)

- F-검정 통계량과 유의확률

- t-통계량과 유의확류

- R<sup>2</sup> (결정계수) 값 검정

    >   r(상관계수)는 회귀분석 이전의 단계에서 실행하여 설명력 확인



#### 회귀분석의 영향력 진단

-   영향력 진단이란 **적합된 회귀모형의 안전성을 평가하는 통계적인 방법**
-   선형회귀분석에서 **회귀직선의 기울기에 영향을 크게 주는 점을 영향점**이라고 함
-   영향력 진단 방법에는 Cook's distance, DFBETAS, DFFITS, Leverage H 등이 있다.
    -   Cook's distance : 쿡의 거리가 **기준값인 1보다 클 경우 영향치**로 간주
    -   DFBETAS : 값이 크지면 영향치 또는 이상치일 가능성 높음. **기준값은 2나 sqrt(n)**(표본을 고려한 경우), DFBETAS 값이 **기준값보다 클 경우 영향치**일 가능성이 높다.
    -   DFFITS : **기준값인 $2\sqrt{((p+1)/n)}$ 보다 클수록 영향치**일 가능성이 높다
    -   Leverage H : 관측치가 다른 관측치 집단으로부터 떨어진 정도를 의미하며, **2 x (p+1)/n** 보다 크면 영향치 이거나 이상치라고 본다.



#### 의사결정나무의 특징

의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형**으로 분류나무와 회귀나무 모형이 있다.

- 장점
    - 결과를 누구에게나 설명하기 용이
    - 만드는 방법이 계산적으로 복잡하지 않음
    - 대용량 데이터에서도 빠르게 만들 수 있음
    - 비정상 잡음 데이터에 대해서도 민감함 없이 분류 가능
    - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않음
    - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다.
    - 모형 분류 정확도가 높다.
- 단점
    - 새로운 자료에 대한 과적합 발생할 가능성이 높다.
    - 분류 경계선 부근의 자료값에 대해서 오차가 크다.
    - 설명변수 간의 중요도를 판단하기 쉽지 않다.
- **교호작용** 효과의 파악 : 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
- 알고리즘
    - CART (Classification and Regression Tree) : 불순도의 측도 지니지수, 이진분리
    - C4.5와 C5.0 : CART와 다르게 다지분리 가능. 불순도 측도 엔트로피 지수
    - CHAID(CHi-Squared Automatic Interaction Detection) : 가지치기 않함. 적당한 크기에서 나무 성장을 중지. 입력변수는 반드시 범주형. 불순도 측도 카이제곱 통계량 사용



#### 비모수 검정

모딥단의 모수에 대한 검정은 모수적 검정과 비모수적 검정으로 구분한다.

-   모수적 검정
    -   모집단의 **분포에 대한 가정을 하고**, 그 가정하에서 검정통계량과 **검정통계량의 분포를 유도해 검정을 실시**하는 방법

-   비모수적 검정
    -   자료가 추출된 **모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시**하는 방법
    -   관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우에 이용
    -   관측된 **자료의 수가 많지 않거나** (30개 미만), 자료가 개체간의 **서열관계를 나타내는 경우**에 이용
-   모수적 검정과 비모수적검정의 차이점
    -   가설의 설정
        -   모수적 검정 : 가정된 **모수의 분포에 대해 가설을 설정**
        -   비모수적 검정 : 가정된 분포가 없으므로 가설은 단지 '분포의 형태가 동일하다' 또는 '분포의 형태가 동일하지 않다'와 같이 **분포의 형태에 대해 설정**한다.
    -   검정 방법
        -   모수적 검정 : 관측된 자료를 이용해 구한 **표본평균, 표본분산** 등을 이용해 검정을 실시
        -   비모수적 검정 : 절대적인 크기에 의존하지 않고 **관측값들의 순위**(rank)나 **두 관측값 차이의 부호** 등을 이용해 검정
-   비모수검정의 예
    -   부호검정 (sign test), 윌콕슨의 순위합검정 (rank sum test), 윌콕슨의 부호순위합검정 (Wilcoxon signed rank test), 만-위트니의 U 검정, 런검정 (run test), 스피어만의 순위상관계수



#### TermDocumentMatrix 에서 sparsity(희박성)의 구하는 법

| Term   | 1    | 2    | 3    | 4    | 5    |
| ------ | ---- | ---- | ---- | ---- | ---- |
| 사과   | 0    | 0    | 1    | 0    | 1    |
| 바나나 | 0    | 1    | 0    | 0    | 0    |

-   sparsity(희박성)은 매트릭스 안에 0인 원소가 있는 %를 의미
-   sparsity = 7/10 = 70%



#### SOM (Self Organizing Map)

가. 개요

- 자기조직화지도 (SOM) 알고리즘은 코호넨 (Kohonen)에 의해 제시, 개발되었으며 코호넨 맵(kohonen maps)이라고도 알려져 있다.

- **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬**하여 지도의 형태로 형상화 한다.

    <img width="466" alt="image" src="https://user-images.githubusercontent.com/291782/163676024-410958e8-d1fa-46b2-abe2-26a2e42e6fb6.png">



나. 구성

- SOM 모델은 위 그림과 같이 두 개의 인공신경망 층으로 구성되어 있다.
    1. 입력층 (input layer: 입력벡터를 받는 층)
        - **입력 변수의 개수와 동일하게 뉴런 수가 존재**한다.
        - 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도 (map)라 부른다.
        - 입력층에 있는 뉴런은 경쟁층에 있는 뉴런들과 연결되어 있으며, 이때 완전연결 (fully connected)되어 있다.
    2. 경쟁층 (competitive layer : 2차원 격자(grid)로 구성된 층)
        - 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
        - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하며 학습한다.
        - 입력 층의 표본 벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best Matching Unit)라고 하며, **코호넨의 승자 독점의 학습 규칙**에 따라 위상학적 이웃 (topological neighbors)에 대한 연결 강도를 조정한다.



다. 특징

- 고차원의 데이터를 저차원의 **지도 형태로 형상화**하기 때문에 시각적으로 이해가 쉽다.
- 실제 데이터가 유사하면 지도상에서 가깝게 표현된다. 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보인다.
- 역전파 (Back Propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 **하나의 전방 패스 (feed-forward flow)를 사용함으로써 속도가 매우 빠른다**. 실시간 학습처리를 할 수 있는 모형이다.



라. SOM과 신경망 모형의 차이점

|         구분          |          신경망 모형          |                  SOM                  |
| :-------------------: | :---------------------------: | :-----------------------------------: |
|       학습방법        |         오차역전파법          |             경쟁학습방법              |
|         구성          |    입력층, 은닉층, 출력층     | 입력층, 2차원 격자(grid)형태의 경쟁층 |
| 기계 학습 방법의 분류 | 지도학습(Supervised learning) |   비지도학습(Unsupervised learning)   |





#### 사회연결망분석의 중심성

- 연결정도 중심성 (Degree centrality) : 한 점에 직접적으로 연결된 점들의 합
    - 인디그리중심성(In-Degree) : 한 점이 다른점으로부터 화살표를 받는 관계의 정도(영향을 받는 관계)
    - 아웃디그리중심성(Out-Degree) : 어떤점이 다른점에 화살표를 주는 정도 (영향을 주는 관계)
- 근접 중심성 (Closeness centrality) : 한 노드로부터 다른 노드에 도달하기까지 필요한 최소 단계의 합
- 매개 중심성 (Betweenness centrality) : 네트워크 내에서 한 점이 담당하는 매개자 혹은 중재자 역할의 정도
- 위세 중심성 (Eigenvector centrality) : 보나시치(Bonacich) 권련지수 : 위에 중심성의 일반적인 형태로, 연결된 노드의 중요성에 가중치를 둬 노드의 중심성을 측정하는 방법





#### 표본추출방법

1. 단순랜덤 추출법 (simple random sampling)

    -   각 샘플에 번호를 부여하여 n개를 추출하는 방법으로 각 샘플은 선택될 확률이 동일하다. (복원, 비복원 추출)

2. 계통추출법 (systematic sampling)

    -   단순랜덤 추출법의 변형된 방식으로 샘플을 나열하여 K개씩 n개의 구간으로 나누고 첫 구간에서 하나를 임의로 선택한 후에 K개식 띄어서 n 개의 표본을 선택
    -   ![systematic-sampling](https://user-images.githubusercontent.com/291782/161560760-0d60d365-a300-4262-8b09-5e9d64125e21.png)

3. 집락추출법 (cluster random sampling)

    -   군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링하는 방법
    -   ![cluster-random-sampling](https://user-images.githubusercontent.com/291782/161560900-294b7296-b204-41f2-9170-0f60ae4d9fc4.png)

4. 층화추출법 (stratified random sampling)

    -   이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출하는 방법으로, 유사한 원소끼리 몇 개의 층(stratum)으로 나누어서 각 층에서 랜덤 추출하는 방법
    -   ![stratified-random-sampling](https://user-images.githubusercontent.com/291782/161561325-a774c94f-ce60-4430-a2fb-f91dcdf969c2.png)

    

    

#### 측정방법 (아주중요)

-   질적척도 : 범주형 자료, 숫자들의 크기 차이가 계산되지 않는 척도
    -   명목척도 : 측정 대상이 어느 **집단**에 속하는지 분류할 때 사용 (성별, 출생지 구분)
    -   순서척도 : 측정 대상의 **서열관계**를 관측하는 척도 (만족도, 선호도, 학년, 신용등급)
-   양적척도 : 수치형자료, 숫자들의 크기 차이를 계산할 수 있는 척도
    -   구간척도(등간척도) : 측정 대상이 갖고 있는 **속성의 양**을 측정하는 것으로 구간이나 구간 사이의 **간격이 의미가 있는** 자료 (온도, 지수)
    -   비율척도 : 간격(차이)에 대한 비율이 의미를 가지는 자료, **절대적인 기준인 0이 존재**하고 **사칙연산이 가능**하며 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리)

순서척도는 명목척도와 달리 매겨진 숫자의 크기를 의미있게 활용 가능 (예: 1등이 2등보다 성적이 높다)

구간척도는 절대적 크기는 측정할 수 없기 때문에 사칙연산 중 더하기와 빼기는 가능. 곱하기나 나눗셈은 불가능





#### 오분류표 (Confusion matrix)

![precision_recall](https://user-images.githubusercontent.com/291782/150641056-4425fc9d-36be-4369-9c35-f76b1522c204.png)

- 참긍정률(TPR)  = $\dfrac{TP}{TP+FN}$ = 재현율(Recall) = 민감도(Sensitive) = ROC의 세로축
- 거짓긍정률(FPR) = $\dfrac {FP}{FP+TN}$ = (1 - 특이도(Specificity)) = ROC의 가로축
- 정확도(Accuracy, 정분류율) = $\dfrac {TP+TN}{TP+TN+FP+FN}$
- 오분류율(Error Rate) : $1 - Accuracy = \dfrac {FN + FP} {P + N}$
- 정밀도(Precision) = $\dfrac {TP}{TP+FP}$
- 재현도(Recall) = 민감도(Sensitive) = $\dfrac {TP}{TP+FN}$ = TPR(참긍정률)
- 특이도(Specificity, TNR, True Negative Rate) = $\dfrac{TN}{TN+FP}$
- F1-Score 에 들어가는 지표는? 정밀도(Precision) 와 재현율(Recall, 민감도)
    - 식 = $2 × \dfrac {Precision × Recall}{Precision + Recall}  $ 
    - 재현율과 정밀도 값이 모두 클 때 F1-Score도 큰 값을 가진다
    - F1-Score는 민감도와 정밀도를 합한 **성능평가지표**로 0~1 사이의 값을 가진다. 1이 좋음





#### 연관규칙

-   개념
    -   연관성 분석은 흔히 장바구니분석(market basket analysis) 또는 서열분석 (sequence analysis) 이라고 불린다.
    -   기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용
    -   장바구니 분석 : 장바구니에 무엇이 같이 들어 있는지에 대한 분석
    -   서열분석 : A를 산 다음에 B를 산다.

-   연관규칙의 측도 : 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 한다.

1. 지지도 (support)

    - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의
    - 지지도(support) = $P(A \cap B) = \dfrac {A와 B가 동시에 포함된 거래수} {전체 거래수} = \dfrac {A \cap B} {전체}$

2. 신뢰도 (confidence)

    - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률. 연관성의 정도를 파악 가능
    - 신뢰도(confidence) = $\dfrac {P(A \cap B)} {P(A)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수} = \dfrac {지지도} {P(A)}$

3. 향상도 (Lift)

    - A가 구매되지 않았을 떄 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가 비이다. 연관규칙 A&rarr;B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다.

    - 향상도(lift) = $\dfrac {P(B|A)} {P(B)} = \dfrac {P(A \cap B)} {P(A)P(B)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수 \; X \; B를 포함하는 거래수} = \dfrac {신뢰도} {P(B)}$ 

        <img width="935" alt="support-confidence-lift" src="https://user-images.githubusercontent.com/291782/163676755-3b1a3876-f2d0-4ce5-b748-1bf5f4038e19.png">





#### 결정계수(R<sup>2</sup>)

- 설명력 : R<sup>2</sup>는 전체 데이터를 회귀모형이 설명할 수 있는 설명력을 의미

- SSR / SST (전체 제곱합에서 회귀제곱합의 비율), $0 \le R^2 \le 1$

    > $SST = \displaystyle \sum(y_i - \overline{y})^2 , \quad SSE = \displaystyle \sum (y_i - \hat{y_i})^2, \quad SSR = SST - SSE$

- 단순선형회귀분석에서는 결정계수는 **상관계수 r ( -1 < r < 1)**의 제곱과 같다.

- 결정계수(R<sup>2</sup>)를 통해 추정된 회귀식이 얼마나 타당한지 검토한다. (결정계수가 1에 가까울수록 회귀모형이 자료를 잘 설명함)

- 독립변수가 종속변수 변동의 몇%를 설명하는지 나타내는 지표이다.

- 다변량 회귀분석에서는 독립변수가 많아지면 결정계수가 높아지므로 독립변수가 유의하든, 그렇지 않든 독립변수의 수가 많아지면 결정계수가 높아지는 단점이 있다.

- 이러한 결정계수의 단점을 보완하기 위해 수정 결정계수(R<sup>2</sup>: adjusted R<sup>2</sup>)를 활용한다. 수정결정계수는 결정계수보다 작은 값으로 산출되는 특징이 있다.

- 수정 결정계수 (adjusted R<sup>2</sup>) 식

    $R_a^2 = 1 - \dfrac {(n-1)(1-R^2)} {n - k - 1} \newline = 1- \dfrac {(n-1) \times (\dfrac {SSE} {SST})} {n-k-1} \newline = 1 - (n-1)\dfrac {MSE} {SST} \newline (k:독립변수 개수, n: 데이터 개수)$

    



#### 잔차(Residuals)

- 예측값과 실제값의 차이

- ```R
    Residuals:
      Min        1Q       Median     3Q       Max
    -29.056    -9.525    -2.272    9.215    43.201
    ```

- 잔차의 최솟값(Min), 사분위수(1Q, Median, 3Q), 최대값(Max)을 보여줌

- 중앙값이 0에 가깝고, 1, 3사분위 수가 거의 대칭을 이루고 있으므로, 잔차가 정규분포에서 것의 벗어나지 않았다고 볼수 있음

> 오차(error)와 잔차(residual)의 차이
>
> - 오차 : 모집단에서 실제값이 회귀선과 비교해 볼 때 나타나는 차이(정확치와 관측치의 차이)
> - 잔차 : 표본에서 나온 관측값이 회귀선과 비교해볼 때 나타나는 차이



#### 회귀계수(Coefficients)

```R
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -17.5791     6.7584  -2.601   0.0123 *  
speed         3.9324     0.4155   9.464 1.49e-12 ***
```



- Estimate는 데이터로 부터 얻은 계수의 추정치(estimate)를 말함
- 절편(Intercept)의 추정치는 -17.5791로,  ```speed```가 0 일때 ```dist```의 값이다
- ```speed```의 계수 추정치는 3.9324로 ```speed```가 1 증가할 때마다 ```dist```가 3.9324 증가한다는 것을 의미함
- 이를 수식으로 정리하면 $dist = -17.5791 + 3.9324 \times speed$
- 추정치 오른쪽 끝의 ```Pr(>|t|)```는 모집단에서 계수가 0 일때, 현재와 같은 크기의 표본에서 이런한 계수가 추정될 확률인 p 값을 나타낸다. 이확률이 매우 작다는 것은, 모집단에서 ```speed```의 계수가 정확히 3.9324가 아니더라도 현재의 표본과 비슷하게 0보다 큰 어떤 범위에 있을 가능성이 높다는 것을 의미한다. 보통 5%와 같은 유의수준을 정하여 p값이 그 보다 작으면 (p < 0.05), ```"통계적으로 유의하다"``` 라고 한다.



#### 모형적합도

```R
Multiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 
F-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12
```

Multiple R-squared, Adjusted R-squared, F-statistic, p-value는 모형이 데이터에 잘 맞는 정도를 보여주는 지표들이다.

- Multiple R-squared: 0.6511
    - 모형 적합도(혹은 설명력)
    - `dist`의 분산을 `speed`가 약 65%를 설명한다
    - 각 사례마다 `dist`에 차이가 있다.
- Adjusted R-squared: 0.6438
    - 독립변수가 여러 개인 다중회귀분석에서 사용
    - 독립변수의 개수와 표본의 크기를 고려하여 R-squared를 보정
    - 서로 다른 모형을 비교할 때는 이 지표가 높은 쪽은 선택한다
- F-statistic: 89.57 on 1 and 48 DF, p-value: 1.49e-12
    - 회귀모형에 대한 (통계적) 유의미성 검증 결과, 유의미함 (p < 0.05)
    - 즉, 이 모형은 주어진 표본 뿐 아니라 모집단에서도 의미있는 모형이라 할 수 있음



#### 결과 보고

논문 등에서 회귀분석의 결과는 다음 순서대로 보고한다.

먼저 모형적합도를 보고한다. F 분포의 파라미터 2개와 그 때의 F 값, p-value와 유의수준의 비교를 적시한다.

> dist에 대하여 speed로 예측하는 회귀분석을 실시한 결과, 이 회귀모형은 통계적으로 유의미하였다(F(1,48) = 89.57, p < 0.05).

다음으로 독립변수에 대해 보고한다.

> speed의 회귀계수는 3.9324로, dist에 대하여 유의미한 예측변인인 것으로 나타났다(t(48) = 9.464, p < 0.05).



#### 결측값 처리 방법

-   단순 대치법
    -   단순 삭제
    -   평균 대치법 : 비조건부 평균 대치법 (단순 평균), 조건부 평균 대치법 (회귀분석을 활용하여 대치)
    -   단순확률 대치법 : 어떤 적절한 확률값을 부여한 후 대치하는 방법. Hot-deck 방법, nearest neighbor 방법
-   다중 대치법 : 단순 대치법을 여러번
    -   **m번의 대치를 통한 m개의 가상적 완전한 자료**를 만들어서 분석하는 방법
    -   **대치**(imputation step) > **분석** (analysis step) > **결합** (combination step) 의 **3단계**를 거침



#### 정상성

-   정상성은 **평균이 일정, 분산이 일정, 공분산**도 단지 시차에만 의존하고 실제 특정 시점 t, s에는 의존하지 않을 떄 만족함

-   평균이 일정할 경우

    -   모든 시점에 대해 일정한 평균을 가짐
    -   평균이 일정하지 않은 **시계열은 차분(difference)**을 통해 정상화 할 수 있다.

    > 차분이란?
    >
    > - 현시점 자료에서 전시점 자료를 빼는 것
    > - 일반차분 (regular difference) : 바로 전 시점의 자료를 빼는 방법
    > - 계절차분 (seasonal difference) : 여러 시점 전의 자료를 뺴는 방법, 주로 계절성을 갖는 자료를 정상화 하는데 사용

-   분산이 일정

    -   분산도 시점에 의존하지 않고 일정해야함
    -   분산이 일정하지 않을 경우 **변환 (transformation)을 통해 정상화** 할 수 있다.

-   공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않는다.     



#### 시계열 자료 분석방법

##### 가. 분석방법

- 회귀분석 (계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등이 있다.



##### 나. 자료 형태에 따른 분석 방법

- 일변량 시계열 분석
    - Box-Jenkins (ARMA), 지수 평활법, 시계열 분해법 등이 있다.
    - 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심을 갖는 경우의 시계열 분석
- 다중 시계열
    - 계량경제 모형, 전이함수 모형, 개입분석, 상태공간 분석, 다변량 ARIMA 등
    - 여러개의 시간(t)에 따른 변수들을 활용하는 시계열 분석
    - 예) 이자율, 인플레이션이 환율에 미치는 요인



##### 다. 이동평균법

- 개념
    - 추세를 파악하여 다음 기간을 예측하는 방법
    - n개의 시계열 데이터를 m기간으로 이동평균하면 n-m+1개의 이동평균 데이터가 생성된다.
- 특징
    - 간단하고 쉽게 미래를 예측가능, 자료의 수가 많고 안정된 패턴을 보이는 경우 예측의 품질이 높음
    - 특정 기간안에 속하는 시계열에 대해서는 동일한 가중치를 부여
    - 불규칙변동이 심하지 않은 경우에는 짧은 기간(m의 개수가 적음), 반대로 불규칙변동이 심한 경우 긴 기간 (m의 개수가 많음)의 평균을 사용
    - 이동평균에서 가장 중요한 것은 적절한 기간을 사용하는 것. 즉, 적절한 n의 개수를 결정하는 것.



##### 라. 지수평활법 (Exponential Smoothing)

- 개념
    - 일정기간의 평균을 이용하는 이동평균법과 달리 모든 시계열 자료를 사용하여 평균을 구하며, 시간의 흐름에 따라 최근 시계열에 더 많은 가중치를 부여하여 미래를 예측하는 방법
- 특징
    - 단기간에 발생하는 불규칙변동을 평활하는 방법
    - 자료의 수가 많고, 안정된 패턴을 보이는 경우일수록 예측 품질이 높음
    - 지수평활법에서 가중치의 역할을 하는 것은 지수평활계수(&alpha;)이며, 불규칙변동이 큰 시계열의 경우 지수평활계수는 작은 값을, 불규칙변동이 작은 시계열의 경우, 큰 값의 지수평활계수를 적용(generally, &alpha; is between 0.05 and 0.3)
    - 지수평활계수는 예측오차 (실제 관측치와 예측치 사이의 잔차제곱합)를 비교하여 예측오차가 가장 작은 값을 선택하는 것이 바람직함
    - 지수평활계수는 과거로 갈수록 지속적으로 감소함
    - 지수평활법은 불규칙변동의 영향을 제거하는 효과가 있으며, 중기 예측 이상에 주로 사용됨
    - 단, 단순지수 평활법의 경우, 장기추세나 계절변동이 포함된 시계열의 예측에는 적합하지 않음



#### 시계열모형

##### 가. 자기회귀 모형 (AR모형, autoregressive model)

- p 시점 전의 자료가 현재 자료에 영향을 주는 모형

- $Z_t = \Phi_1Z_{t-1} + \Phi_2Z_{t-2} + ... + \Phi_pZ_{t-p} + \alpha_t$

- >- Z<sub>t</sub> : 현재 시점의 시계열 자료
    >- Z<sub>t-1</sub>, Z<sub>t-2</sub> ..., Z<sub>p</sub> : 이전, 그 이전 시점 p의 시계열 자료
    >- &Phi;<sub>p</sub> : p 시점이 현재에 어느 정도 영향을 주는지를 나타내는 모수
    >- &alpha;<sub>t</sub> : 백색잡음과정 (white noise process) : 시계열분석에서 오차항을 의미
    >- 평균이 0, 분산이 &sigma;<sup>2</sup>, 자기공분산이 0인 경우를 뜻하며, 시계열간 확률적 독립인 경우 강(strictly) 백색잡음 과정이라고 한다. 백색잡음 과정이 정규분포를 따를 경우 이를 가우시안(Gaussian) 백색잡음과정이라고 한다.

- AR(1) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &alpha;<sub>t</sub>, 직전 시점 데이터로만 분석

- AR(2) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &Phi;<sub>2</sub>Z<sub>t-2</sub> + &alpha;<sub>t</sub>, 연속된 2시점 정도의 데이터로 분석

- AR(2) 모형의 자기상관함수(ACF)와 편자기상관함수(PACF)

- <img width="584" alt="acf-pacf" src="https://user-images.githubusercontent.com/291782/162600723-a762b3b5-6461-4b8c-972e-c100914b3da5.png">



##### 나. 이동평균 모형 (MA 모형, Moving Average model)

- 유한한 개수의 백색잡음의 결합이므로 언제나 정상성을 만족
- 1차 이동평균모형 (MA1 모형)은 이동평균모형 중에서 가장 간단한 모형으로 시계열이 같은 시점의 백색잡음과 바로 전 시점의 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub> -  &phi;<sub>2</sub>&alpha;<sub>t-2</sub> - ... -  &phi;<sub>p</sub>&alpha;<sub>t-p</sub> 
- 2차 이동평균모형 (MA2 모형)은 바로 전 시점의 백색잡음과 시차가 2인 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub>
- AR모형과 반대로 ACF에서 절단점을 갖고, PACF가 빠르게 감소
- $Z_t = \alpha_t - \phi1\alpha_{t-1} - \phi_2\alpha_{t-2}$



##### 다. 자기회귀누적이동평균 모형 (ARIMA(p,d,q) 모형, autoregressive integrated moving average model)

- ARIMA 모형은 비정상시계열 모형이다

- ARIMA 모형은 차분이나 변환을 통해 AR모형이나 MA모형, 이 둘을 합친 ARMA 모형으로 정상화 할 수 있다.

- p는 AR모형, q는 MA모형과 관련이 있는 차수

- 시계열 {Z<sub>t</sub>}의 d번 차분한 시계열이 ARMA (p, q) 모형이면, 시계열 {Z<sub>t</sub>}는 차수가 p,d,q인 ARIMA 모형, 즉 ARIMA(p,d,q) 모형을 갖는다고 한다.

- d=0이면 ARMA(p, q) 모형이라 부르고, 이 모형은 정상성을 만족한다. (ARMA (0 , 0)일 경우 정상화가 불필요)

- p=0 이면 IMA (d, q) 모형이라 부르고, d번 차분하면 MA(q) 모형을 따른다.

- q = 0이면 ARI (p, d) 모형이라 부르며, d번 차분한 시계열이 AR (p) 모형을 따른다.

    > ARIMA (0, 1, 1)의 경우에는 1차분 후 MA(1) 활용
    >
    > ARIMA (1, 1, 0)의 경우에는 1차분후 AR(1) 활용
    >
    > ARIMA (1, ,1 2)의 경우에는 1차분 후 AR(1), MA(2), ARMA (1, 2) 선택 활용
    >
    > => 이런 경우 가장 간단한 모형을 선택하거나 AIC 를 적용하여 점수가 가장 낮은 모형을 선정



##### 라. 분해 시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

    > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
    >
    > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
    >
    > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
    >
    > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인





### 5과목 데이터 시각화

#### 시각화 인사이트 프로세스는?

-   탐색(1단계) > 분석(2단계) > 활용(3단계)



#### 데이터클라우드(워들) 사용하는 단계에 대한 설명

-   데이터클라우드는 탐색 단계에서 비정형 데이터(텍스트 데이터) 측정값에서 관계를 탐색하기 위해 사용하는 시각화이다.



#### 벤프라이(Ben Fry) 7단계

-   정보획득, 분석(분해), 선별, 마이닝, 표현, 정제, 상호작용



#### facet_grid : 집단별 구분

-   facet_grid(Type ~.) # 가로 (라벨이 오른쪽에 존재)
-   facet_grid(. ~ Type) # 세로 (라벨이 위쪽에 존재)
-   facet_grid(Type ~ Origin) # Type은 오른쪽, Origin은 윗쪽에 라벨 존재



#### D3 라이브러리

-   d3.layout.pie() : 이렇게 만드는건 파이 차트만
-   d3.scale.linear() : 막대차트, 스캐터플롯



#### 시각화를 위한 그래픽 디자인 기본 원리

1.   아이소타이프 (ISOTYPE, International System Of TYpographic Picture Education)

     -   많은 양의 데이터를 쉽게 지각할 수 있도록 도와주는 시각표현 방법

     -   정보, 자료, 개념, 의미 등을 나타내기 위해 문자와 숫자 대신 상징적 도형이나 정해진 기호를 조합해 시각적이고 집접적으로 나타내는 방식

     -   ![isotype](https://user-images.githubusercontent.com/291782/158065420-24a0c418-cf69-453e-879c-1b9f758b8b1f.png)

2.   타이포그래피
     -   가장 어려운 일이 서체를 선택하는 것



#### 시각적 이해의 위계

-   데이터는 시각화
-   정보는 디자인
-   지식은 매핑
-   지혜는 정의되지 않은 것으로 표시됨
-   데이터 : 분리된 요소(=개별적 요소 하나 하나), 단어, 숫자, 암호, 도표, 차트 등. 근거나 되는 사실이나 참고 자료를 의미. 원자재. 데이터는 불완전하고 비연속적
-   정보 : 연관된 요소들. 의견, 단락, 균형, 개념, 생각, 질문. 데이터와 달리 그 자체만으로 의미가 있다.
-   지식 : 조직화된 정보. 화제, 이론, 이치, 개념상의 구성. 경험을 통해 **다른 관점과 방법으로 해석할 수 있다.**
-   지혜 : 적용된 지식. 책, 범례, 체계. 종교 철학. **명시적인 언어로 상대에게 전달하기 어렵다.**



#### 시각화 라이브러리 와 인포그래픽스

-   시각화 라이브러리
    -   종류:  PolyMaps, D3.js, Google Chart
    -   라이브러리 설치 필요, 라이브러리가 제공하는 API로 코드 작성해 시각화
-   인포그래픽스
    -   웹서비스 형태로 제공, 회원가입 필요, 제공되는 템플릿으로 구현 가능
    -   종류 : iCharts, Visualize Free, Visual.ly



#### 데이터 구성원리

1.   이벤트 기록으로서 접근
     -   원본 데이터 (raw data, log data)는 명세화의 기본 대상이 된다.
     -   원본 데이터는 특정 이벤트가 발생했을 때 생성된다.
     -   데이터로 부터 통찰을 이끌어 내기 위해서는 **데이터가 어떤 원리로 생성, 구성되었는지**를 항상 염두에 두고 있어야 한다.
2.   객체지향 관점에서의 접근
     -   데이터의 구성과 생성 배경에 대해 고민함으로써 어떤 식으로 시각화할 지에 대한 닶을 찾아갈 수 있다.
     -   데이터의 범위가 주어지면, 데이터의 구조 자체를 설계, 생성하여 이를 토대로 통찰을 뽑아볼 수 있다.
     -   기본적으로 대상을 객체화 한다.
     -   **모든 객체들은 행위와 고유 속성 값**을 갖게 된다.



#### 시각화 방법에 따른 그래프의 종류

-   시간 시각화 : 막대 그래프, 누적막대 그래프, 점그래프
-   분포 시각화 : 파이차트, 도넛차트, 트리맵, 누적연속그래프
-   관계 시각화 : 스케터플롯(산점도, XY그래프), 버블차트, 히스토그램
-   비교 시각화 : 히트맵, 체르노프 페이스, 스타차트, 평행좌표계, 다차원 척도법
-   공간 시각화 : 지도 매핑



- 산점도

    ![scatterplot](https://user-images.githubusercontent.com/291782/158017483-7c785a0c-2e4c-4840-97f0-7dab7b8d36ff.png)



- 점 그래프

    ![point-graph](https://user-images.githubusercontent.com/291782/168115035-f7be41ff-7380-462e-9960-73eb863da672.png)

    

- 평행좌표계

    ![parallel-graph](https://user-images.githubusercontent.com/291782/158017799-206cfc73-ef4f-4a2e-b161-44727750d9a1.png)

    



- 다차원 척도법

    ![mds-graph](https://user-images.githubusercontent.com/291782/168115621-282a7350-ff8e-423e-a6a9-974f54d60089.png)



- 지도 매핑

    ![map-mapping](https://user-images.githubusercontent.com/291782/168115803-af72e984-3b60-46a1-b49b-6b011b886013.png)

    



#### D3.js 설명

1.   특징
     -   JS 기반의 데이터 시각화 라이브러리
     -   HTML5, SVG, CSS로 데이터 시각화
     -   SVG 객체, canvas 객체 등을 기반으로 동작
     -   CSS를 통해 레이아웃과 속성 변경을 통해 디자인적 요소 조작 가능
     -   cross browser 지원
2.   기본 개념
     -   SVG
         -   그림을 그리기 위한 html 태그
         -   rect, circle, line, path, ellipse, polyline 등의 객체를 사용하여 그림
         -   시각화 구현을 위해 HTML5의 SVG 객체가 필요함
     -   SCALE
         -   시각화 그림들이 화면에 부자연스럽게 표현되는 것을 방지하기 위해 사용
         -   **시각화의 최적화**를 도움
         -   domain() : scale **입력** 값의 범위 지정
         -   range() : scale **출력** 값의 범위 지정

-   JS 기반의 데이터 시각화 라이브러리
-   HTML5, SVG, CSS로 데이터 시각화
-   SVG 객체, canvas 객체 등을 기반으로 동작
-   CSS를 통해 레이아웃과 속성 변경을 통해 디자인적 요소 조작 가능
-   모든 브라우저에 동일한 코드에 대한 일관적인 결과 얻을 수 있음





#### 타이포그래피

-   서체
    -   글의 형태를 총칭하는 말로 얼굴에 해당. 타이포 그래피에서 **가장 어려운 일이 서체를 선택**하는 일
    -   세리프 서체 : 돌기가 있음. 가독성이 높아 본문용 (함초롱 바탕체)
    -   산세리프 서체 : 돌기가 없음. 주목성이 높아 제목용 서체 (맑은 고딕)
-   무게
    -   획의 두께를 의미하며, 굵기라고도 한다.
-   크기
    -   글자 크기는 **실제 글자의 크기가 아니**라 글자가 배치되는 금속 활자판의 높이를 의미
    -   같은 크기라도 서체에 따라 실제 글자 크기가 달라짐
-   스타일
    -   각도에 따라 글자 스타일이 달라짐
    -   이탤릭체와 같이 기울이거나 장체, 평체처럼 글자의 폭을 좁히거나 넓힘
-   색채
    -   명도, 채도, 색상의 색채 속성을 활용해 정보 분류 가능
    -   **정보의 중요도나 종속의 관계표현이 가능**
-   간격 (글자 사이, 낱말 사이, 글줄 사이)
    -   가독성에 큰 영향을 미침
    -   **읽어야 할 다음 글자가 다른 글자보다 근접**해 있어야 하며, 이 때문에 글자 사이보다 낱말 사이, 낱말 사이보다 글줄 사이가 넓어야 함



#### TABLEAU (태블로, 시각화 플랫폼 제품)

- MS 데이터 소스, MySQL, ORACLE, IBM OLAP 서버, csv 파일 등 **다양한 데이터**로부터 실시간으로 크로스 테이블을 시각적으로 보여줌
- **VizQLTM** (비주얼 쿼리 언어)을 개발해 사용자가 DB와 상호작용하면서 그래픽 / 시각적인 결과를 얻을 수 있다.
- 태블로 실행 후 데이터 소스에 연결하면, 자동으로 데이터 소스의 필드들을 **디맨션이나 measure로 분할**한다. 필드들을 shelves에 끌어다 두는 것으로 쓸 수 있다. 매우 직관적이며, 분석에 빠르게 적용할 수 있다.
- 태블로는 **크로스탭과 피벗 테이블 기능**을 단 몇번의 클릭만으로 가능하다.



#### 래치(LATCH) 방법

- 위치(Location), 알파벳(Alphabet), 시간(Time), 카테고리(Category), 위계(Hierarchy) 이상 5가지가 정보를 정리 또는 조직화하는 기준

- 위치 : 정보를 공간적인 위치에 배열하는 방법으로 지리적인 것만이 아니라 공간적으로 구분하는 것 모두를 포괄한다.

    ![latch-location](https://user-images.githubusercontent.com/291782/168120447-af159c41-e9fe-4d94-8a00-2cf37f6b7d1e.png)

- 알파벳 : 알파벳 또는 가나다순으로 정렬하는 방법이 흔히 사용됨

    ![latch-alphabet](https://user-images.githubusercontent.com/291782/168120675-1fe8feda-6be0-4031-b0a5-d64c0fa164e1.png)

- 시간 : **연도별 시간 순서**에 따라 강아지를 분류

    ![latch-time](https://user-images.githubusercontent.com/291782/168120883-d4f7f5c3-b178-4c2a-ac05-7618b4c7a098.png)

- 카테고리 : 정보의 속성에 따라 분류할 떄 적합. 상점의 상품분류, 도서관의 서적 분류 등

    ![latch-category](https://user-images.githubusercontent.com/291782/168121082-3f162705-36ce-41f4-a3b5-24b5c07e36bf.png)

- 위계(가중치) : 고도의 변화(낮음에서 높음), 가격의 변화(싼 것에서 비싼것) 등 **정보의 변화에 따라 데이터의 값이나 중요도의 순서로 정보를 조직화 하는 것**

    ![latch-hierarchy](https://user-images.githubusercontent.com/291782/168121378-1652c57f-40bc-48ae-9455-66ff33ae53eb.png)







### 서술형

3단원 메모리 기상화 부터 해라 

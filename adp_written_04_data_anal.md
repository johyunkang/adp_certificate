## 4과목 데이터 분석 (40)

## ADP 데이터 분석 START

### 1장 통계분석

### 1절 통계분석

#### 1. 확률 분포 (p.66)

##### 가. 이산형 확률분포

- 확률 변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포, 확률값은 확률질량함수를 이용하여 계산

> $P(X_i) > 0$      $  i=1,2,...,k$      $\displaystyle \sum_{i=1}^kP(X_i)=1$

- 이산형 확류변수 예시 : 동전 2개를 던져서 앞/뒷면이 나오는 경우의 수(H:앞, T:뒤)

  | 표본공간(&Omega;) | HH(사건) | HT   | TH   | TT   | 합계 |
  | ----------------- | -------- | ---- | ---- | ---- | ---- |
  | P(x)              | 1/4      | 1/4  | 1/4  | 1/4  | 1    |

  

###### 1) 베르누이  확률분포(Bernoulli distribution)

- 결과가 2개만 나오는 경우 (ex. 동전 던지기, 시험의 합/불합격 등)

> $P(X=x)=p^x(1-p)^{1-x}$  (x=1 or 0), E(x) = p,  var(x)=p(1-p)
>
> 예) 메이저리거인 추추가 안타를 칠 확률은 베르누이 분포를 따름. (안타를 치는 사건을 x=1이라고 할 때 안타를 칠 확률은 타율로 적용 가능)



###### 2) 이항분포(Binomial distribution)

- 베르누이 시행을 n번 반복했을 때 k번 성공할 확률
- $P(X = k) = _nC_kP^k(1-p)^{n-k}, \quad  _nC_k = \dfrac {n!} {k!(n-k)!}$
- 한 축구 선수가 페널티킥을 차면 5번 중 4번은 성공한다고 한다. 그럼 이 선수가 10번의 페널티킥을 차서 7번 성공할 확률은?
- 5번 중 4번 성공하기에 성공확률은 4/5 = 0.8, 실패확률은 1-0.8 = 0.2
- $이항분포 = \begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} =  \begin{pmatrix} 10 \\ 7 \end{pmatrix} 0.8^70.2^3$
- $ = \dfrac {10!} {7! \times 3!} 0.8^7 0.2^3 = 0.2013 = 20.13\%$



###### 3) 기하분포(Geometric distribution)

- 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- $p(x) = p(1-p)^{x-1}$
- 어느 야구선수가 홈런을 칠 확률은 0.05라고한다. 이 선수가 6번째 타석에서 홈런을 칠 확률은?
- 성공확률 p=0.05, 실패확률은 1 - 0.05 = 0.95, 6번째 타석에서 성공할 확률이기에 x-1 = 6-1
- $p(1 - p)^{x-1} = 0.05 \times 0.95^{6-1} = 0.0387 = 3.87\%$



###### 4) 다항분포(Multinomial distribution)

- 이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포

- 각 상황의 확률과 각 상황의 횟수를 잘 파악해야 함

- $p(x) = \dfrac {n!} {x_1!x_2! ...x_k!}p1^{x_1}p_2^{x_2}... p_k^{x_k} $

- 국내 인터넷 포털 사이트의 점유율은 아래와 같다.  12명을 임의로 뽑아 사용 사이트를 알아보았을 때, 네이버 7명, 구글 3명, 다음과 ZUM이 각 1명, 기타는 0명이 사용할 확률을 구하시오

  네이버 : 61%, 구글 : 30%, 다음 : 7%, ZUM: 1%, 기타 : 1%

- $\dfrac {12!} {7! \times 3! \times 1! \times 1! \times 0!} \times 0.61^7 \times 0.3^3 \times 0.07^1 \times 0.01^1 \times 0.01^0 = 0.0094$

    

    

###### 5) 포아송분포 (Poisson distribution)

- 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 (예. 가게에 손님이 1시간에 20명씩 방문한다고 할 때, 10분에 손님이 5명씩 방문할 확률)
- 확률을 구하기 위해서는 **평균(&lambda;)과 발생횟수(x)**를 잘 파악해야 함
- $p(x) = \dfrac {e^{-\lambda} \lambda^x} {x!} , \quad e=2.718281...$
- 전공 책 5페이지를 검사했는데, 오타가 총 10개가 발견되었다고 한다. 그럼 이 책에서 어느 한 페이지를 검사하였을 때, 오타가 3개 나올 확률을 구하시오?
- 해설) 포아송 분포는 평균을 잘 구해야함. 문제에 말장난이 섞여 있음. 일단 5페이지에 총 10개의 오타이므로, 1페이지에 평균 2개의 오타가 발견된 셈. 그래서 평균 (&lambda;)=2 이다. 그리고 발생횟수 x=3 이므로...
- $\dfrac {2.718281^{-2} \times 2^3} {3!} = 0.1804$




##### 나. 연속형 확률분포

- 확률 변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포이며, 확률값은 확률밀도함수를 이용하여 계산한다.

###### 1) 균일분포 (일양분포, Uniform distribution)

- 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
- $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
- ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)



###### 2) 정규분포 (Normal distribution)

- 평균이 &mu;이고, 표준편차가 &sigma;인 X의 확률밀도함수
- 표준편차가 클 경우 퍼져보이는 그래프가 나타난다.
- 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
- 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
- ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)



###### 3) 지수분포 (Exponential distribution)

- 어떤 사건이 발생할 때까지 경과한 시간에 대한 연속확률분포이다. (예. 전자렌지의 수명시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간)
- ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)



###### 4) t-분포 (t-distiribution)

- 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
- 표본의 크기가 적을때는 표준정규분포를 위에서 눌러 놓은 것과 같은 형태를 보이지만 표본이 커져서(30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
- 데이터가 연속형일 경우 활용한다.
- **두 집단의 평균이 동일**한지 알고자 할 때 검정통계량으로 활용된다.
- 표준정규분포와 같이 평균 값이 0이며, 자유도에 따라 분포의 모양이 변화한다.
- 자유도가 30미만인 경우, 표준정규분포에 비해 양쪽 끝이 평평하고 두터운 꼬리 모양을 가진다.
- ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)



###### 5) X<sup>2</sup>-분포 (X<sup>2</sup>-distribution) (카이제곱분포)

- 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포이다.
- **두 집단 간의 동질성 검정에 활용**된다.
- 확률변수 X가 표준정규분포(Z)를 따를 때, 자유도가 k인 카이제곱분포를 따른다. 자유도는 표본 자료 중 모집단에 대한 정보를 주는 독립적인 표본 자료의 수와 같으며, 분할표에서의 행과 열의 개수를 통해 구할 수 있다. (자유도(df) = (r-1)(c-1), r=행의 개수, c=열의 개수)
- ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)



###### 6) F-분포 (F-distribution)

- **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량의 분포이다.
- 확률변수는 항상 양의 값만을 갖고 카이제곱분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
- ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)



#### 2. t-검정 (t-test) (p.72)

##### 가. 일표본 t-검정 (one sample t-test)

###### 1) 일표본 t-검정이란?

- **단일모집단에서 관심이 있는 연속형 변수의 평균(&mu;)값을 특정 기준값과 비교**하고자 할 때 사용하는 검정방법이다.

###### 2) 일표본 t-검정의 가정

- 일표본 t-검정에서는 **모집단의 구성요소들이 정규분포를 이룬다는 가정** 하에 검정통계량의 값을 계산한다.
- **종속형 변수는 연속형** 변수여야 하며, **검증하고자 하는 기준값이 있어야** 한다.

###### 3) 일표본 t-검정의 단계

- 1단계 : 가설 설정

- 2단계 : 유의수준 설정

- 3단계 : 검정통계량의 값 및 유의확률 계산

- 4단계 : 기각여부 판단 및 의사결정

  ​      

###### 4) R을 활용한 일표본 t-검정

##### 나. 대응표본 t-검정 (paired sample t-test)

###### 1) 대응표본 t-검정이란?

- **단일모집단에 대해 두 번의 처리를 가했을 때, 두 개의 처리에 따른 평균의 차이를 비교**하고자 할 때 사용하는 검정방법.

###### 2) 대응표본 t-검정의 가정

- **대응표본 t-검정에서는 모집단의 관측값이 정규성(정규분포를 만족한다는 가정)을 만족**해야 한다.

###### 3) 대응표본 t-검정의 단계

- 1단계 : 가설 설정
- 2단계 : 유의수준 설정
- 3단계 : 검정통계량의 값 및 유의확률 계산
- 4단계 : 기각여부 판단 및 의사결정

###### 4) R을 활용한 대응표본 t-검정

##### 다. 독립표본 t-검정 (independent sample t-test)

###### 1) 독립표본 t-검정이란?

- **두 개의 독립된 모집단의 평균을 비교 할때 사용하는 검정방법**

###### 2) 독립표본 t-검정의 가정

- 두 모집단은 **정규성**을 만족해야 함.
- 두 모집단은 서로 **독립적**이어야 함
- **등분산성** 가정을 만족해야 함
- 독립변수는 범주형, 종속변수는 연속형이어야 함

> 등분산성 : 두 모집단의 분산이 서로 같음을 의미

###### 3) 독립표본 t-검정의 단계

- 1단계 : 가설설정
- 2단계 : 유의수준 설정
- 3단계 : 등분산 검정
- 4단계 : 검정통계량의 값 및 유의확률 계산

###### 4) R을 활용한 독립표본 t-검정



#### 3. 분산분석 (ANOVA) (p.80)

##### 가. 분삭분석의 개념

-   두 개 이상 집단들의 평균 간 차이에 대한 통계적 유의성을 검증하는 방법

##### 나. 일원배치 분석 (One-way ANOVA)

###### 1) 특징

-   하나의 범주형 변수의 영향을 알아보기 위해 사용되는 검증 방법
-   모집단의 수에 제한이 없음
-   F 검정 통계량을 이용한다

###### 2) 가정

-   각 집단의 측정치는 서로 독립적이며, 정규분포를 따른다. (정규성 가정)
-   각 집단 측정치의 분산은 같다. (등분산 가정)

###### 3) 통계적 모형

###### 4) 분산분석표

###### 5) 가설검정

-   귀무가설(H<sub>0</sub>) : 집단 간 모평균에는 차이가 없다.
-   대립가설(H<sub>1</sub>) : 집단 간 모평균이 모두 같다고 할 수 없다.

###### 6) 사후 검정

-   귀무가설이 기각되었을 때, 어떤 집단들에 대해서 평균의 차이가 존재하는지를 알아보기 위해 실시하는 분석
-   종류 : 던칸(Duncan)의 MRT(Multiple Range Test) 방법, 피셔(Fisher)의 최소유의차(LSD; Least S... Difference)방법, 튜키(Tukey)의 HSD방법, Scheffe의 방법 등이 있다.

###### 7) R을 이용한 일원배치 분산 분석

##### 다.  이원배치 분산분석 (Two-way ANOVA)

###### 1) 특징

-   두 개의 범주형 변수 A, B의 영향을 알아보기 위해 사용되는 검증 방법

###### 2) 가정

-   정규성, 등분산성

###### 3) 통계적 모형

###### 4) 분산분석표

###### 5) 가설검정

-   귀무가설(H<sub>0</sub>) : 변수에 따른 종속변수의 값(반응값)에는 차이가 없다. &alpha; 와 &beta; 변수의 상호작용 효과가 없다.
-   대립가설 (H<sub>1</sub>) : 변수에 따른 종속변수의 값(반응값)에는 차이가 있다.&alpha; 와 &beta; 변수의 상호작용 효과가 있다.

###### 6) 교호작용 (Interaction Effection)

-   두 가지 이상의 특정 변수 조합에서 일어나는 효과

##### 라. 실험계획법 (DOE, Design Of Experiment)
###### 1) 실험 계획법의 개념

-   시스템이나 프로세스의 결과에 영향을 미치는 인자를 도출하고, 측정 데이터를 통계적으로 분석하기 위한 실험을 설계하는 방법을 의미한다.
-   실험 방식, 데이터 수집 방법, 활용 통계 기법 등 실험의 모든 과정을 설계한다.
-   최소 실험 횟수로 최대의 정보를 얻는 것을 목적으로 한다.



###### 2) 계획 설계의 목적

-   분산분석 및 검정과 추정의 문제 : 어떠한 요인이 특성치 변화에 유의미한 영향을 주는지, 또한 해당 요인의 영향이 어느 정도인지를 파악
-   최적 반응 조건의 결정 문제 : 어떤 인자를 사용해야 가장 원하는 결과값을 얻을 수 있는지를 파악
-   오차항 추정의 문제 : 이해하기 어렵던 오차와 그 변동에 관한 정도를 파악



###### 3) 실험계획의 원리

-   랜덤화의 원리 (Randomization) : 실험순서를 무작위로 선택하여 실시
-   반복의 원리 (Replication) : 인자의 동일 수준 내에서 최소 두 번 이상 실험을 진행
-   블록화의 원리(Blocking) : 험 전체를 시간적, 공간적으로 분할하여 블록으로 만듦
-   직교화의 원리 (Orthogonality) :요인각 직교성을 갖도록 실험을 계획
-   교락의 원리 (Confounding) : 고차항의 교호효과와 블록효과를 교락시키는 방법



###### 4) 주요 용어

-   인자 (Factor) : 실제 실험의 대상, 입력변수 X
-   특성치 (Characteristic Value) : 실험의 모든 결과값, 출력변수 Y
-   수준 (Level) : 실험하기 위한 인자의 조건, 인자의 정도나 값
-   주효과 (Main Effect) : 각 입력변수의 수준간 차이, 인자가 독립적으로 반응에 미치는 영향
-   교호효과 (Interaction Effect) : 특정한 인자 수준의 조합에서 일어나는 효과, 인자들이 혼합되어 반응에 미치는 영향
-   교락 (Confounding) : 2개 이상의 효과 (주효과 또는 교호효과)를 구별할 수 없도록 계획적으로 조합하는 것
-   블록 (Block) : 실험 단위가 균일할 수 있도록 단위를 모은 것
-   반복 (Replication) : 인자들의 동일한 수준 조합에서 다회의 실험을 진행
-   중복 (Repetition) : 한 실험에서 여러 개의 대상을 측정



###### 5) 실험계획법의 종류

1.   요인배치법 (Factorial Design) 
     -   **모든 인자간의 수준 조합에서 실험**이 이루어지는 완전랜덤화방법. 
     -   교호효과를 포함한 모든 요인효과를 추정할 수 있다.
2.   분할법 (Split-plot Design)
     -   완전랜덤화하기 힘들 경우, 몇 단계로 분할하여 각 단계별로 완전 랜덤하게 실험 순서를 결정하는 방법. 
     -   랜덤화가 어려운 것을 1차 단위로, 비교적 쉬운 것을 후 단위로 배치.
3.   교락법 (Confounding method)
     -   **검출할 필요가 없는 교호작용을 다른 요인과 교락하도록 배치**하는 방법.
     -   실험 횟수를 늘리지 않고 실험 전체를 몇 개의 블록으로 나누어 배치하는 방법, 동일 환경에서의 실험 횟수를 줄일 수 있다.
     -   고차의 교호작용을 블록에 교락시키기 때문에, **주효과가 높게 추정**된다.
4.   난괴법 (Randomized Block Design)
     -   실험 단위를 몇 개의 반복으로 나누어 배치하는 방법
     -   실험 오차를 줄일 수 있기 때문에 효율이 높고 비교적 분석이 간단






#### 4. 교차분석 (p.87)

##### 가. 교차분석 (검정)

###### 1) 교차분석의 개념 및 특징

-   범주형 자료인 두 변수 간의 관계를 알아보기 위해 실시하는 분석 기법
-   적합도 검정, 독립성 검정, 동질성 검정에 사용
-   카이제곱($x^2$) 검정 통계량을 이용

###### 2) 교차표

-   두 변수의 각 범주를 교차하여 도수(빈도)를 표 형태로 나타내면 아래와 같다.

    ![cross-table](https://user-images.githubusercontent.com/291782/180609567-a99732d8-7c5b-48f3-9d38-81671cf11745.png)

-   교차분석은 교차표에서 각 셀의 관찰빈도와 기대빈도간의 차이를 검정한다.


##### 나. 적합도 검정

###### 1) 적합도 검정의 의미

-   실험에서 얻어진 관측값들이 예상한 이론과 일치하는지 아닌지를 검정하는 방법
-   관측값들이 어떠한 이론적 분포를 따르고 있는지를 알아볼 수 있다.
-   **모집단 분포에 대한 가정이 옳게 되었는지**를 관측 자료와 비교하여 검정하는 것

###### 2) 가설 설정

-   n개의 표본 자료를 k개의 범주로 분류한 뒤, 각 범주의 관측도수(O)와 주어진 확률 분포에 대해 각 범주에 속하는 기대도수(E)를이 적합하는지의 여부를 검정하는 것
-   귀무가설(H0) : 실제 분포와 이론적 분포 간에는 차이가 없다. (두 분포가 일치한다)
-   대립가설(H1) : 실제 분포와 이론적 분포 간에는 차이가 있다. (두 분포가 일치하지 않는다)

###### 3) 검정 통계량

-   $O_i$: 관찰도수, $E_i$: 기대도수, i=1,2,...,k
-   $X^2 = \displaystyle \sum_{i=1}^k \dfrac {(O_i - E_i)^2} {E_i} $
-   X<sup>2</sup> 통계량 값이 큰 경우 : 관찰도수와 기대도수의 차이가 크며, 적합도가 낮다. 즉 일치한다고 볼 수 없다. 
-   X<sup>2</sup> 통계량 값이 작은 경우 : 관찰도수와 기대도수의 차이가 작으며, 적합도가 높다. 즉 일치한다고 볼 수 있다. 

###### 4) 자유도

-   $df = k - 1$

###### 5) R을 이용한 적합도 검정

-   chisq.test(x, y, p)


##### 다. 독립성 검정

###### 1) 독립성 검정의 의미

-   **범주화** 된 두 변수 **A, B 사이의 관계가 독립인지 아닌지를 검정**
-   검정 통계량을 계산할 때는 **교차표를 활용**

###### 2) 가설 설정

-   두 변수 A, B가 서로 독립적으로 관측값에 영향을 미치는 지의 여부를 검정
-   귀무가설 : 두 변수 사이에 연관이 없다. (독립이다)
-   대립가설 : 두 변수 사이에 연관이 있다. (종속이다)

###### 3) 검정 통계량

-   n: 전체관측도수, $O_i$: 행의 합, $O_j$: 열의 합, $O_{ij}$: 관찰빈도
-   $E_{ij} = \dfrac {O_i \times O_j } {n}$ : 기대빈도
-   $X^2 = \displaystyle \sum_{i=1}^r \sum_{j=1}^c \dfrac {(O_{ij} - E_{ij})^2} {E_{ij}}$
-   $X^2$ 통계량 값이 큰 경우 : 두 변수 사이에는 연관이 있다. 즉, 두 변수는 종속 관계이다.
-   $X^2$ 통계량 값이 작은 경우 : 두 변수 사이에는 연관이 없다. 즉, 두 변수는 독립 관계이다.

###### 4) 자유도

-   R : 행의 수, C: 열의 수
-   $df = (R-1)(C-1)$



##### 라. 동질성 검정

###### 1) 동질성 검정의 의미

-   모집단이 임의의 변수에 따라 R개의 속성으로 범주화되었을 때, R개의 부분 모집단에서 추출한 각 표본인 C개의 범주화된 집단의 분포는 서로 동일한지 아닌지를 검정하는 것을 의미한다.

-   검정 통계량 값을 계산할 때는 **교차표를 활용**하며, 계산법과 검증법은 모두 **독립성 검정과 같은 방법**으로 진행된다.

    

###### 2) 가설 설정

-   j = 1, 2, ..., c 이다.
-   귀무가설 : P<sub>1j</sub> = P<sub>2j</sub> = ... = P<sub>rj</sub> (모든 P<sub>nj</sub> (n=1,2,...r)는 동일하다)
-   대립가설 : Not H<sub>0</sub> (P<sub>nj</sub> (n=1,2,...r) 중 다른 값이 하나 이상 존재한다.)

###### 3) 검정 통계량

-   n: 전체관측도수, O<sub>i</sub> : 행의 합, O<sub>j</sub>: 열의 합, O<sub>ij</sub> : 관찰빈도
-   $E_{ij} = \dfrac {O_i \times O_j} {n}$: 기대빈도
-   $X^2 = \displaystyle \sum_{i=1}^r \sum_{j=1}^c \dfrac {(O_{ij} - E_{ij})^2} {E_{ij}}$
-   $X^2$ 통계량 값이 큰 경우 : $P_{nj}(n=1,2,..., r)$ 중 다른 값이 하나 이상 존재한다.
-   $X^2$ 통계량 값이 작 경우 : 모든  $P_{nj}(n=1,2,..., r)$ 는 동일하다.

###### 4) 자유도

-   $df = (R-1)(C-1)$   R: 행의 수, C: 열의 수



#### 5. 중심극한정리 (Central Limit Theorem)

##### 가. 개념

-   표본의 개수 n이 커질수록 표본 평균의 분포(표집분포)가 정규분포에 가까워지는 현상을 의미
-   평균이 &mu;(뮤)이고, 분산이 &sigma;<sup>2</sup>(sigma) 인 모집단에서 크기가 n인 확률표본을 추출.
-   표본평균($\bar{X}$)은 표본의 크기 n이 크면 근사적으로 평균이 &mu;이고 분산이 $\dfrac {\sigma^2}{n}$인 정규 분포를 따른다.
-   즉, 중심극한정리에서는 표본의 크기가 커질수록 표본평균은 모집단의 평균에 가까워진다는 의미를 가진다.

##### 나. 중심극한정리

-    $N(\mu, \dfrac{\sigma^2}{n})$
-   $Z = \dfrac {\overline {X} - \mu}{\dfrac {\sigma}{\sqrt{n}}}$



### 2절 회귀분석 

#### 1. 정규화 선형회귀 (Regularized Linear Regression) (p.92)

-   선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상(과적합, Overfitting)을 막는 방법
-   **계수의 크기를 제한**하는 방법으로 제약조건을 추가한다.
-   제약 조건의 종류에 따라 **Ridge회귀, LASSO회귀, Elastic Net 회귀모형**

##### 가. 릿지회귀 (Ridge Regression)

-   **가중치들의 제곱합(Squared Sum of weight)을 최소화하는 것을 제약조건으로 추가하는 기법**

-   가중치의 모든 원소가 0에 가까워지는 것을 원하며, 규제 방식을 **L2 규제(Penalty)**라고 한다.

-   &lambda;는 제약조건의 비중을 조절하기 위한 하이퍼 모수(Hyper parameter)에 해당하며, 람다가 커지면 가중치의 값들이 작아지며, 정규화 정도가 커진다. 람다가 작아지면 정규화 정도가 작아지고, 람다가 0이 되면 일반적인 선형회귀 모형이 된다.

    ![ridge-regression](https://user-images.githubusercontent.com/291782/180651450-0f62af00-fb85-4607-baec-e8587d8b495a.png)

    

##### 나. 라쏘회귀 (LASSO Regression)

-   라쏘 (Least Absolute Shrinkage and Selection Operator)는 **가중치 절대값의 합을 최소화하는 것을 제약조건으로 추가**하는 기법

-   가중치가 0에 가까워질 뿐, 실제 0이 되지는 않는다. 하지만 중요하지 않은 가중치는 0이 될 수도 있다.

-   라쏘에서 사용하는 규제방식을 **L1 규제**라고 한다.

    ![lasso-regression](https://user-images.githubusercontent.com/291782/180651548-637ea709-6c6c-448d-a59f-88cdb08d76f2.png)

    

##### 다. 엘라스틱넷(Elastic Net)

-   릿지와 라쏘를 결합한 모델

-   &lambda;<sub>1</sub> 와 &lambda;<sub>2</sub> 두 개의 하이퍼 모수를 가짐

    ![elastic-net](https://user-images.githubusercontent.com/291782/180651632-4979a3ca-b9ee-471e-84df-7cc616968b76.png)



#### 2. 일반화 선형회귀 (GLM, Generalized Linear Regression) (p.93)

-   종속변수를 적절한 함수로 변화시켜 f(x)를 정의한 후, 이 f(x)와 독립변수를 선형 결합으로 모형화하는 '일반화 선형모형(glm)'을 이용한다.
-   일반화 선형모형은 3가지 성분에 의해서 정의 된다.
    -   랜덤성분 (Random Component) : 종속변수 y의 확률분포를 규정하는 성분
    -   체계적 성분 (systematic component) : y의 기댓값인 E(y)를 정의하는 설명변수들 간의 선형 결합(선형식)
    -   연결함수 (link function) : 랜덤성분과 체계적 성분을 연결하는 함수



#### 3. 회귀분석의 영향력 진단 (p.94)

-   영향력 진단이란 **적합된 회귀모형의 안전성을 평가하는 통계적인 방법**이다.
-   회귀직선의 기울기에 영향을 크게 주는 점을 ==영향점==이라고 한다.
-   영향력 진단 방법에는 **Cook's distance, DFBETAS, DFFITS, Leverage H** 등이 있다.
    -   Cook's distance : 쿡의 거리가 기준값인 1보다 클 경우에 영향치로 간주
    -   DFBETAS : **기준 값은 2**나 $\dfrac {2}{\sqrt{n}}$(표본을 고려한 경우)을 사용하며, DFBTAS 값이 **기준값보다 클 경우 영향치**로 간주
    -   DFFITS : **기준값인**  $2\sqrt{\dfrac {(p+1)} {n}}$ 보다 **클수록 영향치일 가능성이 높다고 본다**.
    -   Leverage H : 관측치가 다른 관측치보다 집단으로부터 떨어진 정도를 의미하며, $2 \times \dfrac {p+1} {n}$ **보다 크면 영향치이거나 이상치**라고 본다.



#### 4. 더빈 왓슨(Durbin Watson) 검정

-   오차항이 독립성을 만족하는지를 검정하기 위해 사용
-   더빈 왓슨 동계량이 2에 가까울수록 오차항의 자기상관이 없음을 의미
-   0에 가까울수록 양의 상관관계, 4에 가까울수록 음의 상관관계가 있음을 의미



#### 5. 벌점화된 선택기준 : 변수 선택의 기준으로 사용되는 통계량

##### 가. 수정된 결정계수 (Adjuestd R square)

-   설명변수의 계수가 증가하면 결정계수도 함께 증가하는 속성을 가진다.
-   따라서 수정된 결정계수를 이용해 이러한 단점을 보완하고 변수를 선택할 수 있다.수정된 결정계수는 변수의 개수가 증가함에 따라 처음에는 감소하다가 점점 안정화되고 나중에는 약간 증가하는 경향을 가진다.
-   수정된 결정계수를 이용하여 변수를 선택할 경우, MSE 값이 최소인 시점의 모형을 선택하거나 이 값의 최소와 비슷해서 더 이상 변수를 추가할 필요가 없는 시점의 모형을 선택하게 된다.



##### 나. Mallow's CP

-   Cp 값은 최소자승법(ordinary least squares)을 사용하여 추정된 회귀모형의 적합성을 평가하는데 사용
-   일반적으로 **Cp값이 작고, p+상수 (변수의 개수 + 상수)에 가까운 모형을 선택**한다.






#### 6. 변수 변환 (p.96)

-   회귀분석의 기본가정인 ==정규성, 선형성, 등분산성== 가정을 만족하지 못하는 경우, 변수를 변환함으로써 교정할 수 있다.

##### 가. 변수변환법의 종류

-   로그변환, 제곱근변환 : 대부분의 값이 작은 값으로 구성되어 있는 데이터를 정규화하기 위해 사용
-   지수변환, 제곱변환 : 대부분의 값이 큰 값으로 구성되어 있는 데이터를 정규화하기 위해 사용

##### 나. 더미변수 생성

-   더미변수는 변수의 범주의 수 -1 개 이다.
-   ![dummy](https://user-images.githubusercontent.com/291782/167261233-9cc99675-dae4-4789-8fb7-90fbe525af71.png)

##### 다. Box-cox 변환

-   정규분포를 따르지 않는 반응변수를 정규성을 만족하도록 변환하기 위해 사용하는 방법으로 아래와 같이 y를 $g_{\lambda}(y)$로 변환한다.

-   **&lambda;는 우도함수를 최대화 시키는 조건**으로 계산된다.

    ![box-cox](https://user-images.githubusercontent.com/291782/180652640-157e7ff3-f2bb-4fb9-88d4-cd871bf3c09b.png)




### 2장 정형 데이터마이닝

### 1절 데이터마이닝 개요

#### 1. Feature Selection (변수선택) (p.108)

##### 가. Filter Method

- 각 변수들에 대해 통계적인 점수를 부여 후, 부여된 점수를 이용하여 변수의 순위를 매기고 변수 선택을 진행
- 예) Chi squared test, information gain, correlation coefficient scores 등

##### 나. Wrapper Method

- 변수간의 상호 작용을 감지할 수 있도록 **변수의 일부만을 모델링에 사용한 후 그 결과를 평가하는 작업을 반복**하면서 변수를 선택해 나가는 방법
- 예) recursive feature elimination algorithm

##### 다.  Embedded Method

- Filter method 와 wrapper method를 결합하여 어떤 변수가 가장 크게 기여하는지를 찾아내는 방법으로 **과적합을 줄이기 위해 내부적으로 규제를 가하는 방식**이 사용된다.
- 예) LASSO, Ridge Regression, Elastic Net 등

#### 2. DeepLearning

##### 가. 머신러닝

- 인공지능에 포함되는 개념으로, 경험적인 데이터를 바탕으로 기계가 지식을 습득하여 스스로 성능을 향상하는 기술을 의미
- 학습 방법에 따라 구분
    - 지도학습 (Supervised learning) : 출력값에 대한 정답을 컴퓨터에게 알려줌
    - 비지도학습 (Unsupervised learning) : 출력값에 대한 정답을 컴퓨터에게 안 알려줌
    - 강화학습 (Reinforcement learning) : 출력값의 정답이 주어지지 않은 상태에서 일련의 행동 결과에 대한 보상(reward)이 주어짐

![ai-ml-dl](https://user-images.githubusercontent.com/291782/181033556-e23da061-6d96-4c52-833a-f3cd84e5c45f.png)

-   AI > ML > DL




##### 나. 딥러닝

- **인공신경망에 기반을 둔 기계학습**의 한 종류로, 여러 비선형 변환기법의 조합을 통해 많은 데이터로 부터 특징들을 학습하는 방법
- 종류
    - DNN (Deep Neural Network, 심층 신경망) : 인공신경망(Artifitial Neural Network, ANN)은 입력층, 은닉층, 출력층의 구조로 이루어져 있는데, **심층신경망**(DNN)은 입력층과 출력층 사이에 **여러개의 은닉층**들로 이루어진 신경망 구조
    - 예) 암 진단 시스템, 주가지수 예측, 기업신용평가, 환율 예측 등
    - CNN (Convolutional Neural Network, 합성곱 신경망) : 다계층 퍼셉트론의 한 종류로 여러개의 합성곱 계층과 일반적인 인공 신경망 계층들로 이루어져 있음.
    - 예) 자율 주행 자동차, 이미지, 텍스트, 사운드, 비디오 인식 및 식별 등의 영상, 그림 인식 분야 등
    - RNN (Recurrent Neural Network, 순환 신경망) : 시간의 흐름에 따라 변화하는 데이터를 학습하기 위한 딥러닝 알고리즘으로 기준 시점과 다음 시점에 네트워크를 연결하여 구성한 인공신경망이라고 할 수 있다.
    - 예) 음성 인식, 자동 번역, 단어 의미 판단, 이미지 캡션 생성 등의 자연어 처리 분야 등



##### 다. 프로그래밍 언어별 딥러닝 지원 라이브러리

1) 파이썬 : Theano (Keras, Lasagne), Chainer, Tensorflow, CXXNET
2) C++ : Caffe (이미지 분석 특화), Mxnet (아마존 웹서비스에서 딥러닝 프레임워크를 지원)
3) JAVA : DeepLearning4j (DL4j, 비즈니스용 딥러닝 플랫폼으로 널리 사용)
4) R : darch (RBM, DBN 등의 구조를 가진 신경망 구현을 지원), deepnet






### 2절 분류분석

#### 1. 나이브 베이즈 분류 (Naive Bayes Classification) (p. 112)

##### 가. Bayes theorem (베이즈 정리)

- **두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리**

- 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 다음과 같이 구할 수 있는것

- >  $P(A|B) = \dfrac{P(B\cap A)}{P(B)} = \dfrac {P(B|A)P(A)} {P(B)} = \dfrac {P(B|A)P(A)} {P(B|A)P(A) + P(B|A^C)P(A^C)}$

- P(A|B) : 사건 B가 발생했을 때 사건 A가 발생할 확률 > 사후확률 (posterior)

- P(B|A) : 사건 A가 발생했을 때 사건 B가 발생할 확률 > 우도 (likelihood)

- $P(A \cap B)$ : 사건 A와 B가 동시에 발생할 확률

- P(A) : 사건 A가 발생할 확률 > 사전확률 (prior)

- P(B) : 사건 B가 발생할 확률 > 관찰값 (evidence)

- 위 식을 다음과 같은 식으로도 표현 가능

- > $posterior = \dfrac {prior * likelihood} {evidence}$

![naive-bayes](https://user-images.githubusercontent.com/291782/149884588-2373a115-d8ea-400f-b5e7-66a7ae7cf3c5.png)

##### 나. 나이브 베이즈 분류의 개념

- 변수들에 대한 조건부 독립을 가정하는 알고리즘으로, 어떤 데이터가 특정 클래스에 속하는지를 분류하는 알고리즘이다.
- 문서를 **여러 범주**(예, 스팸, 경제, 스포츠) **중 하나로 판단하는 문제**에 대한 솔루션으로 사용 가능

##### 다. 나이브 베이즈 분류의 계산

- 나이브 베이즈 분류는 하나의 속성 값을 가진으로, 다른 속성이 독립적이라 전제했을 때 해당 속성 값이 클래스 분류에 미치는 영향을 측정한다.
- 속성값에 대해 다른 속성이 독립적이라는 가정은 **클래스 조건 독립성 (Class conditional independence)**라 한다.


#### 2. K-Kearest Neighbor Classification (KNN, K-최근접 이웃 알고리즘) (p.114)

- 특정 범주로 나뉘어진 데이터가 있을 때, 새로운 데이터가 추가 된다면 어떤 범주로 분류할 것인지를 결정할 때 사용할 수 있는 분류 알고리즘. 지도학습의 한 종류

##### 가. KNN 알고리즘의 원리

- 새로운 **데이터의 클래스를 해당 데이터와 가장 가까운 k개 데이터들의 클래스(범주)로 결정**한다.

- 이웃간의 거리를 계산할 때 **유클리디안 거리**(대표적 사용), 맨하탄 거리, 민코우스키 거리 등을 사용

- 유클리디안 거리는 두 점 p와 q가 각각 P=(p1, p2, p3.. , pn), Q = (q1, q2, ... , qn) 의 좌표를 가질때 아래와 같은 공식

- $유클리디안 거리 = \displaystyle \sqrt {(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2} = \sqrt {\displaystyle \sum_{i=1}^n {(p_i - q_i)^2}}$

    ![knn](https://user-images.githubusercontent.com/291782/181268324-c932d18e-c69e-408a-94bd-160f17876c58.png)

    

##### 나. k의 선택

- k의 선택은 학습의 난이도와 데이터의 개수에 따라 결정될 수 있으며, 일반적으로는 **훈련 데이터 개수의 제곱근으로 설정**.

##### 다. KNN 분류 예시

##### 라. KNN의 장단점

- 장점
    - 사용이 간단
    - 범주를 나눈 기준을 알지 못해도 데이터를 분류할 수 있다.
    - 추가된 데이터의 처리가 용이하다
- 단점
    - **k값의 결정이 어렵**다.
    - 비수치 데이터의 경우 유사도를 정의하기 어렵다.
    - 데이터 내에 **이상치가 존재하면 성능에 큰 영향**을 받는다.



#### 3. SVM (Support Vector Machine) (p.116)

##### 가. 개념

- 기계학습 분야 중 하나로 패턴인식, 자료 분석 등을 위한 **지도학습 모델**이며 주로 **회귀와 분류 문제 해결**에 사용된다.
- 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어떤 범주에 속할 것인지를 판단하는 **비확률적 이진 선형 분류 모델을 생성**한다.

##### 나. 작동 원리

- 데이터가 사상된 공간에서 경계로 표현되며, 공간상에 존재하는 여러 경계 중 **가장 큰 폭을 가진 경계를 찾는다**.
- 각 그룹을 구분하는 분류자를 **결정 초평면**(decision hyperline)
- 초평면에 **가장 가까이에 붙어있는 최전방 데이터들을 서포트 벡터**(support vector)
- 서포트 벡터와 초평면 사이의 **수직거리를 마진**(margin)이라 한다.
- SVM은 고차원 혹은 무한 차원의 공간에서 **마진(margin)을 최대화하는 초평면(MMH, Maximum Margin Hyperplane: 최대마진초평면)을 찾아 분류와 회귀를 수행**한다.
- 선형 분류뿐만 아니라 **비선형 분류에도 사용**되는데, 비선형 분류에서는 입력자료를 다차원 공간상으로 매핑(mapping)할 때 **커널 트릭**(kernel trick)을 사용하기도 한다.

![svm-desc](https://user-images.githubusercontent.com/291782/149886433-8723750d-fda3-4528-bba4-4a5a1858b112.png)

> 초평면은 어떤 n차원의 공간보다 한 차원이 낮은 n-1차원의 하위공간(sub space)를 의미함. 즉 2차원 공간에서 초평면은 선이 된다.

##### 다. SVM의 장단점

- 장점
    - 분류와 예측에 모두 사용 가능
    - 신경망 기법에 비해 과적합 정도가 낮다.
    - 예측의 정확도가 높다.
    - 저차원과 고차원의 데이터에 대해서 모두 잘 작동한다.
- 단점
    - 데이터 전처리와 매개변수 설정에 따라 정확도가 달라질 수 있다.
    - 예측이 어떻게 이루어지는지에 대한 이해와 모델에 대한 해석이 어렵다.
    - 대용량 데이터에 대한 모형 구축 시 속도가 느리며, 메모리 할당량이 크다.



### 3절 군집분석

#### 1. Resampling (재표본추출) (p.118)

- 표본을 수많이 재추출하고, 재추출된 표본에 모형을 적합하게 함으로써 생성된 분류기의 성능 측정에 대한 통계적 신뢰도를 높이는 방식이 리샘플링 기법이다. 대표적으로 **k-fold cross validation, 홀드아웃 방법, 붓스트랩** 등이 있다.

##### 가. K-Fold cross validation

- 데이터를 k개의 집단으로 나눈 뒤 k-1개의 집단으로 분류기를 학습 시키고, 나머지 1개의 집단으로 분류기의 성능을 테스트

- 위 과정을 k번 반복하여 모든 데이터가 학습과 검증에 사용될 수 있도록 하고, 최종적으로 k번의 테스트를 통해 얻은 MSE (평균제곱오차)값들의 평균을 해당 모델의 MSE 값으로 사용

    ![k-fold](https://user-images.githubusercontent.com/291782/181273089-b11fce0c-93c6-4ac7-b9b0-5dea65dbebaa.png)

    

##### 나. 붓스트랩 (bootstrap)

- 모집단에서 추출한 표본(샘플)에 대해서 또 다시 재표본(샘플)을 여러 번 추출하여 모델을 평가하거나 데이터의 분포를 파악하는 재표본추출 방법

- **단순랜덤 복원추출법**을 사용하여 표본을 여러개 생성하므로, 특정 데이터가 샘플에 포함될수도 있고, 안될수도 있다.

- **샘플에 한 번도 선택되지 않는 데이터**가 발생할 확률은 **36.8%**이며, 이러한 데이터를 **OOB (out-of-bag) 데이터**라고 하며, OOB데이터의 실제값과 예측값 사이의 오차로 정의되는 값을 **OOB error**라고 한다.

    ![bootstrap](https://user-images.githubusercontent.com/291782/181273706-6872dc51-6cf1-44b0-88bc-384659e1ed2c.png)

    

#### 2. 군집화 기법 종류 (p.120)

##### 가. 밀도기반 군집분석

-   어느 점을 기준으로 주어진 반경 내에 최소 개수만큼의 데이터들은 가질수 있도록 함으로써 특정 밀도함수 혹은 밀도에 의해 군집을 형성해 나가는 기법이다.

-   **DBSCAN** (Density Based Spatial Clustering of Application with Noise): 밀도 한계점에 따라 군집을 형성해 나가는 **대표적인 밀도기반 군집화 기법**으로, 군집화와 동시에 noise를 표시함으로써 데이터를 정확하게 이해할 수 있다.

-   **OPTICS** : 군집화 구조 식별을 위해 부가적 **순서를 생성**하는 밀도기반 기법이다.

-   **DENCLUDE** (DENsity based CLUstEring) : **밀도 분포함수에 기초**한 군집화방법이다.

    >   Density : 밀도, Spatial : 공간,  Cluster: 집단(군집), 

##### 나. 격자기반 군집분석

-   데이터가 존재하는 공간을 격자구조로 이루어진 유한개의 셀들로 양자화한 뒤, 데이터 포인트 대신 셀을 이용해 군집화 과정을 수행하는 기법

-   빠른 처리시간을 가지며, 데이터 내 객체 수에 독립적이고 양자화된 공간의 각 차원에서 **셀의수에만 의존**한다.

-   **STING** (STatistical INformation Grid) : 격자 셀에 저장되어 있는 통계정보를 탐색하는 격자기반 기법이다.

-   **WaveCluster** : **Wavelet 변환 기법을** 사용하여 객체들을 군집화하는 격자기반 기법이다.

-   **CLIQUE** (CLustering In QUEst) : **고차원 데이터 공간의 군집화**를 위한 **격자 및 밀도기반** 기법에 해당한다.

    >   Statistical : 통계, Wavelet : 0을 중심으로 증가와 감소를 반복하는 진폭을 수반한 파동같은 진동





#### 3. 군집분석의 타당성 지표 (p.121)

##### 가. Silhouette (실루엣)

- **군집내의 응집도와 군집 간 분리도를 이용한 지표**로, 군집 내 요소간의 거리가 짧고 서로 다른 군집간 거리가 멀수록 커진다.
- 완벽한 군집화가 이루어졌을 경우 1, 군집화가 전혀 이루어지지 않은 경우에는 -1 값을 가진다.
- $s(i_ = \dfrac {b(i) - a(i)} {max[a(i), b(i)]}) \quad$
- a(i) = i번째 개체와 같은 군집에 속한 요소들 간 거리들의 평균, b(i) = i번째 개체와 다른 군집에 속한 요소들 간 거리들의 평균을 군집마다 구했을 때의 최솟값

##### 나. Dunn Index

- 군집 간 거리의 최소값을 분자, 군집 내 요소 간 거리의 최대값을 분모로 하는 지표
- 분자값이 클수록 군집 간 거리가 크고, 분모값이 작을수록 군집내의 요소들이 모여있다는 것을 의미
- **군집 간 거리는 멀고, 군집 내 분산은 작을수록 군집화가 잘 이루어진 것**이기 때문에 **Dunn Index가 클수록 군집화가 잘 형성**되어 있다고 볼 수 있음.

#### 4. BMU (Best-Matching Unit) (p.121)

- SOM (Self Organizing Maps) 에서 **표본 벡터와 거리가 가장 가까운 프로토타입 벡터**를 선택하는데, BMU는 이 때 선택된 프로토타입 벡터를 나타내는 용어이다.





### 3장 비정형 데이터마이닝

### 1절 텍스트마이닝

#### 1. 텍스트 마이닝 (Text Mining) (p.129)

-   **입력된 텍스트를 구조화해 그 데이터에서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정**
-   **다양한 포맷의 문서로부터 텍스트를 추출**해 단어 구성에 따라 데이터 마트를 구성한다.
-   인터넷 데이터, 소셜미디어 데이터 등과 같은 **자연어로 구성된 비정형 텍스트 데이터 속에서 정보나 관계를 발견**하는 분석 기법

#### 2. 텍스트 마이닝 기능

-   문서요약 (Summarization)
-   문서 분류 (classification)
-   문서 군집 (clustering)
-   특성 추출 (feature extraction)

#### 3. 정보 검색의 적절성

-   분석 결과를 평가하기 위해 **정확도와 재현율**을 사용한다.
-   정확도 (precision): 분석 모델이 정답이라고 예측한 결과중에서 실제로 정답인 경우의 비율 (TP / (TP + FP))
-   재현율 (Recall) : 실제로 정답인 것들 중에서 분석 모델이 정답이라고 내놓은 결과의 비율 (TP / (TP + FN))

![precision_recall](https://user-images.githubusercontent.com/291782/150641056-4425fc9d-36be-4369-9c35-f76b1522c204.png)

#### 4. Corpus

-   데이터마이닝의 절차 중 **데이터의 정제, 통합, 선택, 변환의 과정을 거친 구조화된 단계**로 더 이상 **추가적인 절차 없이 데이터 마이닝 알고리즘 실험에 활용될 수 있는 상태**이다.
-   R의 텍스트마이닝 패키지인 `tm`에서 문서를 관리하는 기본 구조이며, 텍스트 문서들의 집합을 의미한다.

가. `tm` 패키지의 함수

-   VCorpus():  문서를 Corpus class로 만들어주는 함수, 결과는 메모리에 저장되어 현재 구동중인 R메모리에서만 유지된다.
-   PCorpus(): 문서를 Corpus class로 만들어 R외부의 DB나 파일로 관리되게 하는 함수
-   DirSource(), VectorSource(), DataframeSource(): 텍스트를 저장한 디렉토리, 벡터, 데이터프레임으로부터 코퍼스 생성을 위한 소스를 만들어 주는 함수들이다.

나. `tm`패키지의 문서 전처리

-   word, pdf, csv 등 다양한 문서형식을 읽어 들일 수 있다.
-   tm_map(x, FUN): x 데이터에 대해 FUN에 지정한 함수를 적용해주는 함수로, 아래와 같이 FUN 인자에 다양한 함수를 지정하여 활용할 수 있다.
    -   tm_map(data, as.PlainTextDocument) : XML 문서를 text 로 전환
    -   tm_map(data, stripWhitespace) : space 제거
    -   tm_map(data, tolower) : 대문자를 소문자로 변환
    -   tm_map(data, removewords, stopwords("english")) : 띄어쓰기, 시제 표준화
-   DocumentTermMatrix : Corpus로부터 문서별 특정 문자의 빈도표 생성
-   TermDocumentMatrix : Corpus 로 부터 단어별 문서의 빈도표 생성

#### 5. Term-Document Matrix

-   텍스트 마이닝을 불러온 문서에 대해 `plain text`로 전환, `공백 제거`, `lowercase`로 변환, 불용어(`stopward`) 처리, 어간추출(`stemming`) 등의 작업을 수행한 다음에 문서 번호와 단어 간의 사용 여부 또는 빈도수를 이용해 matrix를 만드는 작업이 term document matrix이다.

```R
> data(crude, package = "tm")
> m <- TermDocumentMatrix(crude, control = list(removePunctuation=T, stopwords=T))
> inspect(m) #-- 단어별 문서에서 나온 갯수
> findFreqTerms(m, 10) #-- 10개 이상 사용된 단어를 표시
> findAssocs(m, "oil", 0.7) #-- "old" 단어와 70프로 이상 연관성이 있는 단어를 표시
```



#### 6. Dictionary

-   복수 문자들의 집합으로 **텍스트 마이닝 분석 시 사용하고자 하는 단어들의 집합**이다
-   분석하고자 하는 단어들을 별도의 사전으로 정의해서 해당 단어들에 대해서만 결과를 산출해 보려고 할 떄 사용한다.

#### 7. 감성분석

-   문장에서 사용된 단어의 긍정과 부정 여부에 따라 전체 문장의 긍정/부정 여부를 평가한다.
-   브랜드에 대한 평판 분석 가능
-   각 문장의 긍정/부정 여부는 분석 주체에 따라 다르게 해석할 수 있다.

#### 8. 한글 처리

-   R의 텍스트 마이닝 패키지 : KoNLP
-   KoNLP 패키지 사용을 위해서는 rJava 패키지, JRE 프로그램을 반드시 추가 설치해야 함
-   명사를 추출할때는 extractNoun("문장") 함수를 사용

#### 9. 워드 클라우드

-   문서에 포함된 단어들의 사용 빈도를 효과적으로 보여주기 위해 단어들을 크기, 색 등으로 나타내어 구름 등과 같은 형태로 시각화 하는 기법
-   R패키지 : wordcloud





### 2절 사회연결망 분석

#### 1. 사회연결망 분석 (p.135)

##### 가. SNA (Social Network Analysis) 정의

-   **개인과 집단들 간의 관계를 노드와 링크로 모델링**하여 그것의 위상구조와 확산 및 진화 과정을 계량적으로 분석하는 방법론
-   개인의 인간관계가 인터넷으로 확대된 사람 사이의 네트워크
-   **개인 또는 집단이 하나의 노드(node)**, **연결은 선(link 또는 edge)으로 표현**됨

##### 나. SNA 분류

1. 집합론적 방법

   - **각 객체들 간의 관계를 관계 쌍(pairs of elements)으로 표현**

     ![sna-pair](https://user-images.githubusercontent.com/291782/182100051-5a2c497f-d321-49a9-81fb-8f6d7d4c105f.png)

     

2. 그래프 이론을 이용한 방법

   - 객체를 점(노드 or 꼭지점)으로 표현하고, 연결은 두 점을 연결하는 선으로 표현

     ![sna-graph](https://user-images.githubusercontent.com/291782/182100213-dce9ad48-af1a-4129-80ac-72c3f26761c4.png)

     

3. 행렬을 이용한 방법

   - 각 객체를 행과 열에 배치하고, 각 객체간의 관계가 존재하면 1을 넣고, 존재하지 않으면 0을 넣음.

     ![sna-matrix](https://user-images.githubusercontent.com/291782/182100345-cecd1971-b3d9-466a-b292-1e30b76fd880.png)

     - 1원(1 mode)자료 : 행과 열에 같은 개체가 배열되는 것
     - 2원(2 mode)자료 : 행과 열에 다른 개체가 배열되는 것

     

     
     

     

#### 2. 사회연결망 분석에서 네트워크 구조를 파악하기 위한 기법

- 사회연결망 분석을 위한 4가지 기법
  - 중심성 (Centrality)
  - 밀도 (Density)
  - 구조적 틈새 (Structural hole)
  - 집중도 (Centralization)

##### 가. 중심성(Centrality)

- 연결정도 중심성 (Degree centrality)
  - **한 점에 직접적으로 연결된 점들의 합**
  - 연결된 노드의 수가 많을수록 연결정도 중심성이 높아짐

- 근접 중심성 (Closeness centrality) 
  - **한 노드로부터 다른 노드에 도달하기까지 필요한 최소 단계의 합**
  - 근접 중심성이 높을수록 네트워크의 중앙에 위치함
- 매개 중심성 (Betweenness centrality)
  - 네트워크 내에서 한 점이 담당하는 **매개자 혹은 중재자 역할의 정도**
  - 한 노드가 연결망 내의 다른 노드들 사이의 최다 연결 경로 위에 위치하면 할수록 그 노드의 매개중심성이 높음
- 위세 중심성 (Eigenvector centrality)
  - **보나시치(Bonacich) 권련지수** : 위에 중심성의 일반적인 형태로, 연결된 노드의 중요성에 가중치를 둬 노드의 중심성을 측정하는 방법
  - 자신의 연결정도 중심성으로부터 발생하는 영향력과 자신과 연결된 타인의 영향력을 합하여 결정

#### 3. SNA 적용

- 소셜 네트워크 분석은 통신, 온라인 소셜 미디어, 게임 및 유통업체에서 관심이 높다.
- 분석용 솔루션으로는 KXEN, SAS, XTRACT, Indiro, Onalytica, Unicet, Pajek, Inflow 등이 있다.
- R과 하둡을 연동하는 RHadoop, RHIPE와 같은 기술을 활용해 소셜 네트워크 분석을 수행하는 방법도 있다.

#### 4. SNA 단계 4단계

1. 그래프 생성단계
2. 그래프를 목적에 따라 가공하여 분석하는 단계
3. 커뮤니티를 탐지하고 각 객체 또는 노드의 역할(롤)을 정의해 어떠한 롤도 다른 객체들에게 영향력을 더 효율적으로 줄 수 있는지를 정의하는 단계
4. 위 결과를 데이터화하여 다른 데이터마이닝 기법과 연계하는 단계



#### 5. R에서의 SNA

##### 가. 네트워크 레벨 통계량

degree, shortest paths, reachability, density, reciprocity, transitivity, triad census 등

##### 나. 커뮤니티의 수를 측정하는 방법 (community detection)

1. WALKRAP 알고리즘

   - 일련의 random walk 과정을 통해 커뮤니티를 발견한다.

   - 각 버텍스(vertex, 그래프의 꼭지점)를 하나의 커뮤니티로 취급해 점차 더 큰 그룹을 병합하면서 클러스터링 한다.

     ![sna-walkrap](https://user-images.githubusercontent.com/291782/182102867-971745fe-7b29-402f-8547-e5aab06be5c2.png)

     

2. Edge Betweenness method

   - 그래프에 존재하는 최단거리 (shortest path) 중 몇 개가 그 edge (연결, link)를 거쳐가는 지를 이용해 edge-betweenness 점수를 측정한다.

   - 높은 edge-betweenness 점수를 갖는 edge가 클러스터를 분리하는 속성을 가진다고 가정한다.

     ![sna-edge-betweenness](https://user-images.githubusercontent.com/291782/182103059-3c8db81e-7372-413a-af1a-e3ae0ac02fdb.png)

     

#### 6. 활용방안

- 소셜 네트워크 분석은 데이터가 **몇개의 집단으로 구성되는지, 집단 간의 특징은 무엇이고, 해당 집단에서 영향력 있는 고객은 누구인지, 시간의 흐름과 고객 상태의 변화에 따라 다음에 누가 영향을 받을지**를 기반으로 churn/acquisition prediction, fraud, product recommendation 등에 활용한다.




## ADP 데이터 분석 END



## ADsP 데이터 분석 START
### 1장 데이터 분석 개요

### 1절 데이터 분석 기법의 이해

#### 1. 데이터 처리

##### 나. 활용

-   대기업은 DW와 DM을 통해 분석 데이터를 가져와서 사용한다.

-   신규 시스템이나 DW에 포함되지 못한 자료의 경우, 기**존 운영시스템(Legacy)이나 스테이징 영역(staging area)과 ODS (Operational Data Store)**에서 데이터를 가져와서 DW에서 가져온 내용과 결합하여 활용할 수 있다.

-   운영 시스템에 직접 접근해 데이터를 활용하는 것은 매우 위험하므로 거의 이루어 지지 않고, 스테이징 영역의 데이터는 운영시스템에서 임시로 저장된 데이터이기 때문에 **가급적이면 클린징 영역인 ODS에서 데이터의 전처리를 위해 DW나 DM과 결합하여 활용**하는 것이 가장 이상적이다.

    ![data-process](https://user-images.githubusercontent.com/291782/182161470-0747569d-f279-4738-ac36-afe98ed5f0df.png)

    

    ##### 다. 최종 데이터 구조로 가공

    1)   데이터 마이닝 분류

         -   분류값과 입력변수들을 연관시켜 인구 통계, 요약변수, 파생변수 등을 산출한다.

    2)   정형화된 패턴 처리

         -   비정형 데이터나 소셜 데이터는 정형화된 패턴으로 처리해야 한다.

             1.   비정형 데이터 : DBMS에 저장됐다가 텍스트 마이닝을 거쳐 데이터 마트와 통합한다.
             2.   관계형 데이터 : DBMS에 저장되어 사회 신경망분석을 거쳐 분석결과 통계값이 마트와 통합되어 활용된다.

             

             

#### 2. 시각화(시각화 그래프)

-   **시각화는 가장 낮은 분석 수준의 분석**이지만 잘 사용하면 **복잡한 분석보다도 더 효율적**이다.
-   대용량 데이터를 다루는 **빅데이터분석에서 시각화는 필수**이다.
-   **탐색적 분석(EDA)를 할 때 시각화는 필수**이다.
-   SNA분석(사회연결망분석)을 할 때 자주 활용된다.



#### 3. 공간분석 (GIS, Geographic Information System)

-   공간분석은 **공간적 차원과 관련된 속성들을 시각화하는 분석**이다.
-   지도 위에 관련 속성들을 생성하고 **크기, 모양, 선 굵기** 등으로 구분하여 인사이트를 얻는다.





#### 4. 탐색적 자료 분석 (EDA)

##### 가. 개요

-   탐색적 분석은 다양한 차원과 값을 조합해가며 특이한 점이나 의미 있는 사실을 도출하고 분석의 최종 목적을 달성해가는 과정으로 데이터의 특징과 내재하는 구조적 관계를 알아내기 위한 기법들의 통칭이다.
-   프린스턴 대학의 튜키교수가 1977년 저서를 발표함으로 EDA가 등장한다.

##### 나. EDA의 4가지 주제

-   저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성

##### 다. 탐색적 분석의 효율 예

-   데이터 이해 단계 : 변수의 분포와 특성 파악
-   변수 생성 단계 : 분석 목적에 맞는 주요한 요약 및 파생변수 생성
-   변수선택 단계 : 목적변수에 의미있는 후보 변수 선택
-   위 3 단계에서 활용되고 있다.



#### 5. 통계분석

-   통계 : 어떤 현상을 종합적으로 한눈에 알아보기 쉽게 일정한 체계에 따라 숫자와 표, 그림의 형태로 나타내는 것
-   기술통계 (descriptive statistics) : 모집단에서 표본을 추출하고 **표본이 가지고 있는 정보를 쉽게 파악**할 수 있도록 데이터를 정리하거나 요약하기 위해 하나의 숫자 또는 그래프의 형태로 표현하는 절차
-   추측(추론)통계 (inferential statistics) : **표본의 통계량으로 부터 모집단의 특성인 모수에 관해 통계적으로 추론**하는 절차
-   활용분야
    -   정부의 경제정책 수립과 평가의 근거자료로 활용 (통계청의 실업률, 고용률, 물가지수)
    -   농업 (가뭄, 수해 또는 병충해 등에 강한 품종의 개발 및 개량)
    -   의학 (의학적 치료 방법의 효과나 신약 개발을 위한 임상실험의 결과 분석)
    -   스포츠 (선수들의 체질향상 및 개선, 경기 분석과 전략분석, 선수평가와 기용 등)



#### 6. 데이터마이닝

##### 가. 개요

-   대표적인 고급 데이터 분석법으로 **대용량의 자료**로 부터 정보를 요약하고 미래에 대한 예측을 목표로 자료에 존재하는 **관계, 패턴, 규칙 등을 탐색**하고 이를 모형화함으로써 이전에 알려지지 않은 **유용한 지식을 추출**하는 분석 방법이다.

##### 나. 방법론

-   데이터베이스에서의 지식탐색 : DW에서 DM을 생성하면서 각 데이터들의 속성을 사전분석을 통해 지식을 얻는 방법이다.
-   기계학습(machine learning) : 인공지능의 한 분야로, **인공신경망, 의사결정나무, 클러스터링, 베이지안 분류, SVM** 등이 있다.
-   패턴인식 (pattern recognition) : 사전지식과 패턴에서 추출된 통계정보를 기반으로 자료 또는 패턴을 분류하는 방법으로 **장바구니분석, 연관규칙** 등이 있다.
-   판단 기준 : 정확도, 정밀도, 디텍트 레이트 (detect rate), 리프트 (lift) 등의 값으로 판단
-   시뮬레이션에서는 Throughput, Average Waiting Time, Average Queue Length, Time in System 등의 지표가 활용된다.



##### 다. 활용분야

-   데이터베이스 마케팅 : 방대한 고객의 행동정보를 활용해 목표 마케팅, 고객세분화, 장바구니 분석, 추천시스템 등
-   신용평가 및 조기경보시스템 : 금융기관에서 신용카드 발급, 보험, 대출 발생 시 업무에 적용
-   생물정보학 : 세포의 유전자를 분석하여 질병의 진단과 치료법 또는 신약 개발
-   텍스트마이닝 : e-mail, SNS 등 디지털 텍스트 정보를 통해 고객성향분석, 감성분석, 사회관계망분석 등


### 2장 R프로그래밍 기초

### 1절 R 소개

#### 1. 데이터 분석 도구의 현황

##### 가. R의 탄생

-   R은 오픈소스 프로그램으로 통계. 테이터마이닝과 그래프를 위한 언어
-   최신 통계분석과 마이닝 기능을 제공
-   세계적으로 많은 사용자들이 다양한 예제를 공유
-   많은 패키지가 수시로 업데이트 됨



##### 나. 분석도구의 비교

|                           |      SAS       |      SPSS      |       오프소스 R       |
| ------------------------- | :------------: | :------------: | :--------------------: |
| 프로그램 비용             |   유료, 고가   |   유료, 고가   |        오픈소스        |
| 설치용량                  |     대용량     |     대용량     |     모듈화로 간단      |
| 다양한 모듈 지원 및 비용  |    별도구매    |    별도구매    |        오픈소스        |
| 최근 알고리즘 및 기술반영 |      느림      |   다소 느림    |        매우빠름        |
| 학습자료 입수의 편의성    | 유료 도서 위주 | 유로 도서 위주 | 공개 논문 및 자료 많음 |
| 질의를 위한 공개 커뮤니티 |       NA       |       NA       |       매우 활발        |



##### 다. R의 특징 6가지

1)   오픈소스 프로그램
     -   사용자 커뮤니티에 도움 요청이 쉽고, 많은 패키지가 수시로 업데이트 됨
2)   그래픽 및 성능
     -   그래픽 및 성능이 상용 프로그램과 대등하거나 월등함
3)   시스템 데이터 저장 방식
     -   각 세션 사이마다 시스템에 데이터셋을 저장하므로 매번 데이터를 로딩할 필요가 없고 명령어 스토리도 저장 가능
4)   모든 운영체제
     -   윈도우, 맥, 리눅스 운영체제에서 사용 가능
5)   표준 플랫폼
     -   S통계 언어를 기반으로 구현된다. R/S 플랫폼은 통계전문가들의 사실상의 표준 플랫폼이다.
6)   객체지향언어이며 함수형 언어
     -   통계 기능뿐만 아니라 일반 프로그래밍 언어처럼 자동화하거나 새로운 함수를 생성하여 사용 가능



##### 마. R 기반의 작업 환경

-   R의 메모리
-   64비트 유닉스 환경 : 메모리 무제한
-   x86 64비트 환경 : 128TB 까지 지원
-   64비트 윈도우 환경: 8TB까지 지원

#### 

### 2절 R기초-1

#### 1. 통계 패키지 R

### 2절 R기초-2

### 3절 입력과 출력
#### 1. 데이터 분석 과정

### 4절 데이터 구조와 데이터 프레임 - 1

#### 1. 벡터(Vector)

##### 가. 벡터들은 동질적이다.

-   한 벡터의 모든 원소는 같은 자료형 또는 같은 모드(mode)를 가진다.

##### 나. 벡터는 위치로 인덱스 된다.

##### 다. 벡터는 인덱스를 통해 여러 개의 원소로 구성된 하위 벡터를 반환할 수 있다.

-   V[c(2,3)]은 v벡터의 2, 3번째 원소로 구성된 하위벡터이다.

-   ```R
    > vec
     [1]  1  2  3  4  5  6  7  8  9 10
    > vec[c(2,3)]
    [1] 2 3
    ```

##### 라. 벡터 원소들은 이름을 가질 수 있다.



#### 2. 리스트 (List)

##### 가. 리스트는 이질적이다.

-   여러 자료형의 원소들이 포함될 수 있다.

##### 나. 리스트는 위치로 인덱스 된다.

-   ```R
    > list_data <- list("red", "green", c(10, 20, 30), TRUE, 21.25, 10.07)
    > print(list_data)
    [[1]]
    [1] "red"
    
    [[2]]
    [1] "green"
    
    [[3]]
    [1] 10 20 30
    
    [[4]]
    [1] TRUE
    
    [[5]]
    [1] 21.25
    
    [[6]]
    [1] 10.07
    
    > list_data[2]
    [[1]]
    [1] "green"
    > list_data[[2]]
    [1] "green"
    ```

##### 다. 리스트에서 하위 리스트를 추출할 수 있다.

-   L[c(2, 3)]은 L리스트의 2, 3번째 원소로 이루어진 하위 리스트이다.

##### 라. 리스트의 원소들은 이름을 가질 수 있다.



#### 3. R에서의 자료형태(mode)

|     객체      |                    예시                     |    모드(mode)     |
| :-----------: | :-----------------------------------------: | :---------------: |
|     숫자      |                    3.136                    |  수치형(numeric)  |
|   숫자 벡터   |                c(2, 3, 4, 6)                |  수치형(numeric)  |
|    문자열     |                    "Tom"                    | 문자형(character) |
|  문자열 벡터  |           c("Tom", "yoon", "kim")           | 문자형(character) |
|     요인      |          factor(c("A", "B", "C"))           |  수치형(numeric)  |
|    리스트     |         list("Tom", "yoon", "Kim")          |   리스트(list)    |
| 데이터 프레임 | data.frame(x=1:3, y=c("Tom", "yoon","kim")) |   리스트(list)    |
|     함수      |                    print                    |  함수(function)   |

```R
> mode(factor(c("A","B", "C")))
[1] "numeric"
```



#### 4. 데이터 프레임(data frame)

##### 가. 특징

-   강력하고 융연한 구조. SAS의 데이터셋을 모방해서 만들어 짐
-   데이터 프레임의 리스트의 원소는 벡터 또는 요인이다.
-   그 벡터와 요인은 데이터 프레임의 **열**이다.
-   벡터와 요인들은 동일한 길이이다.
-   데이터 프레임은 표 형태의 데이터 구조이며, 각 열은 서로 다른 데이터 형식을 가질 수 있다.
-   **열에는 이름이 있어**야 한다.



#### 5. 그 밖의 데이터 구조들

##### 가. 단일값 (scalar)

##### 나. 행렬(matrix)

##### 다. 배열(arrays)

##### 라. 요인 (factor)

-   벡터처럼 생겼지만, R에서는 벡터에 있는 고유값(unique value)의 정보를 얻어내는데, 이 고유 값들을 요인의 수준(level)이라고 한다.
-   요인의 두가지 주된 사용처로 범주형 변수, 집단분류가 있다.

#### 6. 벡터, 리스트, 행렬 다루기



### 4절 데이터 구조와 데이터 프레임 - 2

#### 1. 데이터 프레임

#### 2. 자료형 데이터 구조 변환

#### 3. 데이터 구조 변경

#### 4. 벡터의 기본 연산

#### 5. 그 외에 간단한 함수



### 5절 데이터 변형

#### 1. 주요 코드

-   데이터 프레임에 함수 적용 (apply 계열 함수)
-   input 과 output의 형태에 따른 함수
    -   apply (input: array, output: array).  e.g `apply(df, func)`
    -   lapply (input: list or vector, output: list).  e.g `lapply(df, func)`
    -   sapply (input: list or vector, output: vector or array).  e.g `sapply(df, func)`
    -   vapply (input: list or vector, output: vector or array).  e.g `vapply(df, func)`
    -   tapply (input: list or vector and factor, output: vector or array).  e.g `tapply(df, func)`
    -   mapply (input: list or vector, output: vector or array).  e.g `mapply(df, func)`



#### 2. 문자열 날짜 다루기

**기출문제**

### 3장 데이터 마트

### 1절 데이터 변경 및 요약








### 4장 통계 분석

### 1절 통계분석의 이해

#### 2. 통계자료의 획득 방법 (p.282)

##### 가. 총 조사 / 전수 조사 (census)

-   많은 비용과 시간이 소요되므로 특별한 경우를 제외하고는 사용되지 않음. (eg. 인구주택 총 조사)

##### 나. 표본조사 

-   모집단에서 샘플을 추출하여 진행하는 조사
-   모집단 (population) : 대상 집단 전체
-   원소 (element) : 모집단을 구성하는 개체
-   표본 (sample) : 조사하기 위해 추출한 모집단의 일부 원소
-   모수 (parameter) : 표본 관측에 의해 구하고자 하는 모집단에 대한 정보
-   모집단의 정의, 표본의 크기, 조사 방법, 조사기간, 표본추출방법을 정확히 명시해야 함



##### 다. 표본 추출 방법 4가지 (중요)

1. 단순랜덤 추출법 (simple random sampling)

   -   각 샘플에 번호를 부여하여 n개를 추출하는 방법으로 각 샘플은 선택될 확률이 동일하다. (복원, 비복원 추출)

2. 계통추출법 (systematic sampling)

   -   단순랜덤 추출법의 변형된 방식으로 샘플을 나열하여 K개씩 n개의 구간으로 나누고 첫 구간에서 하나를 임의로 선택한 후에 K개식 띄어서 n 개의 표본을 선택
   -   ![systematic-sampling](https://user-images.githubusercontent.com/291782/161560760-0d60d365-a300-4262-8b09-5e9d64125e21.png)

3. 집락추출법 (cluster random sampling)

   -   군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링하는 방법
   -   ![cluster-random-sampling](https://user-images.githubusercontent.com/291782/161560900-294b7296-b204-41f2-9170-0f60ae4d9fc4.png)

4. 층화추출법 (stratified random sampling)

   -   이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출하는 방법으로, 유사한 원소끼리 몇 개의 층(stratum)으로 나누어서 각 층에서 랜덤 추출하는 방법
   -   ![stratified-random-sampling](https://user-images.githubusercontent.com/291782/161561325-a774c94f-ce60-4430-a2fb-f91dcdf969c2.png)

   

   

   ##### 라. 측정 (measurement)

   ==측정방법 (아주 중요)==

   -   질적처도 : 범주형 자료, 숫자들의 크기 차이가 계산되지 않는 척도
       -   명목척도 : 측정 대상이 어느 **집단**에 속하는지 분류할 때 사용 (성별, 출생지 구분)
       -   순서척도 : 측정 대상의 **서열관계**를 관측하는 척도 (만족도, 선호도, 학년, 신용등급)
   -   양적척도 : 수치형자료, 숫자들의 크기 차이를 계산할 수 있는 척도
       -   구간척도(등간척도) : 측정 대상이 갖고 있는 **속성의 양**을 측정하는 것으로 구간이나 구간 사이의 **간격이 의미가 있는** 자료 (온도, 지수)
       -   비율척도 : 간격(차이)에 대한 비율이 의미를 가지는 자료, **절대적인 기준인 0이 존재**하고 **사칙연산이 가능**하며 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리)

   순서척도는 명목척도와 달리 매겨진 숫자의 크기를 의미있게 활용 가능 (예: 1등이 2등보다 성적이 높다)

   구간척도는 절대적 크기는 측정할 수 없기 때문에 사칙연산 중 더하기와 빼기는 가능. 곱하기나 나눗셈은 불가능

   

   

   #### 3. 통계분석 (p.285)

   

   #### 4. 확률 및 확률분포 (p.285)

   ##### 나. 확률분포

   1. 이산형 확률변수

      -   베르누이 확률분포 (Bernoulli distribution)

          -   결과가 2개만 나오는 경우 **성공 또는 실패** (예. 동전 던기지, 시험의 합격/불합격 등)
          -   $P(X = x) = P^x . (1-p)$<sup>1-x</sup> 
          -   (x= 1 or 0), 기댓값: $E(x) = p$, 분산 :$var(x) = p(1-p)$
          -   예) 추신수가 안타를 칠 확률은 베르누이 분포를 따른다.
      -   이항분포 (Binomial distribution)

          -   베르누이 시행을 n번 반복했을 때 k번 성공할 확률
          -   n번 시행 중에 각 시행의 확률이 p일 때, k번 성공할 확률분포
          -   $P(X = k) = _nC_kP^k(1-p)$<sup>n-k</sup> , $_nC_k = \dfrac {n!}{k!(n-k)!}$
          -   기댓값 : $E(X) = np$, 분산 : $V(X) = np(1-p) $  (단, n과 k가 1이면 베르누이 시행)
          -   추신수가 오늘 경기에서 5번 타석에 들어와서 3번 안타를 칠 확률은 이항분포를 따른다. (n=5, k=3, 안타를 칠 확률 P(x) = 타율로 적용 가능)
          -   성공할 확률 p가 0이나 1에 가깝지 않고 n이 충분히 크면 이항분포는 정규분포에 가까워 진다. 성공할 확률 p가 1/2에 가까우면 종모양이 된다.
      -   기하분포 (Geometric distribution)
          -   성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 X번 실패할 확률
          -   예) 추신수가 오늘 경기에서 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률은 기하분포를 따른다.
      -   다항분포 (Multinomial distribution)
          -   이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포
      -   포아송분포 (Poisson distiribution)
          -   시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
          -   예) 책에 오타가 5page 당 10개씩 나온다고 할 떄, 한 페이지에 오타가 3개 나올 확률, 추신수가 최근 5경기에서 홈런을 쳤을 경우, 오늘 경기에서 홈런을 못 칠 확률은 포아송 분포
          -   &lambda; (람다) = 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값, y= 사건이 일어난 수
          -   $P = \dfrac {\lambda^ne^{-\lambda}} {n!}$ (e는 자연상수)
          -   기댓값 : $E(X) = \lambda$, 분산 : $V(X) = \lambda $

   2. 연속형 확률변수

      - 가능한 값이 실수의 어느 특정구간 전체에 해당하는 확률변수 (확률밀도함수)

      - $ f(x)\ge 0 $     $\int_{-\infty}^{\infty}f(x)dx = 1$

      - 균일분포 (일양분포, Uniform distiribution)

        - 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
        - $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
        - ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)

      - 정규분포 (Normal distribution)

        - 평균이 &mu; (뮤) 이고, 표준편차가 &sigma; (시그마) 인 X의 확률밀도 함수
        - 표준편차가 클 경우 퍼져보이는 그래프가 나타남
        - 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
        - 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
        - ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)

      - 지수분포 (Exponential distribution)

        - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포이다.
        - 예) 전자렌지의 수명시간, 콜센터에 전화가 걸려올때 까지의 시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간
        - ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)

      - t분포 (t-distribution)

        - 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
        - 표본이 커져서 (30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
        - 데이터가 연속형일 경우 활용한다.
        - **두 집단의 평균이 동일한지** 알고자 할 때 검정통계량으로 활용된다.
        - ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)

      - X<sup>2</sup>-분포 (chi-square distribution, 카이제곱분포)

        - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포
        - **두 집단 간의 동질성 검정에 활용**된다. (범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용)
        - ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)

      - F-분포 (F-distribution)

        - **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량 분포
        - 확률변수는 항상 양의 값만을 갖고 X<sup>2</sup> 분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
        - ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)

        



#### 5. 추정과 가설검정 (p.293)

##### 가. 추정의 개요

1.   확률표본 (random sample)
     -   확률분포는 분포를 결정하는 평균, 분산 등의 모수(parameter)를 가지고 있다.
     -   특정한 확률분포로부터 독립적으로 반복해 표본을 추출하는 것이다.
     -   각 관찰값들은 서로 독립적이며 동일한 분포를 갖는다.
2.   추정
     -   표본으로부터 미지의 모수를 추측하는 것이다.
     -   추정은 점추정 (point estimation)과 구간추정(interval estimation)으로 구분된다.
     -   점추정 (point estimation)
         -   '**모수가 특정한 값일 것**'이라고 추정하는 것이다.
         -   표본의 평균, 중위수, 최빈값 등을 사용한다.
         -   불편성 (unbiasedness) : 표본에서 얻는 추정량의 **기댓값**은 모집단의 모수와 차이가 없다.
         -   효율성 (efficiency) : 추정량의 분산이 작을수록 좋다.
         -   일치성 (consistency) : 표본의 크기가 아주 커지면, 추정량이 모수와 거의 같아진다.
         -   충족성 (sufficient) : 추정량은 모수에 대하여 모든 정보를 제공한다.
         -   표본평균 (sample mean) : 모집단의 평균(모평균)을 추정하기 위한 추정량. 확률표본의 평균값.
         -   $\overline{X} = \dfrac {1}{n}\displaystyle \sum_{i=1}^{n}X_i$
         -   표본분산 (sample variance) : 모집단의 분산 (모분산)을 추정하기 위한 추정량
         -   $S^2 = \dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n} (X_i - \overline{X})^2$
     -   구간추정 (interval estimation)
         -   **모수가 특정한 구간에 있을 것이라고 선언하는 것**이다.
         -   항상 추정량의 분포에 대한 전제가 주어져야 하고, 구해진 구간 안에 모수가 있을 가능성의 크기 (신뢰수준(confidence interval))가 주어져야 한다.
         -   95% 신뢰수준 하에서 모평균의 신뢰구간 (모분산을 알때는 분자에 &sigma;(시그마)를 넣고, 모분산을 모를땐 분자에 s를 넣는다.)
         -   모분산 &sigma;<sup>2</sup> 이 알려져 있는 경우
         -   $(\overline{X} - 1.96 \dfrac {\sigma}{\sqrt{n}}, \overline{X} + 1.96 \dfrac {\sigma}{\sqrt{n}})$ , 표준정규분포 N(0, 1)를 따르는 $Z = \dfrac {\overline{X} - \mu} {\dfrac {\sigma} {\sqrt{n}}}$
         -   모분산 &sigma;<sup>2</sup> 이 알려져 있지 않은 경우에는 모분산 대신 표본분산을 사용
         -   $(\overline{X} - 2.26 \dfrac {S}{\sqrt{n}}, \overline{X} + 2.26 \dfrac {S}{\sqrt{n}})$ , 자유도가 n-1인 t-분포를 따르는 $T = \dfrac {\overline{X} - \mu} {\dfrac {S} {\sqrt{n}}}$



##### 나. 가설검정

-   귀무가설 (null hypothesis, H<sub>0</sub>) : **비교하는 값과 차이가 없다, 동일하다**를 기본개념으로 하는 가설
-   대립가설 (alternative hypothesis, H<sub>1</sub>) : **뚜렷한 증거가 있을 때 주장하는 가설**
-   검정통계량 (test statistic) : 관찰된 표본으로부터 구하는 통계량, 검정 시 가설의 진위를 판단하는 기준
-   **유의수준** (significance level, &alpha;) : 귀무가설을 기각하게 되는 확률의 크기로 '귀무가설이 옳은데도 이를 기각하는 확률의 크기'
-   기각역 (critical region, C) : 귀무가설이 옳다는 전제하에서 구한 검정통계량의 분포에서 확률이 유의수준 &alpha;인 부분 (반대는 채택역 (acceptance region))
-   ![hypothesis](https://user-images.githubusercontent.com/291782/161790640-44316c2d-58db-4eb9-94e8-182c00887aa5.png)
-   ![alpha-beta](https://user-images.githubusercontent.com/291782/161790831-f7bb96a8-8f59-4a83-b303-0b85d53a9c76.png)





#### 6. 비모수 검정 (p.296)

모집단의 모수에 대한 검정은 **모수적 검정**과 **비모수적 검정**으로 구분한다.

##### 가. 모수적 방법

-   모집단의 분포에 대한 가정을 하고, 그 가정하에서 검정통계량과 검정통계량의 분포를 유도해 검정을 실시하는 방법

##### 나. 비모수적 방법

-   자료가 추출된 **모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시**하는 방법
-   관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우에 이용
-   관측된 **자료의 수가 많지 않거나** (30개 미만), 자료가 개체간의 **서열관계를 나타내는 경우**에 이용

##### 다. 모수적 검정과 비모수적검정의 차이점

1.   가설의 설정
     -   모수적 검정 : 가정된 모수의 분포에 대해 가설을 설정
     -   비모수적 검정 : 가정된 분포가 없으므로 가설은 단지 '분포의 형태가 동일하다' 또는 '분포의 형태가 동일하지 않다'와 같이 **분포의 형태에 대해 설정**한다.
2.   검정 방법
     -   모수적 검정 : 관측된 자료를 이용해 구한 **표본평균, 표본분산** 등을 이용해 검정을 실시
     -   비모수적 검정 : 절대적인 크기에 의존하지 않고 **관측값들의 순위**(rank)나 **두 관측값 차이의 부호** 등을 이용해 검정

##### 라. 비모수 검정의 예

-   부호검정 (sign test), 윌콕슨의 순위합검정 (rank sum test), 윌콕슨의 부호순위합검정 (Wilcoxon signed rank test), 만-위트니의 U 검정, 런검정 (run test), 스피어만의 순위상관계수





### 2절 기초 통계분석

#### 1. 기술통계 (Descriptive Statistics) (p.298)

##### 가. 기술통계의 정의

- 자료의 특성을 표, 그림, 통계량 등을 사용하여 쉽게 파악할 수 있도록 정리/요약하는 것
- 대략저인 통계적 수치를 계산해봄으로써 데이터에 대한 대략적인 이해와 앞으로 분석에 대한 통찰력을 얻기 유리



##### 나. 통계량에 의한 자료 정리

1. 중심위치의 측도
   - 표본평균 (sample mean) : $\overline{X} = \dfrac {1}{n}(X_1 + X_2 + ... X_n) = \displaystyle \sum_{i=1}^{n} \dfrac{X_i}{n}$
   - 중앙값 (median) : 크기순으로 나열 시 중앙에 위치하는 값
     - n이 홀수인 경우 : $\dfrac {(n+1)}{2}$
     - n이 짝수인 경우 : $\dfrac {n}{2}$ 번째 값과 $\dfrac {(n+1)}{2} + 1$번째 값의 평균
2. 산포의 측도
   - 대표적인 산포도 (dispersion)는 분산, 표준편차, 범위 및 사분위수범위
   - 분산 : $S^2 = \dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n}(X_i - \overline{X})^2 = \dfrac {1}{n-1}(\displaystyle \sum_{i=1}^{n}X_i^2 - n\overline{X}^2) $
   - 표준편차 : $S = \sqrt{S^2} = \sqrt{\dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n} (X_i - \overline{X})^2}$
   - 사분위수범위 (interquartile range) : IQR = Q3 - Q1
   - 사분위수 : Q1 (25백분위수), Q2 (50백분위수), Q3 (75백분위수)
   - 백분위수 (percentile) : $\dfrac {(n-1)p} {100 + 1}$번째 값
   - 변동계수 (coefficient of variation) : $V = \dfrac {S} {\overline{X}}$
   - 평균의 표준오차 : $SE(X) = \dfrac {S} {\sqrt{n}}$
   - ![percentile-iqr](https://user-images.githubusercontent.com/291782/162159496-84d2c7a6-f413-4748-ba66-ea6d5a62c71b.png)
3. 분포의 형태에 관한 측도
   - 왜도 (skewness) :분포의 비대칭정도를 나타내는 측도. 왜도가 양수인 경우 왼쪽에 밀집되어 있고, 오른쪽으로 긴 꼬리를 갖는 분포, 음수인 경우는 반대. 왜도가 0 일 경우는 좌우 대칭인 분포
     - ![skewness](https://user-images.githubusercontent.com/291782/162161124-9884c740-21f4-42f1-898f-997cfa30063e.png)
   - 첨도 (kurtosis) : 분포의 중심에서 뾰족한 정도를 나타내는 측도
     - ![kurtosis](https://user-images.githubusercontent.com/291782/162162023-a35083f7-7fbf-4389-9e61-7a552528df08.png)



##### 다. 그래프를 이용한 자료 정리

1. 히스토그램 : 도수분포표를 그래프로 나타낸 것
2. 막대그래프와 히스토그램의 비교
   - 막대그래프 : **범주형 (category)**으로 구분된 데이터 (예. 직업, 종교, 음식)를 표시하며 범주의 순서를 의도에 따라 바꿀수 있다.
   - 히스토그램 : **연속형(continuous)**으로 표시된 데이터 (키, 몸무게, 성적, 연봉)을 표현하며 임의의 순서를 바꿀수 없고 막대의 간격이 없다.



#### 2. 인과관계의 이해 (p.303)

##### 가. 용어

-   종속변수 (반응변수, y) : 다른 변수의 영향을 받는 변수
-   독립변수 (설명변수, x) : 영향을 주는 변수
-   산점도에서 확인할 사항
    -   두 변수 사이의 선형관계(직선관계)가 성립하는가?
    -   두 변수 사이의 함수관계(직선관계 또는 곡선관계)가 성립하는가?
    -   이상값이 존재하는가?
    -   몇 개의 집단으로 구분(층별)되는 가?



##### 나. 공분산 (covariance)

-   두 확률변수 X, Y의 방향성의 조합(선형성)이다.
-   공분산의 부호가 + 이면 양의 방향성, - 이면 음의 방향성을 가짐
-   X, Y가 서로 독립이면 $Cov (X, Y) = 0$ 이다.
-   $Cov(X, Y) = E[(X-\mu_X)(Y - \mu_Y)]$



#### 3. 상관분석 (Correlation Analysis)

##### 가. 상관분석 정의

-   두 변수의 상관관계를 알아보기 위해 상관계수 (correlation coefficient)를 이용하며, 그 공식은 아래와 같다.
-   $r = \dfrac {cov(x, y)} {S_x ⅹ S_y} = \dfrac {\displaystyle \sum_{i=1}^n [(x - \overline{x})(y - \overline{y})]} {n(S_x ⅹ S_y)}$



##### 나. 상관관계의 특성

-   $0.7 \lt  r \le 1$ : 강한 양(+)의 상관관계, $0 \lt  r \le 0.3$ : 거의 상관 음다. ,r = 0 : 상관관계(선형, 직선)가 없다.
-   $-1 \le  r \lt 0.7$ : 강한 음(-)의 상관관계, $-0.7 \le  r \lt 0.3$ : 약한 음(-)의 상관관계



##### 다. 상관분석의 유형

|   구분   | 피어슨                                                  | 스피어만                                                  |
| :------: | ------------------------------------------------------- | --------------------------------------------------------- |
|   개념   | 등간척도 이상으로 측정된 두 변수들의 상관관계 측정 방식 | 서열척도인 두 변수들의 상관관계 측정 방식                 |
|   특징   | 연속형 변수, 정규성 가정, 대부분 많이 사용              | 순서형 변수, 비모수적 방법, 순위를 기준으로 상관관계 측정 |
| 상관계수 | 피어슨 $r$ (적률상관계수)                               | 순위상관계수 $(p, 로우)$                                  |

>   피어슨 스피어만 구분 Tip : 스피어만, 서열척도, 순서, 순위상관계수 등의 단어는 모두 "ㅅ" 으로 시작



##### 라. 상관분석을 위한 R코드

```R
# 분산
var(x,y = NULL, na.rm = FALSE)

# 공분산
cov(x,y = NULL, use="everything",
   method = c("pearson", "kendall", "spearman"))

# 상관관계
cor(x,y = NULL, use = "everything",
   method = c("pearsono", "kendall", "spearman"))
# 상관관계 Hmisc 패키지의 rcorr 사용
rcorr(matrix(data명), type=c("pearson", "kendall", "spearman"))
```



##### 마. 상관분석의 가설 검정

-   상관계수 $r$이 0이면 입력변수 x와 출력변수 y 사이에는 아무런 관계가 없다. (귀무가설: $r = 0$, 대립가설 $r\neq 0$)
-   t 검정통계량을 통해 얻은 p-value 값이 0.05이하인 경우, 대립가설을 채택하게 되어 우리가 구한 상관계수를 활용할 수 있게 됨



##### 바. 상관분석 예제

mtcars 데이터셋의 마일(mpg), 총마력(hp)의 상관관계 분석

```R
> data("mtcars")
> a <- mtcars$mpg
> b <- mtcars$hp
> cov(a, b)
[1] -320.7321
> cor(a, b)
[1] -0.7761684
> cor.test(a, b, method="pearson")

	Pearsons product-moment correlation

data:  a and b
t = -6.7424, df = 30, p-value = 1.788e-07
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.8852686 -0.5860994
sample estimates:
       cor 
-0.7761684 
```

-   분석결과 : 공분산 (covariance)은 -320.73, 상관계수 (correlation coefficient)는 -0.776
-   따라서 mpg와 hp는 음의 방향성을 가지며, 상관계수로 강한 음의 상관관계가 있음을 알 수 있음
-   cor.test를 이용해 나온 p-value 가 1.788e-07로 유의수준 0.05보다 작게 나타나므로 mpg와 hp가 상관관계가 있다고 할 수 있다.



### 3절 회귀분석 (중요. 3 ~ 5문제)

#### 1. 회귀분석의 개요 (p.309)

##### 가. 회귀분석 정의

- 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법
- 변수들 사이의 인과관계를 밝히고 모형을 적합하여 관심있는 변수를 예측하거나 추론하기 위한 분석방법
- 독립변수 개수가 하나이면 단순선형회귀분석, 독립변수가 두 개 이상이면 다중선형회귀분석으로 분석 할 수 있다.



##### 나. 회귀분석의 변수

- 영향을 받는 변수 (y) : 반응변수 (response variable), 종속변수 (dependent variable), 결과변수 (outcome variable)
- 영향을 주는 변수 (x) : 설명변수 (explanatory variable), 독립변수 (independent variable), 예측변수 (predicator variable)



##### 다. 선형회귀분석의 가정

- **선형성** : 입력변수와 출력변수의 관계가 선형이다
- 등분산성 : 오차의 분산이 입력변수와 무관하게 일정하다.
- 독립성 : 입력변수와 오차는 관련이 없다. 독립성을 알아보기 위해 Dubrin-Watson 통계량 사용. 주로 시계열 데이터에서 많이 활용
- 비상관성 : 오차들끼리 상관이 없다.
- 정상성 (정규성) : 오차의 분포가 정규분포를 따른다. Q-Q plot, Kolmogolov-Smirnov 검정, Shaprio-Wilk 검정 등을 활용하여 정규성을 확인



##### 라. 그래프를 활용한 선형회귀분석의 가정 검토

| 선형성                                                       | 등분산성                                                     | 정규성                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![linear](https://user-images.githubusercontent.com/291782/162400779-89b3cc2b-e948-4f2e-a217-8251396f5225.png) | ![residuals](https://user-images.githubusercontent.com/291782/162400366-c11a9068-d55f-4f77-aebb-3432abe8678f.png) | ![normal-qqplot](https://user-images.githubusercontent.com/291782/162400224-35785847-68a1-4025-8ac9-8b9dca8fc9f0.png) |

- 선형성 : 설명변수(x)와 종속변수(y)가 선형적 관계가 있음이 전제되어야 함
- 등분산성 : 설명변수(x) 값에 관계없이 잔차들의 변동성(분산)이 일정한 형태를 보여야 한다. (마름모, 부채꼴, 대각선 모양 모두 안됨)
- 정규성 : Q-Qplot을 출력 시 위 그림과 같이 잔차가 대각 방향의 직선 형태를 지니고 있으면 잔차는 정규분포를 따른다고 할 수 있다.



##### 마. 가정에 대한 검증

- 단순선형회귀분석 : 입력변수와 출력변수간 선형성을 점검하기 위해 산점도를 확인
- 다중선형회귀분석 : 선형회귀분석의 가정인 선형성, 등분산성, 독립성, 정상성이 모두 만족하는지 확인



#### 2. 단순선형회귀분석 (p.311)

- 독립변수가 종속변수에 미치는 영향을 추정하는 통계 기법
- ![linear-regression](https://user-images.githubusercontent.com/291782/162449647-a02d3869-0d0b-4c51-a083-90a63717c624.png)



##### 가. 회귀분석에서의 검토사항

-   회귀계수들이 유의미한가? 
    -   해당 계수의 **t 통계량의 p-value 가 0.05보다 작으면** 해당 회귀계수가 통계적으로 유의미하다고 볼 수 있다.
-   모형이 얼마나 설명력을 갖는가?
    -   결정계수(R<sup>2</sup>)를 확인한다. **결정계수는 0 ~ 1 값을 가지며**, 높은 값을 가질수록 추정된 회귀식의 설명력이 높다.
-   모형이 데이터를 잘 적합하고 있는가?
    -   잔차를 그래프로 그리고 회귀진단을 한다.



##### 나. 회귀계수의 추정 (최소제곱법, 최소자승법)



##### 다. 회귀분석의 검정

1.   회귀계수의 검정

-   10년간의 에어컨 예약대수와 판매대수 (단위: 1,000대)

| 예약대수(X) | 19   | 23   | 26   | 29   | 30   | 38   | 39   | 46   | 49   |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 판매대수(Y) | 33   | 51   | 40   | 49   | 50   | 69   | 70   | 64   | 89   |

위 데이터에 대한 단순회귀분석을 실시

```R
> x <- c(19, 23, 26, 29, 30, 38, 39, 46, 49)
> y <- c(33, 51, 40, 49, 50, 69, 70, 64, 89)
> lm(y~x) # linear regression

Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
      6.409        1.529  

> summary (lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-12.766  -2.470  -1.764   4.470   9.412 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.4095     8.9272   0.718 0.496033    
x             1.5295     0.2578   5.932 0.000581 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.542 on 7 degrees of freedom
Multiple R-squared:  0.8341,	Adjusted R-squared:  0.8104 
F-statistic: 35.19 on 1 and 7 DF,  p-value: 0.0005805
```

-   X의 회귀계수인 t 통계량에 대한 p-value 가 0.0005805로 나타나, 유의수준 0.05보다 작으므로 회귀계수의 추정치들이 통계적으로 유의함
-   결정계수 (R<sup>2</sup>) 0.8341로 높게 나타나 회귀식이 데이터를 적절하게 설명하고 있다 할 수 있음
-   회귀분석 결과 "판매대수 = 6.409 + 1.529 * 예약대수"의 회귀식을 구할 수 있다.



2.   결정계수

![r2](https://user-images.githubusercontent.com/291782/162452511-f3bca308-98f2-4dd9-a7d4-fffad044fe47.png)

-   전체제곱합 (total sum of squares, SST) : $\displaystyle \sum_{i=1}^n(y_i - \overline{y})^2$
-   회귀제곱합 (regression sum of squares, SSR) : $\displaystyle \sum_{i=1}^n(\hat{y}_i - \overline{y})^2$
-   오차제곱합 (error sum of squares, SSE) : $\displaystyle \sum_{i=1}^n(y_i - \hat{y})^2$
-   결정계수 (R<sup>2</sup>)는 전제제곱합 (SST)에서 회귀제곱합 (SSR)의 비율 (SSR / SST), 0 &le; R<sup>2</sup> &le; 1 (SST = SSR + SSE)



3. 회귀직선의 적합도 검토

   -   R2는 독립변수가 종속변수의 변동을 몇 %를 설명하는지 나타내는 지표
   -   다변량 회귀분석에서는 독립변수의 수가 많아지면 R2 가 높아지므로 독립변수가 유의하든 않든 R2가 높아지는 단점이 있음
   -   위 단점을 보완하기 위해 수정 결정계수 (R<sub>a</sub><sup>2</sup>: adjusted R<sup>2</sup>)를 활용한다. 수정결정 계수는 결졍계수보다 작은 값으로 산출되는 특징
   -   수정결정계수 : $1 - \dfrac {(n-1)(1 - R^2)}{n - k - 1} = 1 - \dfrac {(n-1) Ⅹ (\dfrac {SSE} {SST})} {n - k - 1} = 1 - (n - 1) \dfrac {MSE} {SST}$
   -   (k: 독립변수 개수, n : 데이터의 개수)
   -   오차(error) : 모집단에서 실제값이 회귀선과 비교해 볼 때 나타나는 차이 (정확치와 관측지의 차이)
   -   잔차 (residual) : 표본에서 나온 관측값이 회귀선과 비교해볼 때 나타나는 차이

   

   

   #### 3. 다중선형회귀분석 (p.315)

   ##### 가. 다중선형회귀분석 (다변량회귀분석)

   - 다중회귀식 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$

   - 모형의 통계적 유의성

     - 모형의 통계적 유의성은 F-통계량으로 확인

     - 유의수준 5% 하에서 F-통계량의 p-value가 0.05보다 작으면 통계적으로 유의함

     - F-통계량이 크면 p-value가 0.05보다 작아지고 이렇게 되면 귀무가설을 기각한다.

       >   귀무가설 : $H_0: \beta_1 = \beta_2 = ... \beta_k = 0$ vs 대립가설 : $H_1 : \beta_1 \neq \beta_2 \neq ... \neq \beta_k \neq 0$

       | 요인 |      제곱합      | 자유도 |      제곱평균       |   F-통계량    |
       | :--: | :--------------: | :----: | :-----------------: | :-----------: |
       | 회귀 | 회귀제곱합(SSR)  |   k    |    MSR = SSR / k    | F = MSR / MSE |
       | 오차 | 오차제곱합(SSE)  | n-k-1  | MSE = SSE / (n-k-1) |               |
       |  계  | 전체제곱합 (SST) |  n-1   |                     |               |

     - 회귀자유도(k) + 오차자유도(n-k-1) = 전체자유도(n-1) 
     - F(회귀자유도, 오차자유도)  = F(k, n-k-1)   ([참조블로그](https://hyen4110.tistory.com/50))
     - 모형의 설명력은 결정계수(R<sup>2</sup>)나 수정결정계수(R<sub>a</sub><sup>2</sup>)를 확인

     - 모형의 적합성 : 잔차와 종속변수의 산점도로 확인

     - 데이터가 전제하는 가정을 만족하는가? 선형성, 독립성, 등분산성, 비상관성, 정상성

     - 다중공선성 (multicollinearity)

       -   다중회귀분석에서 설명변수들 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란
       -   다중공선성 검사방법
           -   분산팽창요인 (VIF) : 4보다 크면 다중공선성이 존재, 10보다 크면 심각한 문제가 있는것으로 해석
           -   상태지수 : 10이상이면 문제가 있음, 30보다 크면 심각한 문제가 있음

     - 분산분석표 (위 아래 중 뭐가 맞음?? **중요*****) [출처](https://m.blog.naver.com/leejist/221381304538)

     -   | 원천     | 제곱합(SS) | 자유도(df) | 평균제곱(MS)    | F             |
         | -------- | ---------- | ---------- | --------------- | ------------- |
         | 집단간   | SSB        | k-1        | MSB = SSB / k-1 | F = MSB / MSW |
         | 집단내   | SSW        | n-k        | MSW = SSW / n-k | F(k-1, n-k)   |
         | 총(합계) | SST        | n-1        |                 |               |

       -   총제곱합 = 집단간 제곱합 + 집단내 제곱합
           -   SSB (Sum of Squared Between groups, 집단간 제곱합)
           -   SSW (Sum of Squared Within groups, 집단내 제곱합)
           -   SST (Sum of Squared Total, 총제곱합)
       -   총평균제곱 = 집단간 평균제곱 + 집단내 평균제곱
           -   MSB (Mean of Squared Between groups, 집단간 평균제곱)
           -   MSW (Mean of Squared Within groups, 집단내 평균제곱)
           -   MST (Mean of Squared Total, 총평균 제곱)
           -   k: 집단의 수, n: 관측치의 수(표본의 수)


   

4. #### 4. 회귀분석의 종류 (p.316)

   -   단순회귀 : $Y = \beta_0 + \beta_1X + \epsilon$ : 독립변수가 1개이며 종속변수와의 관계가 직선
   -   다중회귀 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon $ : 독립변수가 k개 이며 종속변수와의 관계가 선형 (1차함수)
   -   로지스틱회귀 : $P(y) = \dfrac {1} {1 + exp[-(\beta_0 + \beta_1X_1 + ... + \beta_kX_k + \epsilon)]}$  : 종속변수가 범주형(2진변수)인 경우에 적용되며, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장될 수 있음
   -   다항회귀 : K=2이고 2차 함수인 경우
       -   $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{11}X_1^2 + \beta_{22}X_2^2 + \beta_{12}X_1X_2 + \epsilon$ 
       -   독립변수와 종속변수와의 관계가 1차함수 이상인 관계 (단, k=1이면 2차 함수 이상)
   -   곡선회귀
       -   2차 곡선인 경우 : $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon $
       -   3차 곡선인 경우 : $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon $
       -   독립변수가 1개이며 종속변수와의 관계가 곡선
   -   비선형회귀 : $Y = \alpha e^{-\beta X} + \epsilon $ : 회귀식의 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형



#### 5. 회귀분석 사례 (p.316)



#### 6. 최적회귀방정식 (p.319)

-   AIC : 전진선택법 : AIC 점수가 가장 낮은 변수 추가, 후진제거법 : AIC 점수가 가장 낮은 변수 제거
-   ==F-Value, T-value, AIC, BIC 다시 공부해라 (중요)==



##### step 함수 이용한 전진 선택법

```R
> x1 <- c(7,1,11,11,7,11,3,1,2,21,1,11,10)
> x2 <- c(26,29,56,31,52,55,71,31,54,47,40,66,68)
> x3 <- c(6,15,8,8,6,9,17,22,18,4,23,9,8)
> x4 <- c(60,52,20,47,33,22,6,44,22,26,34,12,12)
> y <- c(78.5, 74.3, 104.3, 87.6, 95.9, 109.2, 102.7, 72.5, 93.1, 115.9, 83.8, 113.3, 109.4)
> df <- data.frame(x1, x2, x3, x4, y)
> head(df)
  x1 x2 x3 x4     y
1  7 26  6 60  78.5
2  1 29 15 52  74.3
3 11 56  8 20 104.3
4 11 31  8 47  87.6
5  7 52  6 33  95.9
6 11 55  9 22 109.2


> # step 함수를 사용한 전진선택법의 적용
> step(lm(y~1, data=df), scope=list(lower=~1, upper=~x1+x2+x3+x4), direction="forward")
Start:  AIC=71.44
y ~ 1

       Df Sum of Sq     RSS    AIC
+ x4    1   1831.90  883.87 58.852
+ x2    1   1809.43  906.34 59.178
+ x1    1   1450.08 1265.69 63.519
+ x3    1    776.36 1939.40 69.067
<none>              2715.76 71.444

Step:  AIC=58.85
y ~ x4

       Df Sum of Sq    RSS    AIC
+ x1    1    809.10  74.76 28.742
+ x3    1    708.13 175.74 39.853
<none>              883.87 58.852
+ x2    1     14.99 868.88 60.629

Step:  AIC=28.74
y ~ x4 + x1

       Df Sum of Sq    RSS    AIC
+ x2    1    26.789 47.973 24.974
+ x3    1    23.926 50.836 25.728
<none>              74.762 28.742

Step:  AIC=24.97
y ~ x4 + x1 + x2

       Df Sum of Sq    RSS    AIC
<none>              47.973 24.974
+ x3    1   0.10909 47.864 26.944

Call:
lm(formula = y ~ x4 + x1 + x2, data = df)

Coefficients:
(Intercept)           x4           x1           x2  
    71.6483      -0.2365       1.4519       0.4161  

> 
```

- 벌점화 방식을 적용한 전진선택법을 실시한 결과, 가장 먼저 선택된 변수는 ```AIC``` 값이 ```58.852```으로 가장 낮은 ```x4``` 였다.`x4`에 `x1`을 추가하였을 떄 `AIC` 값이 `28.742`로 낮아지게 되었고, `x2`를 추가하였을 때 `AIC` 값이 `24.974`로 최소화되어 더 이상 `AIC`를 낮출 수 없어 변수선택을 종료하게 되었다.
- 최종적으로 선택된 추정된 회귀식은 `y = 71.6483 - 0.23645*x4 + 1.4519*x1 + 0.4161*x2` 이다.





### 4절 시계열 분석

#### 1. 시계열 자료 (p.328)

##### 나. 시계열 자료의 종류

- 비정상성 시계열 자료 : 시계열 분석을 실시할 떄 다루기 어려운 자료로 대부분의 시계열 자료가 이에 해당
- 정상성 시계열 자료 : 비정상성 시계열을 핸들링해 다루기 쉬운 시계열 자료로 변환한 자료



#### 2. 정상성 (p.328)

정상성은 **평균이 일정, 분산이 일정**, 공분산도 단지 시차에만 의존하고 **실제 특정 시점 t, s에는 의존하지 않을 떄 만족**함

##### 가. 평균이 일정할 경우

- 모든 시점에 대해 일정한 평균을 가짐
- 평균이 일정하지 않은 **시계열은 차분(difference)**을 통해 정상화 할 수 있다.

> 차분이란?
>
> - 현시점 자료에서 전시점 자료를 빼는 것
> - 일반차분 (regular difference) : 바로 전 시점의 자료를 빼는 방법
> - 계절차분 (seasonal difference) : 여러 시점 전의 자료를 뺴는 방법, 주로 계절성을 갖는 자료를 정상화 하는데 사용



##### 나. 분산이 일정

- 분산도 시점에 의존하지 않고 일정해야함
- 분산이 일정하지 않을 경우 **변환 (transformation)을 통해 정상화** 할 수 있다.



##### 다. 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않는다.     



#### 3. 시계열 자료 분석방법 (p.329)

##### 가. 분석방법

- 회귀분석 (계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등이 있다.



##### 나. 자료 형태에 따른 분석 방법

- 일변량 시계열 분석
  - Box-Jenkins (ARMA), 지수 평활법, 시계열 분해법 등이 있다.
  - 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심을 갖는 경우의 시계열 분석
- 다중 시계열
  - 계량경제 모형, 전이함수 모형, 개입분석, 상태공간 분석, 다변량 ARIMA 등
  - 여러개의 시간(t)에 따른 변수들을 활용하는 시계열 분석
  - 예) 이자율, 인플레이션이 환율에 미치는 요인



##### 다. 이동평균법

- 개념
  - 추세를 파악하여 다음 기간을 예측하는 방법
  - n개의 시계열 데이터를 m기간으로 이동평균하면 n-m+1개의 이동평균 데이터가 생성된다.
- 특징
  - 간단하고 쉽게 미랠르 예측가능, 자료의 수가 많고 안정된 패턴을 보이는 경우 예측의 품질이 높음
  - 특정 기간안에 속하는 시계열에 대해서는 동일한 가중치를 부여
  - 불규칙변동이 심하지 않은 경우에는 짧은 기간(m의 개수가 적음), 반대로 불규칙변동이 심한 경우 긴 기간 (m의 개수가 많음)의 평균을 사용
  - 이동평균에서 가장 중요한 것은 적절한 기간을 사용하는 것. 즉, 적절한 n의 개수를 결정하는 것.



##### 라. 지수평활법 (Exponential Smoothing)

- 개념
  - 일정기간의 평균을 이용하는 이동평균법과 달리 모든 시계열 자료를 사용하여 평균을 구하며, 시간의 흐름에 따라 최근 시계열에 더 많은 가중치를 부여하여 미래를 예측하는 방법
  - <img width="593" alt="es" src="https://user-images.githubusercontent.com/291782/162599656-cb8a5076-7f8d-4160-a29c-223be2fd0570.png">
  - 여기서 F<sub>n+1</sub>은 n시점 다음의 예측값, &alpha;는 지수평활계수, Z<sub>n</sub>은 n시점의 관측값이며, 지수평활계수가 과거로 갈수록 지수형태로 감소하는 형태인 것을 확인할 수 있음
  - 다음은  [블로그](https://ko.logpresso.com/documents/time-series) 글을 참조하여 조금 더 쉽게 설명
  - $S_t = \alpha Y_{t-1} + (1-\alpha)S_{t-1}$
  - S<sub>t</sub> : 다음 예측치, Y<sub>t-1</sub> : 현재 값, S<sub>t-1</sub> : 이전 예측치, &alpha; : 0보다 크고 1보다 작은 스무딩 매개 변수
- 특징
  - 단기간에 발생하는 불규칙변동을 평활하는 방법
  - 자료의 수가 많고, 안정된 패턴을 보이는 경우일수록 예측 품질이 높음
  - 지수평활법에서 가중치의 역할을 하는 것은 지수평활계수(&alpha;)이며, 불규칙변동이 큰 시계열의 경우 지수평활계수는 작은 값을, 불규칙변동이 작은 시계열의 경우, 큰 값의 지수평활계수를 적용(generally, &alpha; is between 0.05 and 0.3)
  - 지수평활계수는 예측오차 (실제 관측치와 예측치 사이의 잔차제곱합)를 비교하여 예측오차가 가장 작은 값을 선택하는 것이 바람직함
  - 지수평활계수는 과거로 갈수록 지속적으로 감소함
  - 지수평활법은 불규칙변동의 영향을 제거하는 효과가 있으며, 중기 예측 이상에 주로 사용됨
  - 단, 단순지수 평활법의 경우, 장기추세나 계절변동이 포함된 시계열의 예측에는 적합하지 않음



#### 4. 시계열모형 (p.331)

##### 가. 자기회귀 모형 (AR모형, autoregressive model)

- p 시점 전의 자료가 현재 자료에 영향을 주는 모형

- $Z_t = \Phi_1Z_{t-1} + \Phi_2Z_{t-2} + ... + \Phi_pZ_{t-p} + \alpha_t$

- >- Z<sub>t</sub> : 현재 시점의 시계열 자료
  >- Z<sub>t-1</sub>, Z<sub>t-2</sub> ..., Z<sub>p</sub> : 이전, 그 이전 시점 p의 시계열 자료
  >- &Phi;<sub>p</sub> : p 시점이 현재에 어느 정도 영향을 주는지를 나타내는 모수
  >- &alpha;<sub>t</sub> : 백색잡음과정 (white noise process) : 시계열분석에서 오차항을 의미
  >- 평균이 0, 분산이 &sigma;<sup>2</sup>, 자기공분산이 0인 경우를 뜻하며, 시계열간 확률적 독립인 경우 강(strictly) 백색잡음 과정이라고 한다. 백색잡음 과정이 정규분포를 따를 경우 이를 가우시안(Gaussian) 백색잡음과정이라고 한다.

- AR(1) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &alpha;<sub>t</sub>, 직전 시점 데이터로만 분석

- AR(2) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &Phi;<sub>2</sub>Z<sub>t-2</sub> + &alpha;<sub>t</sub>, 연속된 2시점 정도의 데이터로 분석

- AR(2) 모형의 자기상관함수(ACF)와 편자기상관함수(PACF)

- <img width="584" alt="acf-pacf" src="https://user-images.githubusercontent.com/291782/162600723-a762b3b5-6461-4b8c-972e-c100914b3da5.png">



##### 나. 이동평균 모형 (MA 모형, Moving Average model)

- 유한한 개수의 백색잡음의 결합이므로 언제나 정상성을 만족
- 1차 이동평균모형 (MA1 모형)은 이동평균모형 중에서 가장 간단한 모형으로 시계열이 같은 시점의 백색잡음과 바로 전 시점의 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub> -  &phi;<sub>2</sub>&alpha;<sub>t-2</sub> - ... -  &phi;<sub>p</sub>&alpha;<sub>t-p</sub> 
- 2차 이동평균모형 (MA2 모형)은 바로 전 시점의 백색잡음과 시차가 2인 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub>
- AR모형과 반대로 ACF에서 절단점을 갖고, PACF가 빠르게 감소
- $Z_t = \alpha_t - \phi1\alpha_{t-1} - \phi_2\alpha_{t-2}$



##### 다. 자기회귀누적이동평균 모형 (ARIMA(p,d,q) 모형, autoregressive integrated moving average model)

- ARIMA 모형은 비정상시계열 모형이다

- ARIMA 모형은 차분이나 변환을 통해 AR모형이나 MA모형, 이 둘을 합친 ARMA 모형으로 정상화 할 수 있다.

- p는 AR모형, q는 MA모형과 관련이 있는 차수

- 시계열 {Z<sub>t</sub>}의 d번 차분한 시계열이 ARMA (p, q) 모형이면, 시계열 {Z<sub>t</sub>}는 차수가 p,d,q인 ARIMA 모형, 즉 ARIMA(p,d,q) 모형을 갖는다고 한다.

- d=0이면 ARMA(p, q) 모형이라 부르고, 이 모형은 정상성을 만족한다. (ARMA (0 , 0)일 경우 정상화가 불필요)

- p=0 이면 IMA (d, q) 모형이라 부르고, d번 차분하면 MA(q) 모형을 따른다.

- q = 0이면 ARI (p, d) 모형이라 부르며, d번 차분한 시계열이 AR (p) 모형을 따른다.

  > ARIMA (0, 1, 1)의 경우에는 1차분 후 MA(1) 활용
  >
  > ARIMA (1, 1, 0)의 경우에는 1차분후 AR(1) 활용
  >
  > ARIMA (1, ,1 2)의 경우에는 1차분 후 AR(1), MA(2), ARMA (1, 2) 선택 활용
  >
  > => 이런 경우 가장 간단한 모형을 선택하거나 AIC 를 적용하여 점수가 가장 낮은 모형을 선정



##### 라. 분해 시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

  > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
  >
  > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
  >
  > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
  >
  > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인



##### 마. R을 이용한 시계열 분석

- 맥북 documents의 R 폴더 확인



### 5절 다차원척도법

#### 1. 다차원척도법 (multidimensional scaling) (p.340)

- 객체간 근접성 (proximilty)을 시각화 하는 통계기법
- 개체들을 대상으로 변수들을 측정한 후 개체들 사이의 유사성 / 비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석방법
- <img width="659" alt="multi-dimension-scaling" src="https://user-images.githubusercontent.com/291782/162624740-2e70ee2b-4e6a-44fb-bcf8-30d0a34a0e49.png">



#### 2. 다차원척도법 목적

- 데이터속에 잠재해 있는 패턴(pattern), 구조를 찾아냄
- 소수차원의 공간에 기하학적으로 표현
- 데이터 축소 (data reduction)의 목적으로 다차원척도법을 이용. 즉 데이터에 포함되는 정보를 끄집어내기 위해서 다차원척도법을 탐색수단으로써 사용



#### 3. 다차원척도법 방법

- 개체들의 거리 계산에는 **유클리드 거리행렬을 활용**한다.

  $d_{ii} = \sqrt{(x_{il} - x_{il})^2 + ... + (x_{iR} - x_{ir})^2}$

- 최적모형의 적합은 부적합도를 최소로 하는 방법으로 일정 수준이하로 될 때까지 반복해서 수행

- |   STRESS    |         적합도 수준          |
  | :---------: | :--------------------------: |
  |      0      |        완벽 (perfect)        |
  |  0.05 이내  |    매우 좋은 (excellent)     |
  | 0.05 ~ 0.10 |     만족 (satisfactory)      |
  | 0.10 ~ 0.15 | 보통 (acceptable, but doubt) |
  |  0.15 이상  |         나쁨 (poor)          |



#### 4. 다차원척도법 종류 (p.341)

##### 가. 계량적 MDS (Metric MDS)

- 데이터가 **구간척도나 비율척도인 경우 활용**한다. (전통적인 다차원척도법)



##### 나. 비계량적 MDS (nonmetric MDS)

- 데이터가 **순서척도**인 경우 활용. 개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환 (monotone transformation)하여 거리를 생성한 후 적용



### 6절 주성분 분석

#### 1. 주성분분석 (Principal Component Analysis) (p.345)

- 여러 변수들의 변량을 '주성분 (principal component)'이라는 **서로 상관성이 높은 변수들의 선형 결합**으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법
- 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로 첫 번째 주성분과 상관성이 없어서(낮아서) 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만듦



#### 2. 주성분분석의 목적

- 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 해줌
- 다중공선성이 존재하는 경우, **상관성이 없는(적은) 주성분으로 변수들을 축소**하여 모형 개발에 활용된다. **회귀분석** 등의 모형 개발 시 입력변수들간의 상관관계가 높은 **다중공선성(multicollinearity)**이 존재할 경우 모형이 잘 못 만들어져 문제가 생김
- 차원을 축소한 후에 **군집분석을 수행하면 군집화 결과와 연산속도를 개선**할 수 있다.



#### 3. 주성분분석 vs 요인분석

##### 가. 요인분석(factor analysis)

- 등간척도 (혹은 비율척도)로 측정한 두 개 이상의 변수들에 잠재되어 있는 공통인자를 찾아내는 기법



##### 나. 공통점

- 모두 데이터를 축소하는데 활용됨. 원래 데이터를 활용해서 몇 개의 새로운 데이터를 만들 수 있다.



##### 다. 차이점

1. 생성된 변수의 수
   - 요인분석은 몇 개라고 지정 없이 (2 or 3, 4, 5..) 만들 수 있다.
   - 주성분분석은 제1주성분, 제2주성분, 제3주성분 정도로 활용된다. (대개 4개 이상은 넘지 않음)
2. 생성된 변수의 이름
   - 요인분석은 분석자가 요인의 이름을 명명한다.
   - 주성분분석은 주로 제1주성분, 제2주성분 등으로 표현
3. 생성된 변수들 간의 관계
   - 요인분석은 새 변수들은 대등한 관계를 갖고 '어떤 것이 더 중요하다'라는 의미는 없다. 단, 분류/예측에 그 다음 단계로 사용된다면 그 때 중요성의 의미가 부여된다.
   - 주성분분석은 제1주성분이 가장 중요하고, 그 다음 제2주성분이 중요하게 취급
4. 분석 방법의 의미
   - 요인분석은 목표변수를 고려하지 않고 그냥 데이터가 주어지면 변수들을 비슷한 성격들로 묶어서 새로운 [잠재] 변수들을 만든다.
   - 주성분분석은 목표 변수를 고려하여 목표 변수를 잘 예측/분류하기 위하여 원래 변수들의 선형 결합으로 이루어진 몇 개의 주성분(변수)들을 찾아내게 된다.



#### 4. 주성분의 선택법 (p.346)

- 주성분분석의 결과에서 누적기여율(cumulative proportion)이 85% 이상이면 주성분의 수로 결정할 수 있다.
- <img width="643" alt="pca-ex1" src="https://user-images.githubusercontent.com/291782/162625936-20559085-98d6-49f3-831f-8e9310c8d8c9.png">
- <img width="372" alt="scree-plot" src="https://user-images.githubusercontent.com/291782/162625976-440be6b8-33af-4388-aced-f7c9f5becc6c.png">



#### 5. 주성분 분석 사례 (p.347)

- USArrests 자료를 이용한 분석





### 5장 정형 데이터 마이닝

### 1절 데이터마이닝의 개요

#### 1. 데이터마이닝 (p.385)

##### 가. 개요

- 데이터마이닝은 대용량 데이터에서 의미있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법



##### 나. 통계분석과의 차이점

- 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 데이터마이닝은 다양한 수리 알고리즘을 이용해 **데이터베이스의 데이터로부터 의미있는 정보를 찾아내는 방법을 통칭**



##### 다. 종류

- 정보를 찾는 방법론에 따른 종류
  - 인공지능 (Artificial Intelligence)
  - 의사결정나무 (Decision Tree)
  - K-평균군집화 (K-means clustering)
  - 연관분석 (Association Rule)
  - 회귀분석 (Regression)
  - 로짓분석 (Logit Analysis)
  - 최근접이웃 (Nearest Neighborhood)
- 분석대상, 활용목적, 표현방법에 따른 분류
  - 시각화분석 (Visualization Analysis)
  - 분류 (Classification)
  - 군집화 (Clustering)
  - 포케스팅 (Forecasting)



#### 2. 데이터마이닝의 분석방법

- 지도학습 (Supervisied Learning)
  - 의사결정나무 (DT), 인공신경망 (Artifician Neural Network), 일반화 선형 모형 (GLM, Generalized Linear Model)
  - 회귀분석 (regression analysis), 로지스틱 회귀분석 (logistic regression analysis), 사례기반 추론 (case-based reasoning), 최근접이웃법 (KNN)
- 비지도학습 (Unsupervised Learning)
  - OLAP (On-Line Analytical Processing), 연관성 규칙발견 (Association Rule Discovery, Market Basket)
  - 군집분석 (K-Means Clustering), SOM (Slef Organizing Map)



#### 3. 분석 목적에 따른 작업 유형과 기법

- 예측 (Predictive Modeling)
  - 분류규칙 (Classification) : 과거의 데이터로부터 고객 특성을 찾아내어 분류모형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것으로 목표 마케팅 및 고객 신용평가 모형에 활용됨 (사용기법 : 회귀분석, 판별분석, 신경망, 의사결정나무)
- 설명 (Descriptive Modeling)
  - 연관규칙 (Association) : 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업. 제품이나 서비스의 교차판매 (cross selling), 매장진열 (display), 첨부우편 (attached mailings), 사기적발 (fraud detection) 등의 다양한 분야에 활용됨 (사용기법 : 동시발생 매트릭스)
  - 연속규칙 (sequence) : 연관 규칙에 시간관련 정보가 포함된 형태. 고객의 구매이력 (history) 속성이 반드시 필요. 목표 마케팅 (target marketing), 일대일 마케팅 (one to one marketing)에 활용. (사용기법 : 동시발생 매트릭스)
  - 데이터 군집화 (Clustering) : 고객 레코드들을 유사한 특성을 지닌 몇개의 소그룹으로 분할하는 작업. 작업의 특성이 분류규칙 (Classification)과 유사하나 분석대상 데이터에 결과 값이 없으며, 판촉활동이나 이벤드 대상을 선정하는데 활용 (사용기법 : K-Means Clustering)



#### 4. 데이터마이닝 추진 단계 (p.387)

1. 목적 설정
2. 데이터 준비
3. 가공
4. 기법적용
5. 검증



#### 5. 데이터마이닝을 위한 데이터 분할

- 구축용 (training data, 50%) : 추정용, 훈련용 데이터라고도 불리며 데이터마이닝 모델을 만드는데 활용
- 검정용 (validation data, 30%) : 구축과 모형의 과대추정 또는 과소추정을 미세 조정하는데 활용
- 시험용 (test data, 20%) : 모델의 성능을 검증하는데 활용
- 데이터의 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우
  - 홀드아웃(hold-out) 방법 : 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용하는 방법. 학습용 데이터와 시험용으로 분리
  - 교차확인 (cross-validation) 방법 : 주어진 데이터를 k개의 하부집단으로 구분하여, k-1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습. k번 반복 측정한 결과를 평균낸 값을 최종적으로 사용. k-fold 교차분석을 주로 많이 사용.



#### 6. 성과분석 (p.389)

##### 가. 오분류에 대한 추정치

- |      |          | 예측     | 예측     |      |
  | ---- | -------- | -------- | -------- | ---- |
  |      |          | Positive | Negative |      |
  | 실제 | Positive | TP       | FN       | P    |
  | 실제 | Negative | FP       | TN       | N    |

  - 참긍정률(TPR)  = $\dfrac{TP}{TP+FN}$ = 재현율(Recall) = 민감도(Sensitive) = ROC의 세로축
  - 거짓긍정률(FPR) = $\dfrac {FP}{FP+TN}$ = (1 - 특이도(Specificity)) = ROC의 가로축
  - 정확도(Accuracy, 정분류율) = $\dfrac {TP+TN}{TP+TN+FP+FN}$
  - 오분류율(Error Rate) : $1 - Accuracy = \dfrac {FN + FP} {TP+TN+FP+FN}$
  - 정밀도(Precision) = $\dfrac {TP}{TP+FP}$
  - 재현도(Recall) = 민감도(Sensitive) = $\dfrac {TP}{TP+FN}$ = TPR(참긍정률)
  - 특이도(Specificity, TNR, True Negative Rate) = $\dfrac{TN}{TN+FP}$
  - F1-Score 에 들어가는 지표는? 정밀도(Precision) 와 재현율(Recall, 민감도)
    - 식 = $2 × \dfrac {Precision × Recall}{Precision + Recall}  $ 
    - 재현율과 정밀도 값이 모두 클 때 F1-Score도 큰 값을 가진다
    - F1-Score는 민감도와 정밀도를 합한 **성능평가지표**로 0~1 사이의 값을 가진다. 1이 좋음



##### 나. ROCR 패키지로 성과분석

1. ROC Curve (Receiver Operating Characteristic Curve)
   - ROC 커브란 가로축을 FPR (False Positive Rate = 1 - 특이도) 값으로 두고, 세로축을 TPR (True Positive Rate, 민감도) 값으로 두어 시각화한 그래프
   - 2진 분류 (binary classfication)에서 모형의 성능을 평가하기 위해 많이 사용되는 척도
   - ROC 곡선 아래의 면적을 의미하는 AUROC (Area Under ROC) 값이 크면 클수록 (1에 가까울수록 ) 모형의 성능이 좋다고 평가.
   - AUROC : 0.9 ~ 1.0 (excellent), 0.8 ~ 0.9 (good), 0.7 ~ 0.8 (fair), 0.6 ~ 0.7 (poor), 0.5 ~ 0.6 (fail)



##### 다. 이익도표 (Lift chart)

1. 이익도표의 개념

   - 이익도표는 분류모형의 성능을 평가하기 위한 척도로, 분류된 관측치에 대해 얼마나 예측이 잘 이루어졌는지를 나타내기 위해 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표

2. 이익도표의 활용 예시

   ![lift-chart](https://user-images.githubusercontent.com/291782/162685389-03b19520-81e3-49ca-b533-8c816820138f.png)

   - 전체 2000명 중 381명 구매
   - Frequency of 'buy' : 2000명 중 실제로 구매한 사람
   - % Captured of response : 반응검출율: 해당 등급의 실제 구매자 / 전체 구매자
   - % response : 반응률 = 해당 등급의 실제 구매자 / 200명 (=2000명 / 10구간)
   - Lift : 향상도 : 반응률 / 기본 향상도 (좋은 모델이라면 Lift가 빠른 속도로 감소해야 한다.)

   

   







### 2절 분류분석

#### 1. 분류분석과 예측분석 (p.396)

##### 가. 분류분석의 정의

- 데이터가 어떤 그룹에 속하는지 예측하는데 사용
- 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있음
- 교사학습 (supervised learning)에 해당하는 예측기법



##### 나. 예측분석의 정의

- 시계열분석처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것
- 여러 개의 설명변수(독립변수)가 아닌, 한 개의 설명변수로 생각하면 됨



##### 다. 분류분석과 예측분석의 공통점과 차이점

- 공통점
  - 레코드의 특정 속성의 값을 미리 알아맞히는 점
- 차이점
  - 분류  :레코드(튜플)의 **범주형 속성**의 값을 알아 맞춤
  - 예측 : 레코드 (튜플)의 **연속형 속성**의 값을 알아 맞춤



##### 라. 예

- 분류
  - 학생들의 국어, 영어, 수학 점수를 통해 내신등급을 맞추는 것
  - 카드회사에서 회원들의 가입 정보를 통해 1년 후 신용등급을 맞추는 것
- 예측
  - 학생들의 여러 가지 정보를 입력하여 수능점수를 맞추는 것
  - 카드회사 회원들의 가입정보를 통해 년 매출액을 맞추는 것



##### 마. 분류 모델링

- 신용평가모형 (우량, 불량)
- 사기방지모형 (사기, 정상)
- 이탈모형 (이탈, 유지)
- 고객세분화 (VVIP, VIP, GOLD, SIVER, BRONZE)



##### 바.  분류 기법

- 회귀분석, 로지스틱 회귀분석 (logistic regression)
- 의사결정나무 (DT), CART (Classification and Regression Tree), C5.0
- 베이지안분류 (Bayesian classification), Naive Bayesian
- 인공신경망 (ANN, artificial neural network)
- SVM (support vector machine, 지지도벡터기계)
- KNN (K-nearest neighborhood)
- 규칙기반의 분류와 사례기반추론 (Case-Based Reasoning)



#### 2. 로지스틱 회귀분석 (Logistic Regression) (p.397)

- 반응변수가 범주형인 경우 적용되는 회귀분석모형

- 신규 설명변수 추정 및 기준치에 따라 분류하는 목적(분류모형)으로 활용

- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)라고 함

  > 오즈비(odds ratio) : 오즈(odds)는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 확률
  >
  > ex) 16강에 한국과 브라질이 진출을 성공/실패할 확률과 각각의 오즈와 오즈비는 아래와 같음
  >
  > |  구분  | 16강 성공확률 | 16강 실패확률 |
  > | :----: | :-----------: | :-----------: |
  > | 브라질 |      0.8      |      0.2      |
  > |  한국  |      0.1      |      0.9      |
  >
  > odds (브라질) : $\dfrac {0.8} {1 - (0.8)} = \dfrac {0.8} {0.2} = 4$
  >
  > odds (한국) : $\dfrac {0.1} {1-0.1} = \dfrac {1} {9}$
  >
  > Odds ratio : $\dfrac {odds(브라질)} {odds(한국)} = \dfrac {4}{\dfrac {1}{9}} = 36$
  >
  > 오즈비가 36 이 나타나 브라질이 16강에 진출할 확률이 한국의 16강 진출 확률보다 36배 높다고 볼 수 있다.

- 선형회귀분석과 로지스틱 회귀분석 비교

  |    목적     |  선형회귀분석  |         로지스틱 회귀분석          |
  | :---------: | :------------: | :--------------------------------: |
  |  종속변수   |  연속형 변수   |               (0, 1)               |
  | 계수 추정법 |   최소제곱법   |           최대우도추정법           |
  |  모형 검정  | F-검정, T-검정 | 카이제곱 검정 (X<sup>2</sup>-test) |

  > 최대우도추정법 (MLE : Maximum Likelihood Estimation) : 모수가 미지의 &theta; (theta)인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 &theta;를 추정하는 기법

- glm() 함수를 활용하여 로지스틱 회귀분석 실행

- R코드 : glm(종속변수 ~ 독립변수1 +...+ 독립변수k, family=binomial, data=데이터셋명)



#### 3. 의사결정나무

##### 가. 정의

- 분류함수를 의사결정 규칙으로 이뤄진 **나무 모양으로 그리는 방법**
- 의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형**으로 분류나무와 회귀나무 모형이 있다.



##### 나. 예측력과 해석력

- 기대 집단의 사람들 중 가장 많은 반응을 보일 **고객의 유치방안을 예측**하고자 하는 경우에는 **예측력**에 치중한다.
- 신용평가에서는 심사 결과 부적격 판정이 나온 경우 고객에게 부적격 **이유를 설명**해야 하므로 **해석력**에 치중한다.



##### 다. 의사결정나무의 활용

1. 세분화
2. 분류
3. 예측
4. 차원축소 및 변수선택
5. 교호작용 효과의 파악 
   - 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
   - 범주의 병햡 또는 연속형 변수의 이산화



##### 라. 의사결정나무의 특징

- 장점
  - 결과를 누구에게나 설명하기 용이
  - 만드는 방법이 계산적으로 복잡하지 않음
  - 대용량 데이터에서도 빠르게 만들 수 있음
  - 비정상 잡음 데이터에 대해서도 민감함 없이 분류 가능
  - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않음
  - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다.
  - 모형 분류 정확도가 높다.
- 단점
  - 새로운 자료에 대한 과적합 발생할 가능성이 높다.
  - 분류 경계선 부근의 자료값에 대해서 오차가 크다.
  - 설명변수 간의 중요도를 판단하기 쉽지 않다.



##### 마. 의사결정나무의 분석 과정

- 분석과정은 크게 성장(growing), 가지치기(pruning), 타당성 평가, 해석 및 예측으로 이뤄짐



##### 바. 나무의 성장

- 분리기준 : 이산형 목표변수

  |       기준값        | 분리기준                                                     |
  | :-----------------: | ------------------------------------------------------------ |
  | 카이제곱 통계량 P값 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
  |      지니 지수      | 지니 지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택 |
  |    엔트로피 지수    | 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식마디를 형성 |

- 분리기준 : 연속형 목표변수

  |        기준값        | 분리기준                                                     |
  | :------------------: | ------------------------------------------------------------ |
  | 분산분석에서 F통계량 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
  |    분산의 감소량     | 분산의 감소량을 최대화하는 기준의 최적분리에 의해서 자식마디를 형성 |

  

#### 4. 불순도의 여러 가지 측도 (p.405)

- 목표변수가 범주형 변수인 의사결정나무의 분류규칙을 선택하기 위해서는 카이제곱 통계량, 지니지수, 엔트로피 지수를 활용

1. 카이제곱 통계량

   - 각 셀에 대한 ((실제도수 - 기대도수)의 제곱 / 기대도수)의 합
   - 기대도수 = 행의 합계 X 열의 합계 / 전체 합계
   - $X^2 =\displaystyle \sum_{i=1}^k \dfrac {(O_i - B_i)^2} {B_i}$ (k: 범주의 수, O: 실제도수, B: 기대도수)
   - ([카이제곱 참조블로그](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=soulfree90&logNo=50172915445))

2. 지니지수

   - 노드의 불순도를 나타내는 값
   - 지니지수의 값이 클수록 이질적(Diversity)이며 순수도(Purity)가 낮다고 볼 수 있다.
   - $Gini(T) = 1 - \displaystyle \sum_{l=1}^kP_l^2$

   ![gini-index](https://user-images.githubusercontent.com/291782/163111164-6b8ab5b6-f0be-4f50-a05e-7677a384df1f.png)

3. 엔트로피 지수

   - 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도

   - 엔트로피 지수의 값이 클수록 순수도(purity)가 낮다고 볼 수 있다.

   - 엔트로피 지수가 가장 작은 예측 변수와 이때의 최적분리 규칙에 의해 자식마디를 형성

   - $Entropy(T) = -(\displaystyle \sum_{l=1}^kP_llog_2P_l)$

     ![entropy-index](https://user-images.githubusercontent.com/291782/163111601-5dbfaf6e-dc4c-43d1-b1fc-993931bcd2d5.png)

     ![index-ex](https://user-images.githubusercontent.com/291782/163112135-a23e077a-5b2d-4cbe-9a13-93c8fd036ab7.png)

     ![gini-ex](https://user-images.githubusercontent.com/291782/163118061-6084da81-8347-4b19-8ec6-fb7a40ccd977.png)



#### 5. 의사결정나무 알고리즘 (p.408)

1. CART (Classification And Regression Tree)
   - 가장 많이 활용되는 의사결정나무 알즘으로 불순도의 측도로 출력(목적) 변수가 **범주형일 경우 지니지수**를, **연속형인 경우 이진분리**(binary split)를 사용
   - 개별 입력변수 뿐만 아니라 입력변수들의 선형겹할들 중에서 최적의 분리를 찾을 수 있다.
2. C4.5와 C5.0
   - CART와는 다르게 각 마디에서 다지분리(multiple split)가 가능하며 범주형 입력변수에 대해서는 범주의 수만큼 분리가 일어난다.
   - 불순도의 측도로는 **엔트로피지수**를 사용한다.
3. CHAID (Chi-squared Automatic Interaction Detection)
   - 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수이어야 한다.
   - 불순도의 측도로는 **카이제곱** 통계량을 사용



#### 6. 의사결정나무 예시 (p.408)

```R
> # install.packages("party")
> # party 패키지를 이용하여 의사결정나무 사용 
> library(party)
> # 7:3 으로 train , test 데이터 나누기
> idx <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))
> train.data <- iris[idx==1,]
> test.data <- iris[idx==2,]
> head(train.data)
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
4           4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa
> iris.tree <- ctree(Species~., data=train.data)
> plot(iris.tree)
```

![dt-plot1](https://user-images.githubusercontent.com/291782/163331102-a0caafe6-b2e8-4b1e-9516-5028ad874a3a.png)

```R
> plot(iris.tree, type="simple")
```

![dt-plot-simple](https://user-images.githubusercontent.com/291782/163331226-b5ca401e-fba8-4ce0-b210-9a1dd09cd533.png)

```R
> # 예측된 데이터와 실제 데이터 비교
> table(predict(iris.tree), train.data$Species)
            
             setosa versicolor virginica
  setosa         36          0         0
  versicolor      0         36         3
  virginica       0          0        30

> # test data를 적용하여 정확성 확인 
> test.pre <- predict(iris.tree, newdata=test.data)
> table(test.pre, test.data$Species)
            
test.pre     setosa versicolor virginica
  setosa         12          0         0
  versicolor      2         12         1
  virginica       0          2        16
```





### 3절 앙상블 분석

#### 1. 앙상블 (ensemble) (p.412)

##### 가. 정의

- 여러 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측 모형을 만드는 방법으로 다중 모델 조합(combining multiple models), 분류기 조합(classifier combination)이 있다.





##### 나. 학습방법의 불안정성

- 가장 안정적인 방법으로는 1-nearest neighbor (가장 가까운 자료만 변하지 않으면 예측 모형이 변하지 않음), 선형회귀모형(최소제곱법으로 추정해 모형 결정)이 존재한다.
- 가장 불안정한 방법으로는 의사결정나무가 있다.



##### 다. 앙상블 기법의 종류

1. 배깅
   - 여러개의 붓스트랩(bootstrap) 자료를 생성하고 각 붓스트랩 자료에 예측 모형을 만든 후 결합하여 최종 예측모형을 만드는 방법이다. 붓스트랩은(bootstrap)은 주어진 자료에서 동일한 크기의 표본을 랜덤 복원추출로 뽑은 자료를 의미
   - 보팅(voting)은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해서 최종 결과를 선정하는 과정
   - 최적 의사결정나무를 구축 시 가장 어려운 작업이 가지치기(pruning)이지만 배깅에서는 가지치기를 하지 않고 최대한 성장한 의사결정나무들을 활용
2. 부스팅
   - 예측력이 약한 모형(weak learner)들을 결합하여 강한 예측모형을 만드는 방법
   - 부스팅 방법 중 Freund & Schapire가 제안한 **Adaboost**는 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다. (단, 가중치의 합은 1)
   - 훈련 오차를 빨리 그리고 쉽게 줄일 수 있음
   - 예측 오차가 향상되어 Adaboost의 성능이 배깅보다 뛰어난 경우가 많다.
3. 랜덤포레스트 (random forest)
   - 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 **약한 학습기들을 생성한 후 이를 선형결합하여 최종 학습기를 만드는 방법**
   - random forest 패키지는 random input 에 따른 forest of tree를 이용한 분류방법
   - 수천 개의 변수를 통해 변수제거 없이 실행되므로 정확도 측면에서 좋은 성과를 보인다.
   - 이론적 설명이나 최종 결과해석이 어렵다는 단점이 있지만 예측력이 매우 높은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅과 부스팅보다 비슷하거나 좋은 예측력을 보인다.



### 4절 인공신경망 분석

#### 1. 인공신경망 분석 (ANN) (p.418)

##### 가. 인공신경망이란?

- 뇌를 기반으로 한 추론 모델
- 뉴런은 기본적인 정보처리 단위



##### 나. 인공신경망의 연구

- 헵(Hebb) : 신경세포(뉴런) 사이의 연결강도(weight)를 조정하여 학습규칙을 개발
- 로젠블럿(Rosenblatt, 1955) : 퍼셉트론 (perceptron)이라는 인공세포를 개발
- 비선형성의 한계점 발생 : XOR (eXclusive OR) 문제를 풀지 못하는 한계를 발견
- 홉필드(Hopfild), 러멜하트(Rumelhart), 맥클랜드(McClelland) : 역전파알고리즘(Backpropagation)을 활용하여 비선형성을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형이 등장했다.



##### 라. 인공신경망의 학습     

- 신경망은 가중치를 반복적으로 조정하며 학습한다.
- 뉴런은 링크(Link)로 연결되어 있고, 각 링크에는 수치적인 가중치가 있다.



##### 마. 인공신경망의 특징

- 뉴런의 활성화 함수

  - **시그모이드 함수**의 경우 로지스틱 회귀분석과 유사하며, 0 ~ 1의 확률값을 가진다.

    <img width="878" alt="sigmoid-fn" src="https://user-images.githubusercontent.com/291782/163423927-052bd171-81cc-45b6-97d8-70ba67c1a4e5.png">

  - softmax 함수 : 표준화지수 함수로도 불리며, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수

    - $y_i = \dfrac {exp(z_j)} {\displaystyle \sum_{i=1}^Lexp(z_i)}, j = 1, ... ,L$

  - Relu함수 : 입력값이 0 이하는 0, 0 초과는 x값을 가지는 함수. 최근 딥러닝에서 많이 활용하는 활성화 함수

    - $Y^{relu} = \begin{cases} 0,\; if \quad x \le 0 \\ x, \; if \quad x \gt 0  \end{cases}$

  - 단일 뉴런의 학습 (단층 퍼셉트론)

    - 퍼셉트론은 선형 결합기와 하드 리미터로 구성된다.

    - 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눈다.

      <img width="818" alt="hyperplane" src="https://user-images.githubusercontent.com/291782/163426523-78c739e8-fe97-499d-8d72-f111438f3988.png">



##### 바. 신경망 모형 구축 시 고려사항

1. 입력 변수
2. 가중치의 초기값과 다중 최소값 문제
3. 학습모드
4. 은닉층(hidden layer)과 은닉노드(hidden node)의 수
5. 과대 적합 문제



### 5절 군집분석

#### 1. 군집분석 (p.424)

##### 가. 개요

- 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명하는 분석 방법
- 특성에 따라 고객을 여러 개의 배타적인 집단으로 나누는 것



##### 나. 특징

1. 요인분석과의 차이점 : 요인분석은 유사한 변수를 함께 묶어주는 것이 목적
2. 판별분석과의 차이점 : 판별분석은 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적



#### 2. 거리 (p.425)

##### 가. 연속형 변수의 경우

- 유클리디안 거리 (Euclidean distance) : 데이터간의 유사성을 측정할 때 많이 사용하는 거리. 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않았다.

  $d(x,y) = \displaystyle \sqrt{(x_1 - y_1)^2 + \dots + (x_p - y_p)^2} = \sqrt{(x - y)^`(x-y)}$

- 표준화 거리 (statistical distance) : 해당 변수의 표준편차로 척도 변환 후 유클리디안 거리를 계산하는 방법. 표준화하게 되면 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다.

  $d(x , y) = \sqrt{(x - y)^`D^{-1}(x - y)} , \quad D = diag$ {$s_{11}, \dots , s_{pp}$}

- 마할라노비스 (Mahalanobis) 거리 : 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리 (standardized distance). 두 벡터 사이의 거리를 산포를 의미하는 표본공분산으로 나눠주어야 하며, 그룹에 대한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란한다.

  $d(x, y) = \sqrt {(x-y)^`S^{-1} (x - y)} \quad S = ${$S_{ij}$}는 공분산행렬

- 맨하탄거리 (Manhattan) 거리 : 유클리디안 거리와 함께 가장 많이 사용되는 거리로 맨하탄 도시에서 건물을 가기 위한 최단 거리를 구하기 위해 고안딘 거리.

  $d(x,y) = \displaystyle \sum_{i=1}^p |x_i - y_i|$

- 민코우스키 (Minkowski) 거리 : 맨하탄 거리와 유클리디안 거리를 한 번에 표현한 공식으로 L1 거리 (맨하탄), L2 거리(유클리디안 거리)라 불리고 있다.

  $d(x, y) = [\displaystyle \sum_{i=1}^p |x_i - y_i|^m]^{1/m} \quad m=1, m=2$



##### 나. 범주형 변수의 경우

- 자카드 거리 : $1 - J(A, B) = \dfrac {|A \cup B| - |A \cap B|} {|A \cup B|}$

- 자카드 계수 : $J(A, B) = \dfrac {|A \cap B|} {|A \cup B|}$

- 코사인 거리 : 문서를 유사도를 기준으로 분류 혹은 그룹핑 할 떄 유용하게 사용한다.

  $d_{cos}(A, B) = 1 - \dfrac {A \cdot B } {||A||_2 \cdot || B ||_2}$

  <img width="855" alt="cos-dist" src="https://user-images.githubusercontent.com/291782/163675115-997131bf-6f0b-4ddf-a8ea-afd69bb1ea62.png">

  

  #### 3. 계층적 군집분석 (p.426)

  - 계층적 군집방법은 n개의 군집으로 시작해 점차 군집의 개수를 줄여 나가는 방법
  - 방법에는 합병형 방법 (agglomerative: bottom-up) 과 분리형 방법 (Divisive : top-down)이 있다.

  

  ##### 가. 최단연결법 (single linkage, nearest neighbor)

  - n*n 거리행렬에서 거리와 가장 가까운 데이터를 묶어서 군집을 형성

  - 군집과 군집 또는 데이터와의 거리를 계산 시 최단거리(min)를 거리로 계산하여 거리행렬 수정을 진행한다.

    <img width="864" alt="nearest-neighbor" src="https://user-images.githubusercontent.com/291782/163675248-f9e952bb-a8d2-47bb-a1f3-d7eeb5f0c658.png">

  

  ##### 나. 최장연결법 (complete linkage, farthest neighbor)

  - 군집과 군집 또는 데이터와의 거리를 계산할 때 최장거리 (max)로 거리를 계산하여 거리행렬을 수정하는 방법

    <img width="867" alt="farthest-neighbor" src="https://user-images.githubusercontent.com/291782/163675302-c5ce2e2f-f964-472b-8d3c-1994bbc3ac39.png">

  

  ##### 다. 평균연결법 (average linkage)

  - 군집과 군집 또는 데이터와의 거리를 계산할 때 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법

    <img width="861" alt="mean-linkage" src="https://user-images.githubusercontent.com/291782/163675340-b400d381-04fd-42f6-9a6f-bc9671680442.png">

  

  

  ##### 라. 와드연결법 (ward linkage)

  - 군집내 편차들의 제곱합을 고려한 방법
  - 군집 간 정보의 손실을 최소화하기 위해 군집화를 진행

  

  ##### 마. 군집화

  - 거리행렬을 통해 가장 가까운 거리의 객체들간의 관계를 규명하고 덴드로그램을 그림

  - 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수를 선정. (보통 5개 이상은 잘 활용하지 않음)

  - 군집화 단계

    1. 거리행렬을 기준으로 덴드로그램을 그림
    2. 덴드로그램 최상단부터 세로축의 개수에 따라 가로선을 그어 군집의 개수를 선택
    3. 각 객체들의 구성을 고려해서 적절한 군집수를 선정

    <img width="845" alt="dendrogram" src="https://user-images.githubusercontent.com/291782/163675452-b00ff6c3-2012-46ff-9414-f8dac0405b89.png">





#### 4. 비계층적 군집분석 (p.429)

n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최소화한 군집을 형성하는 것

##### 가. K-평균 군집분석 (k-means clustering)의 개념

- 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작



##### 나. K-평균 군집분석 (k-means clustering) 과정

- 원하는 군집의 개수와 초기 값(seed)들을 정해 seed 중심으로 군집을 형성
- 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류
- 각 군집의 seed 값을 다시 계산
- 모든 개체가 군집으로 할당될 때까지 위 과정들을 반복



##### 다. K-평균 군집분석의 특징

- 거리 계산을 통해 군집화가 이루어지므로 **연속형 변수에 활용이 가능**
- K개의 **초기 중심값은 임의로 선택이 가능**하며 가급적이면 멀리 떨어지는 것이 바람직하다.
- 초기 중심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집 혼합되지 않고 층으로 나누어질 수 있어 주의하여야 한다. **초기 중심값의 선정에 따라 결과가 달라**질 수 있다.
- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 **탐욕적(greedy) 알고리즘**이므로 안정된 군집은 보장하나 최적이라는 보장은 없다.
- 장점
  - 알고리즘이 단순하며, 빠르게 수행되어 분석 방법 적용이 용이
  - 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다.
  - 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다.
  - 다양한 형태의 데이터에 적용이 가능
- 단점
  - 군집의 수, 가중치와 거리 정의가 어렵다.
  - 사전에 주어진 목적이 없으므로 결과 해석이 어렵다.
  - 잡음이나 이상값의 영향을 많이 받는다.
  - 볼록한 형태가 아닌 (non-convex) 군집이 (예를 들어 U형태의 군집) 존재할 경우에는 성능이 떨어진다.
  - 초기 군집수 결정에 어려움이 있다.





#### 5. 혼합 분포 군집 (mixture distribution clustering) (p.431)

##### 가. 개요

- 모형 기반 (model-based)의 군집 방법이며, 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용
- K개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 K개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다.
- 혼합모형에서 모수와 가중치의 추정 (최대가능도추정)에는 **EM알고리즘**이 사용됨



##### 나. 혼합 분포모형으로 설명할 수 있는 데이터 형태

<img width="769" alt="image" src="https://user-images.githubusercontent.com/291782/163675800-e3cf8239-129f-469a-97ec-bc981a843bc3.png">

- (a)는 자료의 분포형태가 다봉형의 형태를 띠므로 단일 분포로의 적합은 적절하지 않으며, 대략 3개 정도의 정규분포 결합을 통해 설명될 수 있을것으로 생각할 수 있다.
- (b)의 경우에도 여러 개의 이변량 정규분포의 결합을 통해 설명될 수 있을 것이다. 두 경우 모두 반드시 정규분포로 제한할 필요는 없다.



##### 다. EM (Expectation-Maximization) 알고리즘의 진행 과정

<img width="942" alt="em-algo" src="https://user-images.githubusercontent.com/291782/163675876-58a4834d-0dbe-49d2-aeae-c2e22e2e963d.png">



##### 라. EM 알고리즘의 진행 과정

1) 혼합분포 군집모형의 특징
   - K-평균군집의 절차와 유사하지만 **확률분포를 도입하여 군집을 수행**한다.
   - 군집을 몇개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다.
   - EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있다.
   - 군집의 크기가 너무 작으며 추정의 정도가 떨어지거나 어려울 수 있다.
   - K-평균군집과 같이 **이상치 자료에 민감**하므로 사전에 조치가 필요하다.





#### 6. SOM (Self Organizing Map) (p.432)

##### 가. 개요

- 자기조직화지도 (SOM) 알고리즘은 코호넨 (Kohonen)에 의해 제시, 개발되었으며 코호넨 맵(kohonen maps)이라고도 알려져 있다.

- **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬**하여 지도의 형태로 형상화 한다.

  <img width="466" alt="image" src="https://user-images.githubusercontent.com/291782/163676024-410958e8-d1fa-46b2-abe2-26a2e42e6fb6.png">



##### 나. 구성

- SOM 모델은 위 그림과 같이 두 개의 인공신경망 층으로 구성되어 있다.
  1. 입력층 (input layer: 입력벡터를 받는 층)
     - **입력 변수의 개수와 동일하게 뉴런 수가 존재**한다.
     - 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도 (map)라 부른다.
     - 입력층에 있는 뉴런은 경쟁층에 있는 뉴런들과 연결되어 있으며, 이때 완전연결 (fully connected)되어 있다.
  2. 경쟁층 (competitive layer : 2차원 격자(grid)로 구성된 층)
     - 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
     - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하며 학습한다.
     - 입력 층의 표본 벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best Matching Unit)라고 하며, 코호넨의 승자 독점의 학습 규칙에 따라 위상학적 이웃 (topological neighbors)에 대한 연결 강도를 조정한다.



##### 다. 특징

- 고차원의 데이터를 저차원의 **지도 형태로 형상화**하기 때문에 시각적으로 이해가 쉽다.
- 실제 데이터가 유사하면 지도상에서 가깝게 표현된다. 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보인다.
- 역전파 (Back Propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스 (feed-forward flow)를 사용함으로써 속도가 매우 빠른다. 실시간 학습처리를 할 수 있는 모형이다.



##### 라. SOM과 신경망 모형의 차이점

|         구분          |          신경망 모형          |                  SOM                  |
| :-------------------: | :---------------------------: | :-----------------------------------: |
|       학습방법        |         오차역전파법          |             경쟁학습방법              |
|         구성          |    입력층, 은닉층, 출력층     | 입력층, 2차원 격자(grid)형태의 경쟁층 |
| 기계 학습 방법의 분류 | 지도학습(Supervised learning) |   비지도학습(Unsupervised learning)   |





#### 7. 최신 군집분석 기법들 (p.434)

##### 가. iris 데이터를 활용한 기법 확인

1. Hierarchical Clustering (R 예제)

   ```R
   > idx <- sample(1:dim(iris)[1], 40)
   > iris.s <- iris[idx,]
   > iris.s$Species <- NULL
   > hc <- hclust(dist(iris.s), method="ave")
   > plot(hc, hang=-1, labels=iris$Species[idx])
   ```

   ![iris-dendrogram](https://user-images.githubusercontent.com/291782/163717239-dfae4a63-53d9-4f32-839a-557368465b31.png)

   

2. K-means Clustering (R 예제) 꼭 해봐라

   -   비계층적 군집방법으로 사용가능

   2-1 군집화

   ```R
   > # k-means
   > # 군집화 
   > data(iris)
   > newiris <- iris
   > newiris$Species <- NULL
   > kc <- kmeans(newiris, 3)
   > # 결과비교 
   > table(iris$Species, kc$cluster)
               
                 1  2  3
     setosa     50  0  0
     versicolor  0  2 48
     virginica   0 36 14
   > # 군집화 그래프
   > plot(newiris[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
   ```

   ![k-means-cluster](https://user-images.githubusercontent.com/291782/163717403-9c05da80-59ce-476a-931f-078491605354.png)

   




### 6절 연관분석

연관분석의 개념, 측도와 장단점을 완벽히 학습해야 함

#### 1. 연관규칙 (p.437)

##### 가. 연관규칙분석(Association analysis)의 개념

- 연관성 분석은 흔히 장바구니분석(market basket analysis) 또는 서열분석 (sequence analysis) 이라고 불린다.
- 기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용
- 장바구니 분석 : 장바구니에 무엇이 같이 들어 있는지에 대한 분석
- 서열분석 : A를 산 다음에 B를 산다.



##### 나. 연관규칙의 형태

- 조건과 반응의 형태 (if-then)로 이루어져 있다. (if A then B: 만일 A가 일어나면 B가 일어난다.)



##### 다. 연관규칙의 측도

- 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 한다.

1. 지지도 (support)

   - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의
   - 지지도 = $P(A \cap B) = \dfrac {A와 B가 동시에 포함된 거래수} {전체 거래수} = \dfrac {A \cap B} {전체}$

2. 신뢰도 (confidence)

   - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률. 연관성의 정도를 파악 가능
   - 신뢰도 = $\dfrac {P(A \cap B)} {P(A)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수} = \dfrac {지지도} {P(A)}$

3. 향상도 (Lift)

   - A가 구매되지 않았을 떄 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가 비이다. 연관규칙 A&rarr;B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다.

   - 향상도 = $\dfrac {P(B|A)} {P(B)} = \dfrac {P(A \cap B)} {P(A)P(B)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수 \; X \; B를 포함하는 거래수} = \dfrac {신뢰도} {P(B)}$ 

     <img width="935" alt="support-confidence-lift" src="https://user-images.githubusercontent.com/291782/163676755-3b1a3876-f2d0-4ce5-b748-1bf5f4038e19.png">



##### 라. 연관규칙의 절차

- 최소 지지도 결정 > 품목 중 최소 지지도를 넘는 품목 분류 > 2가지 품목 집합 생성 > 반복적으로 수행해 빈발품목 집합을 찾음



##### 마. 연관규칙의 장점과 단점

- 장점
  - 탐색적인 기법으로 조건 반응으로 표현되는 연관성 분석의 결과를 쉽게 이해할 수 있다.
  - 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없으므로 유용하게 활용 된다.
  - 사용이 편리한 분석 데이터의 형태로 거래 내용에 대한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 갖는다.
  - 분석을 위한 계산이 간단한다.
- 단점 (개선방안)
  - 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어난다.
  - 이를 개선하기 위해 유사한 품목을 한 범주로 일반화 한다.
  - 너무 세분화한 품목을 갖고 연관성 규칙을 찾으면 의미없는 분석이 될 수도 있다.



##### 바. 순차패턴 (sequence analysis)

- 동시에 구매될 가능성이 큰 상품군을 찾아내는 연관성분석에 시간이라는 개념을 포함시켜 순차적으로 구매 가능성이 큰 상품군을 찾아내는 것



#### 2. 기존 연관성분석의 이슈 (p.439)

- 대용량 데이터에 대한 연관성분석이 불가능
- 시간이 많이 걸리거나 실행 시 다운되는 현상이 발생할 수 있다.





#### 4. 연관성분석 활용방안 (p.440)

- 장바구니 분석의 경우 실시간 상품추천을 통한 교차판매에 응용
- 순차패턴 분석은 A를 구매한 사람에게 B를 구매하지 않는 경우, B를 추천하는 교차판매 캠페인에 사용



#### 5. 연관성분석 예제 (p.441)

- 분석내용 : Groceries 데이터셋은 식료품 판매점의 1달 동안의 POS 데이터이며, 총 169개의 제품과 9835건의 거래건수를 포함하고 있다. 거래내역을 **inspect** 함수로 확인할 수 있으며, **apriori** 함수로 최소지지도와 신뢰도는 각각 0.01, 0.3으로 설정한 뒤 연관규칙분석을 실시했다.

- ```R
  > # 연관성분석
  > # Groceries 데이터셋
  > #install.packages("arules") # Groceries 데이터셋을 위한 패키지 설치
  > library(arules)
  > # 분석결과
  > data(Groceries)
  > inspect(Groceries[1:3])
      items                                                      
  [1] {citrus fruit, semi-finished bread, margarine, ready soups}
  [2] {tropical fruit, yogurt, coffee}                           
  [3] {whole milk}
  
  
  > apriori(Groceries, parameter = list (support = 0.01, confidence=0.3))
  Apriori
  
  Parameter specification:
   confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext
          0.3    0.1    1 none FALSE            TRUE       5    0.01      1     10  rules TRUE
  
  Algorithmic control:
   filter tree heap memopt load sort verbose
      0.1 TRUE TRUE  FALSE TRUE    2    TRUE
  
  Absolute minimum support count: 98 
  
  set item appearances ...[0 item(s)] done [0.00s].
  set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
  sorting and recoding items ... [88 item(s)] done [0.00s].
  creating transaction tree ... done [0.00s].
  checking subsets of size 1 2 3 4 done [0.00s].
  writing ... [125 rule(s)] done [0.00s].
  creating S4 object  ... done [0.00s].
  set of 125 rules 
  
  
  > inspect(sort(rules, by=c("lift"), decreasing=TRUE)[1:20])
       lhs                                       rhs                support    confidence coverage   lift     count
  [1]  {citrus fruit, other vegetables}       => {root vegetables}  0.01037112 0.3591549  0.02887646 3.295045 102  
  [2]  {tropical fruit, other vegetables}     => {root vegetables}  0.01230300 0.3427762  0.03589222 3.144780 121  
  [3]  {beef}                                 => {root vegetables}  0.01738688 0.3313953  0.05246568 3.040367 171  
  [4]  {citrus fruit, root vegetables}        => {other vegetables} 0.01037112 0.5862069  0.01769192 3.029608 102  
  [5]  {tropical fruit, root vegetables}      => {other vegetables} 0.01230300 0.5845411  0.02104728 3.020999 121  
  [6]  {other vegetables, whole milk}         => {root vegetables}  0.02318251 0.3097826  0.07483477 2.842082 228  
  [7]  {whole milk, curd}                     => {yogurt}           0.01006609 0.3852140  0.02613116 2.761356  99  
  [8]  {root vegetables, rolls/buns}          => {other vegetables} 0.01220132 0.5020921  0.02430097 2.594890 120  
  [9]  {root vegetables, yogurt}              => {other vegetables} 0.01291307 0.5000000  0.02582613 2.584078 127  
  [10] {tropical fruit, whole milk}           => {yogurt}           0.01514997 0.3581731  0.04229792 2.567516 149  
  [11] {yogurt, whipped/sour cream}           => {other vegetables} 0.01016777 0.4901961  0.02074225 2.533410 100  
  [12] {other vegetables, whipped/sour cream} => {yogurt}           0.01016777 0.3521127  0.02887646 2.524073 100  
  [13] {tropical fruit, other vegetables}     => {yogurt}           0.01230300 0.3427762  0.03589222 2.457146 121  
  [14] {root vegetables, whole milk}          => {other vegetables} 0.02318251 0.4740125  0.04890696 2.449770 228  
  [15] {whole milk, whipped/sour cream}       => {yogurt}           0.01087951 0.3375394  0.03223183 2.419607 107  
  [16] {citrus fruit, whole milk}             => {yogurt}           0.01026945 0.3366667  0.03050330 2.413350 101  
  [17] {onions}                               => {other vegetables} 0.01423488 0.4590164  0.03101169 2.372268 140  
  [18] {pork, whole milk}                     => {other vegetables} 0.01016777 0.4587156  0.02216573 2.370714 100  
  [19] {whole milk, whipped/sour cream}       => {other vegetables} 0.01464159 0.4542587  0.03223183 2.347679 144  
  [20] {curd}                                 => {yogurt}           0.01728521 0.3244275  0.05327911 2.325615 170  
  >
  ```





     









ADsP 5절 데이터 변형

## ADsP 데이터 분석 END

t분포 f분포

p.112 나이브베이즈 분류a
다차원척도법
의사결정나무 R코드
군집분석 유클리디언거리
연관분석

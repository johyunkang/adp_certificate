## 4과목 데이터 분석 (40)

## ADP 데이터 분석 START

### 1장 통계분석

### 1절 통계분석

#### 1. 확률 분포 (p.66)

##### 가. 이산형 확률분포

- 확률 변수가 가질 수 있는 값이 명확하고 셀 수 있는 경우의 분포, 확률값은 확률질량함수를 이용하여 계산

> $P(X_i) > 0$      $  i=1,2,...,k$      $\displaystyle \sum_{i=1}^kP(X_i)=1$

-   이산형 확류변수 예시 : 동전 2개를 던져서 앞/뒷면이 나오는 경우의 수(H:앞, T:뒤)

    | 표본공간(&Omega;) | HH(사건) | HT   | TH   | TT   | 합계 |
    | ----------------- | -------- | ---- | ---- | ---- | ---- |
    | P(x)              | 1/4      | 1/4  | 1/4  | 1/4  | 1    |

    

###### 1) 베르누이  확률분포(Bernoulli distribution)

- 결과가 2개만 나오는 경우 (ex. 동전 던지기, 시험의 합/불합격 등)

> $P(X=x)=p^x(1-p)^{1-x}$  (x=1 or 0), E(x) = p,  var(x)=p(1-p)
>
> 예) 메이저리거인 추추가 안타를 칠 확률은 베르누이 분포를 따름. (안타를 치는 사건을 x=1이라고 할 때 안타를 칠 확률은 타율로 적용 가능)



###### 2) 이항분포(Binomial distribution)

- 베르누이 시행을 n번 반복했을 때 k번 성공할 확률
- $P(X = k) = _nC_kP^k(1-p)^{n-k}, \quad  _nC_k = \dfrac {n!} {k!(n-k)!}$
- 한 축구 선수가 페널티킥을 차면 5번 중 4번은 성공한다고 한다. 그럼 이 선수가 10번의 페널티킥을 차서 7번 성공할 확률은?
- 5번 중 4번 성공하기에 성공확률은 4/5 = 0.8, 실패확률은 1-0.8 = 0.2
- $이항분포 = \begin{pmatrix} n \\ x \end{pmatrix} p^x(1-p)^{n-x} =  \begin{pmatrix} 10 \\ 7 \end{pmatrix} 0.8^70.2^3$
- $ = \dfrac {10!} {7! \times 3!} 0.8^7 0.2^3 = 0.2013 = 20.13\%$



###### 3) 기하분포(Geometric distribution)

- 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- $p(x) = p(1-p)^{x-1}$
- 어느 야구선수가 홈런을 칠 확률은 0.05라고한다. 이 선수가 6번째 타석에서 홈런을 칠 확률은?
- 성공확률 p=0.05, 실패확률은 1 - 0.05 = 0.95, 6번째 타석에서 성공할 확률이기에 x-1 = 6-1
- $p(1 - p)^{x-1} = 0.05 \times 0.95^{6-1} = 0.0387 = 3.87\%$



###### 4) 다항분포(Multinomial distribution)

- 이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포

- 각 상황의 확률과 각 상황의 횟수를 잘 파악해야 함

- $p(x) = \dfrac {n!} {x_1!x_2! ...x_k!}p1^{x_1}p_2^{x_2}... p_k^{x_k} $

- 국내 인터넷 포털 사이트의 점유율은 아래와 같다.  12명을 임의로 뽑아 사용 사이트를 알아보았을 때, 네이버 7명, 구글 3명, 다음과 ZUM이 각 1명, 기타는 0명이 사용할 확률을 구하시오

    네이버 : 61%, 구글 : 30%, 다음 : 7%, ZUM: 1%, 기타 : 1%

- $\dfrac {12!} {7! \times 3! \times 1! \times 1! \times 0!} \times 0.61^7 \times 0.3^3 \times 0.07^1 \times 0.01^1 \times 0.01^0 = 0.0094$

    

    

###### 5) 포아송분포 (Poisson distribution)

- 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 (예. 가게에 손님이 1시간에 20명씩 방문한다고 할 때, 10분에 손님이 5명씩 방문할 확률)
- 확률을 구하기 위해서는 **평균(&lambda;)과 발생횟수(x)**를 잘 파악해야 함
- $p(x) = \dfrac {e^{-\lambda} \lambda^x} {x!} , \quad e=2.718281...$
- 전공 책 5페이지를 검사했는데, 오타가 총 10개가 발견되었다고 한다. 그럼 이 책에서 어느 한 페이지를 검사하였을 때, 오타가 3개 나올 확률을 구하시오?
- 해설) 포아송 분포는 평균을 잘 구해야함. 문제에 말장난이 섞여 있음. 일단 5페이지에 총 10개의 오타이므로, 1페이지에 평균 2개의 오타가 발견된 셈. 그래서 평균 (&lambda;)=2 이다. 그리고 발생횟수 x=3 이므로...
- $\dfrac {2.718281^{-2} \times 2^3} {3!} = 0.1804$




##### 나. 연속형 확률분포

- 확률 변수가 가질 수 있는 값이 연속적인 실수여서 셀 수 없는 경우의 분포이며, 확률값은 확률밀도함수를 이용하여 계산한다.

###### 1) 균일분포 (일양분포, Uniform distribution)

- 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
- $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
- ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)



###### 2) 정규분포 (Normal distribution)

- 평균이 &mu;이고, 표준편차가 &sigma;인 X의 확률밀도함수
- 표준편차가 클 경우 퍼져보이는 그래프가 나타난다.
- 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
- 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
- ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)



###### 3) 지수분포 (Exponential distribution)

- 어떤 사건이 발생할 때까지 경과한 시간에 대한 연속확률분포이다. (예. 전자렌지의 수명시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간)
- ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)



###### 4) t-분포 (t-distiribution)

- 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
- 표본의 크기가 적을때는 표준정규분포를 위에서 눌러 놓은 것과 같은 형태를 보이지만 표본이 커져서(30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
- 데이터가 연속형일 경우 활용한다.
- **두 집단의 평균이 동일**한지 알고자 할 때 검정통계량으로 활용된다.
- 표준정규분포와 같이 평균 값이 0이며, 자유도에 따라 분포의 모양이 변화한다.
- 자유도가 30미만인 경우, 표준정규분포에 비해 양쪽 끝이 평평하고 두터운 꼬리 모양을 가진다.
- ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)



###### 5) X<sup>2</sup>-분포 (X<sup>2</sup>-distribution) (카이제곱분포)

- 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포이다.
- **두 집단 간의 동질성 검정에 활용**된다.
- 확률변수 X가 표준정규분포(Z)를 따를 때, 자유도가 k인 카이제곱분포를 따른다. 자유도는 표본 자료 중 모집단에 대한 정보를 주는 독립적인 표본 자료의 수와 같으며, 분할표에서의 행과 열의 개수를 통해 구할 수 있다. (자유도(df) = (r-1)(c-1), r=행의 개수, c=열의 개수)
- ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)



###### 6) F-분포 (F-distribution)

- **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량의 분포이다.
- 확률변수는 항상 양의 값만을 갖고 카이제곱분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
- ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)



#### 2. t-검정 (t-test) (p.72)

##### 가. 일표본 t-검정 (one sample t-test)

###### 1) 일표본 t-검정이란?

- **단일모집단에서 관심이 있는 연속형 변수의 평균(&mu;)값을 특정 기준값과 비교**하고자 할 때 사용하는 검정방법이다.

###### 2) 일표본 t-검정의 가정

- 일표본 t-검정에서는 **모집단의 구성요소들이 정규분포를 이룬다는 가정** 하에 검정통계량의 값을 계산한다.
- **종속형 변수는 연속형** 변수여야 하며, **검증하고자 하는 기준값이 있어야** 한다.

###### 3) 일표본 t-검정의 단계

- 1단계 : 가설 설정

- 2단계 : 유의수준 설정

- 3단계 : 검정통계량의 값 및 유의확률 계산

- 4단계 : 기각여부 판단 및 의사결정

    ​    

###### 4) R을 활용한 일표본 t-검정

##### 나. 대응표본 t-검정 (paired sample t-test)

###### 1) 대응표본 t-검정이란?

- **단일모집단에 대해 두 번의 처리를 가했을 때, 두 개의 처리에 따른 평균의 차이를 비교**하고자 할 때 사용하는 검정방법.

###### 2) 대응표본 t-검정의 가정

- **대응표본 t-검정에서는 모집단의 관측값이 정규성(정규분포를 만족한다는 가정)을 만족**해야 한다.

###### 3) 대응표본 t-검정의 단계

- 1단계 : 가설 설정
- 2단계 : 유의수준 설정
- 3단계 : 검정통계량의 값 및 유의확률 계산
- 4단계 : 기각여부 판단 및 의사결정

###### 4) R을 활용한 대응표본 t-검정

##### 다. 독립표본 t-검정 (independent sample t-test)

###### 1) 독립표본 t-검정이란?

- **두 개의 독립된 모집단의 평균을 비교 할때 사용하는 검정방법**

###### 2) 독립표본 t-검정의 가정

- 두 모집단은 **정규성**을 만족해야 함.
- 두 모집단은 서로 **독립적**이어야 함
- **등분산성** 가정을 만족해야 함
- 독립변수는 범주형, 종속변수는 연속형이어야 함

> 등분산성 : 두 모집단의 분산이 서로 같음을 의미

###### 3) 독립표본 t-검정의 단계

- 1단계 : 가설설정
- 2단계 : 유의수준 설정
- 3단계 : 등분산 검정
- 4단계 : 검정통계량의 값 및 유의확률 계산

###### 4) R을 활용한 독립표본 t-검정



#### 3. 분산분석 (ANOVA) (p.80)

##### 가. 분삭분석의 개념

-   두 개 이상 집단들의 평균 간 차이에 대한 통계적 유의성을 검증하는 방법

##### 나. 일원배치 분석 (One-way ANOVA)

###### 1) 특징

-   하나의 범주형 변수의 영향을 알아보기 위해 사용되는 검증 방법
-   모집단의 수에 제한이 없음
-   F 검정 통계량을 이용한다

###### 2) 가정

-   각 집단의 측정치는 서로 독립적이며, 정규분포를 따른다. (정규성 가정)
-   각 집단 측정치의 분산은 같다. (등분산 가정)

###### 3) 통계적 모형

###### 4) 분산분석표

###### 5) 가설검정

-   귀무가설(H<sub>0</sub>) : 집단 간 모평균에는 차이가 없다.
-   대립가설(H<sub>1</sub>) : 집단 간 모평균이 모두 같다고 할 수 없다.

###### 6) 사후 검정

-   귀무가설이 기각되었을 때, 어떤 집단들에 대해서 평균의 차이가 존재하는지를 알아보기 위해 실시하는 분석
-   종류 : 던칸(Duncan)의 MRT(Multiple Range Test) 방법, 피셔(Fisher)의 최소유의차(LSD; Least S... Difference)방법, 튜키(Tukey)의 HSD방법, Scheffe의 방법 등이 있다.

###### 7) R을 이용한 일원배치 분산 분석

##### 다.  이원배치 분산분석 (Two-way ANOVA)

###### 1) 특징

-   두 개의 범주형 변수 A, B의 영향을 알아보기 위해 사용되는 검증 방법

###### 2) 가정

-   정규성, 등분산성

###### 3) 통계적 모형

###### 4) 분산분석표

###### 5) 가설검정

-   귀무가설(H<sub>0</sub>) : 변수에 따른 종속변수의 값(반응값)에는 차이가 없다. &alpha; 와 &beta; 변수의 상호작용 효과가 없다.
-   대립가설 (H<sub>1</sub>) : 변수에 따른 종속변수의 값(반응값)에는 차이가 있다.&alpha; 와 &beta; 변수의 상호작용 효과가 있다.

###### 6) 교호작용 (Interaction Effection)

-   두 가지 이상의 특정 변수 조합에서 일어나는 효과

##### 라. 실험계획법 (DOE, Design Of Experiment)

###### 1) 실험 계획법의 개념

-   시스템이나 프로세스의 결과에 영향을 미치는 인자를 도출하고, 측정 데이터를 통계적으로 분석하기 위한 실험을 설계하는 방법을 의미한다.

###### 2) 계획 설계의 목적

###### 3) 실험계획의 원리

-   랜덤화의 원리 (Randomization) :
-   반복의 원리 (Replication) :
-   블록화의 원리(Blocking) : 
-   직교화의 원리 (Orthogonality) :
-   교락의 원리 (Confounding) : 

###### 4) 주요 용어

-   인자 (Factor) :
-   특성치 (Characteristic Value) :
-   수준 (Level) :
-   주효과 (Main Effect) :
-   교호효과 (Interaction Effect) :
-   교락 (Confounding) :
-   블록 (Block) :
-   반복 (Replication) :
-   중복 (Repetition) :

###### 5) 실험계획법의 종류

1.   요인배치법 (Factorial Design)
2.   분할법 (Split-plot Design)
3.   교락법 (Confounding method)
4.   난괴법 (Randomized Block Design)

#### 4. 교차분석 (p.87)

##### 가. 교차분석 (검정)

###### 1) 교차분석의 개념 및 특징

-   범주형 자료인 두 변수 간의 관계를 알아보기 위해 실시하는 분석 기법
-   적합도 검정, 독립성 검정, 동질성 검정에 사용
-   카이제곱($x^2$) 검정 통계량을 이용

###### 2) 교차표

##### 나. 적합도 검정

###### 1) 적합도 검정의 의미

-   모집단 분포에 대한 가정이 옳게 되었는지를 관측 자료와 비교하여 검정하는 것

###### 2) 가설 설정

###### 3) 검정 통계량

###### 4) 자유도

-   $df = k - 1$

###### 5) R을 이용한 적합도 검정

-   chisq.test(x, y, p)

##### 다. 독립성 검정

###### 1) 독립성 검정의 의미

-   범주화 된 두 변수 A, B 사이의 관계가 독립인지 아닌지를 검정
-   검정 통계량을 계산할 때는 교차표를 활용

###### 2) 가설 설정

-   두 변수 A, B가 서로 독립적으로 관측값에 영향을 미치는 지의 여부를 검정
-   귀무가설 : 두 변수 사이에 연관이 없다. (독립이다)
-   대립가설 : 두 변수 사이에 연관이 있다. (종속이다)

###### 3) 검정 통계량

###### 4) 자유도

-   R : 행의 수, C: 열의 수
-   $df = (R-1)(C-1)$

##### 라. 동질성 검정

###### 1) 동질성 검정의 의미

-   교차표를 활용하며, 계산법과 검증법은 모두 독립성 검정과 같은 방법으로 진행

###### 2) 가설 설정

-   j = 1, 2, ..., c 이다.
-   귀무가설 : P<sub>1j</sub> = P<sub>2j</sub> = ... = P<sub>rj</sub> (모든 P<sub>nj</sub> (n=1,2,...r)는 동일하다)
-   대립가설 : Not H<sub>0</sub> (P<sub>nj</sub> (n=1,2,...r) 중 다른 값이 하나 이상 존재한다.)

###### 3) 검정 통계량

###### 4) 자유도

-   $df = (R-1)(C-1)$   R: 행의 수, C: 열의 수

#### 5. 중심극한정리 (Central Limit Theorem)

##### 가. 개념

-   표본의 개수 n이 커질수록 표본 평균의 분포(표집분포)가 정규분포에 가까워지는 현상을 의미
-   평균이 &mu;이고, 분산이 &sigma;<sup>2</sup> 인 모집단에서 크기가 n인 확률표본을 추출.

##### 나. 중심극한정리

-   $N(\mu, \dfrac{\sigma^2}{n})$
-   $Z = \dfrac {\overline {X} - \mu}{\dfrac {\sigma}{\sqrt{n}}}$

### 2절 회귀분석 

#### 1. 정규화 선형회귀 (Regularized Linear Regression) (p.92)

-   선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현산(과적합, Overfitting)을 막는 방법
-   계수의 크기를 제한하는 방법으로 제약조건을 추가한다.
-   제약 조건의 종류에 따라 Ridge회귀, LASSO회귀, Elastic Net 회귀모형

##### 가. 릿지회귀 (Ridge Regression)

-   가중치들의 제곱합(Squared Sum of weight)을 최소화하는 것을 제약조건으로 추가하는 기법
-   가중치의 모든 원소가 0에 가까워지는 것을 원하며, 규제 방식을 L2 규제(Penalty)라고 한다.
-   &lambda;는 제약조건의 비중을 조절하기 위한 하이퍼 모수(Hyper parameter)에 해당하며, 람다가 커지면 가중치의 값들이 작아지며, 정규화 정도가 커진다. 람다가 작아지면 정규화 정도가 작아지고, 람다가 0이 되면 일반적인 선형회귀 모형이 된다.

##### 나. 라쏘회귀 (LASSO Regression)

-   라쏘 (Least Absolute Shrinkage and Selection Operator)는 가중치 절대값의 합을 최소화하는 것을 제약조건으로 추가하는 기법
-   가중치가 0에 가까워질 뿐, 실제 0이 되지는 않는다. 하지만 중요하지 않은 가중치는 0이 될 수도 있다.
-   라쏘에서 사용하는 규제방식을 L1 규제라고 한다.

##### 다. 엘라스틱넷(Elastic Net)

-   릿지와 라쏘를 결합한 모델
-   &lambda;<sub>1</sub> 와 &lambda;<sub>2</sub> 두 개의 하이퍼 모수를 가짐

#### 2. 일반화 선형회귀 (GLM, Generalized Linear Regression) (p.93)

-   종속변수를 적절한 함수로 변화시켜 f(x)를 정의한 후, 이 f(x)와 독립변수를 선형 결합으로 모형화하는 '일반화 선형모형(glm)'을 이용한다.
-   일반화 선형모형은 3가지 성분에 의해서 정의 된다.
    -   랜덤성분 (Random Component)
    -   체계적 성분 (systematic component)
    -   연결함수 (link function)

#### 3. 회귀분석의 영향력 진단 (p.94)

-   영향력 진단이란 적합된 회귀모형의 안전성을 평가하는 통계적인 방법이다.
-   회귀직선의 기울기에 영향을 크게 주는 점을 ==영향점==이라고 한다.

#### 4. 더빈 왓슨(Durbin Watson) 검정

-   오차항이 독립성을 만족하는지를 검정하기 위해 사용
-   더빈 왓슨 동계량이 2에 가까울수록 오차항의 자기상관이 없음을 의미
-   0에 가까울수록 양의 상관관계, 4에 가까울수록 음의 상관관계가 있음을 의미

#### 5. 벌점화된 선택기준 : 변수 선택의 기준으로 사용되는 통계량

##### 가. 수정된 결정계수 (Adjuestd R square)

##### 나. Mallow's CP

#### 6. 변수 변환 (p.96)

-   회귀분석의 기본가정인 ==정규성, 선형성, 등분산성== 가정을 만족하지 못하는 경우, 변수를 변환함으로써 교정할 수 있다.

##### 가. 변수변환법의 종류

-   로그변환, 제곱근변환 : 대부분의 값이 작은 값으로 구성되어 있는 데이터를 정규화하기 위해 사용
-   지수변환, 제곱변환 : 대부분의 값이 큰 값으로 구성되어 있는 데이터를 정규화하기 위해 사용

##### 나. 더미변수 생성

-   더미변수는 변수의 범주의 수 -1 개 이다.
-   ![dummy](https://user-images.githubusercontent.com/291782/167261233-9cc99675-dae4-4789-8fb7-90fbe525af71.png)

##### 다. Box-cox 변환

-   정규분포를 따르지 않는 반응변수를 정규성을 만족하도록 변환하기 위해 사용하는 방법
-   &lambda;는 우도함수를 최대화 시키는 조건으로 계산된다.

### 2장 정형 데이터마이닝

### 1절 데이터마이닝 개요

#### 1. Feature Selection (변수선택) (p.108)

##### 가. Filter Method

- 각 변수들에 대해 통계적인 점수를 부여 후, 부여된 점수를 이용하여 변수의 순위를 매기고 변수 선택을 진행
- 예) Chi squared test, information gain, correlation coefficient scores 등

##### 나. Wrapper Method

- 변수간의 상호 작용을 감지할 수 있도록 **변수의 일부만을 모델링에 사용한 후 그 결과를 평가하는 작업을 반복**하면서 변수를 선택해 나가는 방법
- 예) recursive feature elimination algorithm

##### 다.  Embedded Method

- Filter method 와 wrapper method를 결합하여 어떤 변수가 가장 크게 기여하는지를 찾아내는 방법으로 **과적합을 줄이기 위해 내부적으로 규제를 가하는 방식**이 사용된다.
- 예) LASSO, Ridge Regression, Elastic Net 등

#### 2. DeepLearning

##### 가. 머신러닝

- 인공지능에 포함되는 개념으로, 경험적인 데이터를 바탕으로 기계가 지식을 습득하여 스스로 성능을 향상하는 기술을 의미
- 학습 방법에 따라 구분
    - 지도학습 (Supervised learning) : 출력값에 대한 정답을 컴퓨터에게 알려줌
    - 비지도학습 (Unsupervised learning) : 출력값에 대한 정답을 컴퓨터에게 안 알려줌
    - 강화학습 (Reinforcement learning) : 출력값의 정답이 주어지지 않은 상태에서 일련의 행동 결과에 대한 보상(reward)이 주어짐

##### 나. 딥러닝

- **인공신경망에 기반을 둔 기계학습**의 한 종류로, 여러 비선형 변환기법의 조합을 통해 많은 데이터로 부터 특징들을 학습하는 방법
- 종류
    - DNN (Deep Neural Network, 심층 신경망) : 입력층과 출력층 사이에 여러개의 은닉층들로 이루어진 신경망 구조
    - 예) 암 진단 시스템, 주가지수 예측, 기업신용평가, 환율 예측 등
    - CNN (Convolutional Neural Network, 합성곱 신경망) : 다계층 퍼셉트론의 한 종류로 여러개의 합성곱 계층과 일반적인 인공 신경망 계층들로 이루어져 있음.
    - 예) 자율 주행 자동차, 이미지, 텍스트, 사운드, 비디오 인식 및 식별 등의 영상, 그림 인식 분야 등
    - RNN (Recurrent Neural Network, 순환 신경망) : 시간의 흐름에 따라 변화하는 데이터를 학습하기 위한 딥러닝 알고리즘으로 기준 시점과 다음 시점에 네트워크를 연결하여 구성한 인공신경망이라고 할 수 있다.
    - 예) 음성 인식, 자동 번역, 단어 의미 판단, 이미지 캡션 생성 등의 자연어 처리 분야 등

##### 다. 프로그래밍 언어별 딥러닝 지원 라이브러리

1) 파이썬
2) C++
3) JAVA
4) R



### 2절 분류분석

#### 1. 나이브 베이즈 분류 (Naive Bayes Classification) (p. 112)

##### 가. Bayes theorem (베이즈 정리)

- **두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리**

- 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률을 다음과 같이 구할 수 있는것

- >  $P(A|B) = \dfrac{P(B\cap A)}{P(B)} = \dfrac {P(A)P(B|A)} {P(B)} = \dfrac {P(A)P(B|A)} {P(A)P(B|A) + P(A^C)P(B|A^C)}$

- P(A|B) : 사건 B가 발생했을 때 사건 A가 발생할 확률 > 사후확률 (posterior)

- P(B|A) : 사건 A가 발생했을 때 사건 B가 발생할 확률 > 우도 (likelihood)

- $P(A \cap B)$ : 사건 A와 B가 동시에 발생할 확률

- P(A) : 사건 A가 발생할 확률 > 사전확률 (prior)

- P(B) : 사건 B가 발생할 확률 > 관찰값 (evidence)

- 위 식을 다음과 같은 식으로도 표현 가능

- > $posterior = \dfrac {prior * likelihood} {evidence}$

![naive-bayes](https://user-images.githubusercontent.com/291782/149884588-2373a115-d8ea-400f-b5e7-66a7ae7cf3c5.png)

##### 나. 나이브 베이즈 분류의 개념

- 변수들에 대한 조건부 독립을 가정하는 알고리즘으로, 어떤 데이터가 특정 클래스에 속하는지를 분류하는 알고리즘이다.
- 문서를 여러 범주(예, 스팸, 경제, 스포츠) 중 하나로 판단하는 문제에 대한 솔루션으로 사용 가능

##### 다. 나이브 베이즈 분류의 계산

- 속성값에 대해 다른 속성이 독립적이라는 가정은 **클래스 조건 독립성 (Class conditional independence)**라 한다.

#### 2. K-Kearest Neighbor Classification (KNN, K-최근접 이웃 알고리즘) (p.114)

- 특정 범주로 나뉘어진 데이터가 있을 때, 새로운 데이터가 추가 된다면 어떤 범주로 분류할 것인지를 결정할 때 사용할 수 있는 분류 알고리즘. 지도학습의 한 종류

##### 가. KNN 알고리즘의 원리

- 새로운 데이터의 클래스를 해당 데이터와 가장 가까운 k개 데이터들의 클래스(범주)로 결정한다.
- 이웃간의 거리를 계산할 때 **유클리디안 거리**(대표적 사용), 맨하탄 거리, 민코우스키 거리 등을 사용

##### 나. k의 선택

- k의 선택은 학습의 난이도와 데이터의 개수에 따라 결정될 수 있으며, 일반적으로는 훈련 데이터 개수의 제곱근으로 설정.

##### 다. KNN 분류 예시

##### 라. KNN의 장단점

- 장점
    - 사용이 간단
    - 범주를 나눈 기준을 알지 못해도 데이터를 분류할 수 있다.
    - 추가된 데이터의 처리가 용이하다
- 단점
    - k값의 결정이 어렵다.
    - 비수치 데이터의 경우 유사도를 정의하기 어렵다.
    - 데이터 내에 이상치가 존재하면 성능에 큰 영향을 받는다.

#### 3. SVM (Support Vector Machine) (p.116)

##### 가. 개념

- 기계학습 분야 중 하나로 패턴인식, 자료 분석 등을 위한 지도학습 모델이며 주로 회귀와 분류 문제 해결에 사용된다.
- 주어진 데이터 집합을 바탕으로 하여 새로운 데이터가 어떤 범주에 속할 것인지를 판단하는 **비확률적 이진 선형 분류 모델을 생성**한다.

##### 나. 작동 원리

- 데이터가 사상된 공간에서 경계로 표현되며, 공간상에 존재하는 여러 경계 중 가장 큰 폭을 가진 경계를 찾는다.
- 각 그룹을 구분하는 분류자를 결정 초평면(decision hyperline)
- 초평면에 가장 가까이에 붙어있는 최전방 데이터들을 서포트 벡터(support vector)
- 서포트 벡터와 초평면 사이의 **수직거리를 마진**(margin)이라 한다.

![svm-desc](https://user-images.githubusercontent.com/291782/149886433-8723750d-fda3-4528-bba4-4a5a1858b112.png)

> 초평면은 어떤 n차원의 공간보다 한 차원이 낮은 n-1차원의 하위공간(sub space)를 의미함. 즉 2차원 공간에서 초평면은 선이 된다.

##### 다. SVM의 장단점

- 장점
    - 분류와 예측에 모두 사용 가능
    - 신경망 기법에 비해 과적합 정도가 낮다.
    - 예측의 정확도가 높다.
    - 저차원과 고차원의 데이터에 대해서 모두 잘 작동한다.
- 단점
    - 데이터 전처리와 매개변수 설정에 따라 정확도가 달라질 수 있다.
    - 예측이 어떻게 이루어지는지에 대한 이해와 모델에 대한 해석이 어렵다.
    - 대용량 데이터에 대한 모형 구축 시 속도가 느리며, 메모리 할당량이 크다.



### 3절 군집분석

#### 1. Resampling (재표본추출) (p.118)

- 표본을 수많이 재추출하고, 재추출된 표본에 모형을 적합하게 함으로써 생성된 분류기의 성능 측정에 대한 통계적 신뢰도를 높이는 방식이 리샘플링 기법이다. 대표적으로 k-fold cross validation, 홀드아웃 방법, 붓스트랩 등이 있다.

##### 가. K-Fold cross validation

- 데이터를 k개의 집단으로 나눈 뒤 k-1개의 집단으로 분류기를 학습 시키고, 나머지 1개의 집단으로 분류기의 성능을 테스트
- 위 과정을 k번 반복하여 모든 데이터가 학습과 검증에 사용될 수 있도록 하고, 최종적으로 k번의 테스트를 통해 얻은 MSE (평균제곱오차)값들의 평균을 해당 모델의 MSE 값으로 사용

##### 나. 붓스트랩 (bootstrap)

- 모집단에서 추출한 표본(샘플)에 대해서 또 다시 재표본(샘플)을 여러 번 추출하여 모델을 평가하거나 데이터의 분포를 파악하는 재표본추출 방법
- **단순랜덤 복원추출법**을 사용하여 표본을 여러개 생성하므로, 특정 데이터가 샘플에 포함될수도 있고, 안될수도 있다.
- 샘플에 한 번도 선택되지 않는 데이터가 발생할 확률은 36.8%이며, 이러한 데이터를 OOB (out-of-bag) 데이터라고 하며, OOB데이터의 실제값과 예측값 사이의 오차로 정의되는 값을 OOB error라고 한다.

#### 2. 군집화 기법 종류 (p.120)

##### 가. 밀도기반 군집분석

##### 나. 격자기반 군집분석



#### 3. 군집분석의 타당성 지표 (p.121)

##### 가. Silhouette (실루엣)

- 군집내의 응집도와 군집 간 분리도를 이용한 지표로, 군집 내 요소간의 거리가 짧고 서로 다른 군집간 거리가 멀수록 커진다.
- 완벽한 군집화가 이루어졌을 경우 1, 군집화가 전혀 이루어지지 않은 경우에는 -1 값을 가진다.

##### 나. Dunn Index

- 군집 간 거리의 최소값을 분자, 군집 내 요소 간 거리의 최대값을 분모로 하는 지표
- 군집 간 거리는 멀고, 군집 내 분산은 작을수록 군집화가 잘 이루어진 것이기 때문에 Dunn Index가 클수록 군집화가 잘 형성되어 있다고 볼 수 있음.

#### 4. BMU (Best-Matching Unit) (p.121)

- SOM (Self Organizing Maps) 에서 **표본 벡터와 거리가 가장 가까운 프로토타입 벡터**를 선택하는데, BMU는 이 때 선택된 프로토타입 벡터를 나타내는 용어이다.



### 3장 비정형 데이터마이닝

### 1절 텍스트마이닝

#### 1. 텍스트 마이닝 (Text Mining) (p.129)

-   입력된 텍스트를 구조화해 그 데이터에서 패턴을 도출한 후 결과를 평가 및 해석하는 일련의 과정
-   다양한 포맷의 문서로부터 텍스트를 추출해 단어 구성에 따라 데이터 마트를 구성한다.
-   인터넷 데이터, 소셜미디어 데이터 등과 같은 자연어로 구성된 비정형 텍스트 데이터 속에서 정보나 관계를 발견하는 분석 기법

#### 2. 텍스트 마이닝 기능

-   문서요약 (Summarization)
-   문서 분류 (classification)
-   문서 군집 (clustering)
-   특성 추출 (feature extraction)

#### 3. 정보 검색의 적절성

-   분석 결과를 평가하기 위해 **정확도와 재현율**을 사용한다.
-   정확도 (precision): 분석 모델이 정답이라고 예측한 결과중에서 실제로 정답인 경우의 비율 (TP / (TP + FP))
-   재현율 (Recall) : 실제로 정답인 것들 중에서 분석 모델이 정답이라고 내놓은 결과의 비율 (TP / (TP + FN))

![precision_recall](https://user-images.githubusercontent.com/291782/150641056-4425fc9d-36be-4369-9c35-f76b1522c204.png)

#### 4. Corpus

-   데이터마이닝의 절차 중 데이터의 정체, 통합, 선택, 변환의 과정을 거친 구조화된 단계로 더 이상 추가적인 절차 없이 데이터 마이닝 알고리즘 실험에 활용될 수 있는 상태이다.

#### 5. Term-Document Matrix

-   텍스트 마이닝을 불러온 문서에 대해 plain text로 전환, 공백 제거, lowercase로 변환, 불용어(stopward) 처리, 어간추출(stemming) 등의 작업을 수행한 다음에 문서 번호와 단어 간의 사용 여부 또는 빈도수를 이용해 matrix를 만드는 작업이 term document matrix이다.

#### 6. Dictionary

-   텍스트 마이닝 분석 시 사용하고자 하는 단어들의 집합

#### 7. 감성분석

-   문장에서 사용된 단어의 긍정과 부정 여부에 따라 전체 문장의 긍정/부정 여부를 평가한다.
-   브랜드에 대한 평판 분석 가능

#### 8. 한글 처리

-   R의 텍스트 마이닝 패키지 : KoNLP
-   명사를 추출할때는 extractNoun("문장") 함수를 사용

#### 9. 워드 클라우드

-   문서에 포함된 단어들의 사용 빈도를 효과적으로 보여주기 위해 단어들을 크기, 색 등으로 나타내어 구름 등과 같은 형태로 시각화 하는 기법





### 2절 사회연결망 분석

#### 1. 사회연결망 분석 (p.135)

##### 가. SNA (Social Network Analysis) 정의

-   개인과 집단들 간의 관계를 노드와 링크로 모델링하여 그것의 위상구조와 확산 및 진화 과정을 계량적으로 분석하는 방법론
-   개인의 인간관계가 인터넷으로 확대된 사람 사이의 네트워크
-   개인 또는 집단이 하나의 노드(node), 연결은 선(link 또는 edge)으로 표현됨

##### 나. SNA 분류

1.   집합론적 방법
     -   각 객체들 간의 관계를 관계 쌍(pairs of elements)으로 표현
2.   그래프 이론을 이용한 방법
     - 객체를 점(노드 or 꼭지점)으로 표현하고, 연결은 두 점을 연결하는 선으로 표현
3.   행렬을 이용한 방법
     - 각 객체를 행과 열에 배치하고, 각 객체간의 관계가 존재하면 1을 넣고, 존재하지 않으면 0을 넣음.

#### 2. 사회연결망 분석에서 네트워크 구조를 파악하기 위한 기법

##### 가. 중심성(Centrality)

- 연결정도 중심성 (Degree centrality) : 한 점에 직접적으로 연결된 점들의 합
- 근접 중심성 (Closeness centrality) : 한 노드로부터 다른 노드에 도달하기까지 필요한 최소 단계의 합
- 매개 중심성 (Betweenness centrality) : 네트워크 내에서 한 점이 담당하는 매개자 혹은 중재자 역할의 정도
- 위세 중심성 (Eigenvector centrality) : 보나시치(Bonacich) 권련지수 : 위에 중심성의 일반적인 형태로, 연결된 노드의 중요성에 가중치를 둬 노드의 중심성을 측정하는 방법

#### 3. SNA 적용

- 소셜 네트워크 분석은 통신, 온라인 소셜 미디어, 게임 및 유통업체에서 관심이 높다.
- 분석용 솔루션으로는 KXEN, SAS, XTRACT, Indiro, Onalytica, Unicet, Pajek, Inflow 등이 있다.

#### 4. SNA 단계

1. 그래프 생성단계
2. 그래프를 목적에 따라 가공하여 분석하는 단계
3. 커뮤니티를 탐지하고 각 객체 또는 노드의 역할(롤)을 정의해 어떠한 롤도 다른 객체들에게 영향력을 더 효율적으로 줄 수 있는지를 정의하는 단계
4. 위 결과를 데이터화하여 다른 데이터마이닝 기법과 연계하는 단계



#### 5. R에서의 SNA

##### 가. 네트워크 레벨 통계량

degree, shortest paths, reachability, density, reciprocity, transitivity, triad census 등

##### 나. 커뮤니티의 수를 측정하는 방법 (community detection)

1. WALKRAP 알고리즘
    - 일련의 random walk 과정을 통해 커뮤니티를 발견한다.
2. Edge Betweenness method
    - 그래프에 존재하는 최단거리 (shortest path) 중 몇 개가 그 edge (연결, link)를 거쳐가는 지를 이용해 edge-betweenness 점수를 측정한다.

#### 6. 활용방안

- 소셜 네트워크 분석은 데이터가 몇개의 집단으로 구성되는지, 집단 간의 특징은 무엇이고, 해당 집단에서 영향력 있는 고객은 누구인지, 시간의 흐름과 고객 상태의 변화에 따라 다음에 누가 영향을 받을지를 기반으로 churn/acquisition prediction, fraud, product recommendation 등에 활용한다.





### 4장 서술형 문제
#### 3. 회귀분석 (p.151)

##### 나. 예상문제 (Cars93)

Q. MASS 패키지의 Cars93 이라는 데이터셋의 가격(price)을 종속변수로 선정하고, 엔지크기, RPM, 무게를 이용해서 다중회귀분석을 실시했다. 아래의 결과를 해석하시오

```R
> # 회귀분석 (p.152)
> library(MASS)
> head(Cars93)
  Manufacturer   Model    Type Min.Price Price Max.Price MPG.city MPG.highway            AirBags DriveTrain
1        Acura Integra   Small      12.9  15.9      18.8       25          31               None      Front
2        Acura  Legend Midsize      29.2  33.9      38.7       18          25 Driver & Passenger      Front
3         Audi      90 Compact      25.9  29.1      32.3       20          26        Driver only      Front
4         Audi     100 Midsize      30.8  37.7      44.6       19          26 Driver & Passenger      Front
5          BMW    535i Midsize      23.7  30.0      36.2       22          30        Driver only       Rear
6        Buick Century Midsize      14.2  15.7      17.3       22          31        Driver only      Front
  Cylinders EngineSize Horsepower  RPM Rev.per.mile Man.trans.avail Fuel.tank.capacity Passengers Length Wheelbase
1         4        1.8        140 6300         2890             Yes               13.2          5    177       102
2         6        3.2        200 5500         2335             Yes               18.0          5    195       115
3         6        2.8        172 5500         2280             Yes               16.9          5    180       102
4         6        2.8        172 5500         2535             Yes               21.1          6    193       106
5         4        3.5        208 5700         2545             Yes               21.1          4    186       109
6         4        2.2        110 5200         2565              No               16.4          6    189       105
  Width Turn.circle Rear.seat.room Luggage.room Weight  Origin          Make
1    68          37           26.5           11   2705 non-USA Acura Integra
2    71          38           30.0           15   3560 non-USA  Acura Legend
3    67          37           28.0           14   3375 non-USA       Audi 90
4    70          37           31.0           17   3405 non-USA      Audi 100
5    69          39           27.0           13   3640 non-USA      BMW 535i
6    69          41           28.0           16   2880     USA Buick Century
> lm(Price~EngineSize+RPM+Weight, data=Cars93)

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Coefficients:
(Intercept)   EngineSize          RPM       Weight  
 -51.793292     4.305387     0.007096     0.007271 

> attach(Cars93) # 함수 호출 후 모든 코드에서 컬럼들을 직접 전근할 수 있게 해줌

> lm(Price~EngineSize+RPM+Weight, data=Cars93)

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Coefficients:
(Intercept)   EngineSize          RPM       Weight  
 -51.793292     4.305387     0.007096     0.007271  

> summary(lm(Price~EngineSize+RPM+Weight, data = Cars93))

Call:
lm(formula = Price ~ EngineSize + RPM + Weight, data = Cars93)

Residuals:
    Min      1Q  Median      3Q     Max 
-10.511  -3.806  -0.300   1.447  35.255 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -51.793292   9.106309  -5.688 1.62e-07 ***
EngineSize    4.305387   1.324961   3.249  0.00163 ** 
RPM           0.007096   0.001363   5.208 1.22e-06 ***
Weight        0.007271   0.002157   3.372  0.00111 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.504 on 89 degrees of freedom
Multiple R-squared:  0.5614,	Adjusted R-squared:  0.5467 
F-statistic: 37.98 on 3 and 89 DF,  p-value: 6.746e-16

> 
```

1.   해석방법 : 다변량 회귀분석은 아래와 같은 단계로 분석할 수 있다.
     -   1단계: 다변량 모형에 대한 가설검정 실시
     -   2단계: 각 변수의 계수에 대한 가설검정 실시
     -   3단계: 결정계수를 통한 모형에 대한 설명력 확인
     -   4단계: 다중공선성의 확인을 통한 모형의 안정성 확인
     -   5단계: 잔차분석을 통한 다변량 회귀분석의 가정 확인
2.   해석결과
     -   위의 분석은 1~3단계 까지 해석 가능
     -   모형의 구조는 formula를 통해 확인할 수 있으며, 가격(Price)을 종속변수로 예측하기 위해 엔진사이즈(EngineSize), 회전수(RPM), 무게(Weight)라는 3가지 설명변수를 활용하여 모형이 설계되었음을 확인할 수 있다.
     -   1단계 : 다변량 회귀분석에서 종속변수인 가격(Price)에 대한 설명변수들 간의 모형에 대한 통계적 타당성을 가설 검정한다.
         -   귀무가설(H<sub>0</sub>) : EngineSize = RPM = Weight = 0
         -   대립가설(H<sub>1</sub>) : 적어도 하나의 설명변수는 0이 아니다.
         -   F-통계량은 37.98이며 p-value 가 6.746e-16 로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형은 통계적으로 매우 유의함을 알 수 있다.
     -   2단계 : 다변량 회귀분석에 활용된 각 설명변수들의 계수들에 대한 통계적 타당성을 가설 검정한다.
         -   **첫 번째 설명변수인 엔지사이즈(EngineSize)**에 대한 통계적 가설검정을 실시
             -   귀무가설(H<sub>0</sub>) : EngineSize = 0
             -   대립가설(H<sub>1</sub>): EngineSize $\neq$ 0
             -   t-통계량은 3.249 이며 p-value 값이 0.00163이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 EngineSize는 통계적으로 유의함을 알 수 있다.
         -   **두 번째 설명변수인 RPM**의 경우, t-통계량은 5.208이며 p-value 가 1.22e-06이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다.그러므로 RPM은 통계적으로 유의함을 알 수 있다.
         -   **세 번째 설명변수인 Weigh**t의 경우, t-통계량은 3.372이며 p-value가 0.00111이므로 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 모든 설명변수는 통계적으로 유의함을 알 수 있다.
     -   3단계 : 통계적으로 유의성을 확인한 다변량 회귀모형이 전체 데이터를 얼마나 **잘 설명하는지** 확인하기 위해 **결정계수(R<sup>2</sup>)**를 확인한다.
         -   결정계수를 확인하기 위해 Multiple R-squared와 R-squared를 확인결과 0.5614 와 0.5467 로 나타났으며, 이는 전체 데이터를 설계된 다변량 회귀모형이 56.14%, 54.67%를 설명하고 있다고 해석할 수 있다.
     -   최종적으로 결과를 종합해보면 **추정된 다변량 회귀식**은 아래와 같다.
     -   ```Price = -51.79 + 4.31*EngineSize + 0.007*RPM + 0.007*Weight ```
     -   회귀식을 통해 EngineSize, RPM, Weight 가 모두 증가할수록 가격(Price)도 증가하는 것을 확인할 수 있으며, **Price에 가장 영향을 많이 끼치는 변수는 EngineSize이며, 차량의 가격에서 EngineSize를 가장 많이 신경 써야한다고 결론을 도출**할 수 있다.


##### 다. 예상문제 (swiss)

Q. swiss 데이터는 프랑스어를 사용하는 스위스 내 지역의 출산율과 관련된 자료이다. 아래는 각 변수의 내용과 출산율을 농업종사자 비율 등 5개의 변수로 설명하기 위한 모형을 추정한 결과이다. 아래의 결과를 해석하시오

```R
> # 서술형 회귀분석 (swiss 데이터 p.154)
> # 설명변수 : Agriculture : 농업, Infant.Mortality (영아사망률)
> # 종속변수 : Fertility 출산
> summary(step(lm(Fertility~1, data=swiss),
+             scope = list(lower = ~1, upper=~Agriculture+Examination+Education
+                           +Catholic+Infant.Mortality),
+             direction="both"))
Start:  AIC=238.35
Fertility ~ 1

                   Df Sum of Sq    RSS    AIC
+ Education         1    3162.7 4015.2 213.04
+ Examination       1    2994.4 4183.6 214.97
+ Catholic          1    1543.3 5634.7 228.97
+ Infant.Mortality  1    1245.5 5932.4 231.39
+ Agriculture       1     894.8 6283.1 234.09
<none>                          7178.0 238.34

Step:  AIC=213.04
Fertility ~ Education

                   Df Sum of Sq    RSS    AIC
+ Catholic          1     961.1 3054.2 202.18
+ Infant.Mortality  1     891.2 3124.0 203.25
+ Examination       1     465.6 3549.6 209.25
<none>                          4015.2 213.04
+ Agriculture       1      62.0 3953.3 214.31
- Education         1    3162.7 7178.0 238.34

Step:  AIC=202.18
Fertility ~ Education + Catholic

                   Df Sum of Sq    RSS    AIC
+ Infant.Mortality  1    631.92 2422.2 193.29
+ Agriculture       1    486.28 2567.9 196.03
<none>                          3054.2 202.18
+ Examination       1      2.46 3051.7 204.15
- Catholic          1    961.07 4015.2 213.04
- Education         1   2580.50 5634.7 228.97

Step:  AIC=193.29
Fertility ~ Education + Catholic + Infant.Mortality

                   Df Sum of Sq    RSS    AIC
+ Agriculture       1    264.18 2158.1 189.86
<none>                          2422.2 193.29
+ Examination       1      9.49 2412.8 195.10
- Infant.Mortality  1    631.92 3054.2 202.18
- Catholic          1    701.74 3124.0 203.25
- Education         1   2380.38 4802.6 223.46

Step:  AIC=189.86
Fertility ~ Education + Catholic + Infant.Mortality + Agriculture

                   Df Sum of Sq    RSS    AIC
<none>                          2158.1 189.86
+ Examination       1     53.03 2105.0 190.69
- Agriculture       1    264.18 2422.2 193.29
- Infant.Mortality  1    409.81 2567.9 196.03
- Catholic          1    956.57 3114.6 205.10
- Education         1   2249.97 4408.0 221.43

Call:
lm(formula = Fertility ~ Education + Catholic + Infant.Mortality + 
    Agriculture, data = swiss)

Residuals:
     Min       1Q   Median       3Q      Max 
-14.6765  -6.0522   0.7514   3.1664  16.1422 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)      62.10131    9.60489   6.466 8.49e-08 ***
Education        -0.98026    0.14814  -6.617 5.14e-08 ***
Catholic          0.12467    0.02889   4.315 9.50e-05 ***
Infant.Mortality  1.07844    0.38187   2.824  0.00722 ** 
Agriculture      -0.15462    0.06819  -2.267  0.02857 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.168 on 42 degrees of freedom
Multiple R-squared:  0.6993,	Adjusted R-squared:  0.6707 
F-statistic: 24.42 on 4 and 42 DF,  p-value: 1.717e-10
```

1.   해석 방법

     -   벌점화 방식(AIC)의 변수선택법을 활용한 다변량 회귀분석은 아래와 같은 단계로 분석할 수 있다.
     -   1단계 : 변수선택법을 결정하고, 초기모형을 세팅
     -   2단계 : 선택된 최적 모형의 AIC를 계산
     -   3단계 : 선택된 모형에서 변수를 추가/삭제 할 경우의 각 모형에서 AIC를 계산
     -   4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정
     -   5단계 : 2 ~ 4단계를 반복하고 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적모형으로 선정
     -   6단계 : 다변량 모형에 대한 F-test를 통해 가설검정 실시
     -   7단계 : 각 변수의 계수에 대한 t-test를 통해 가설검정 실시
     -   8단계 : 결정계수를 통한 모형에 대한 설명력 확인
     -   9단계 : 다중공선성의 확인을 통한 모형의 안정성 확인
     -   10단계 : 잔차분석을 통한 다변량 회귀분석의 가정 확인

2.   해석 결과

     -   위의 분석 결과는 해석방법 10단계 중 1~8단계까지 해석이 가능. 모형의 구조는 formula를 통해 확인 가능하며, step 함수를 통해 종속변수에 대해 설명변수가 없을 경우부터 모든 설명변수가 포함될 때의 회귀모형을 비교해 최적의 회귀방정식을 도출할 수 있다. 또, direction에서 ```both```는 단계적 선택법, ```forward```는 전진선택법, ```backward```는 후진제거법을 의미

     -   출산율(Fertility)을 종속변수로 설정하고, 설명변수가 없을 때 부터 최대 모든 설명변수가 포함된 회귀식까지 설정하여 최적의 회귀식을 도출한다

     -   1단계 : 변수선택법을 결정하고, 초기 모형을 설정

         위의 분석결과에서 direction이 ```both```로 설정되어 변수선택법을 단계적 선택법으로 선정했음을 확인할 수 있다. 또, 초기 모형은 ```Fertility~1``` 로 설명변수가 하나도 없는 상태에서부터 시작함을 의미한다.```scope```의 경우 모형 선정 중 최소 설명변수가 아무것도 없는것부터 설명변수가 모두 있는 것까지를 비교하여 모형을 선정한다는 것이다.

     -   2단계 : 선택된 최적 모형의 AIC를 계산

         분석 결과에서 시작 모혀은 ```Fertility~1```이 **최적모형**로 설정되어 있으며 start에서 **AIC 값이 238.35**로 계산되어 있다.

     -   3단계 : 선택된 모형에서 변수를 추가/삭제 할 경우의 각 모형의 AIC를 계산한다.

         ```Fertility~1``` 모형에 대해 설명변수 5개에 대한 각각의 AIC값을 계산하여 자유도 등과 함께 나타낸다. **Education의 AIC 값이 213.04, Examination의 AIC 값은 214.97등으로 나타나 있다.**

     -   4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정한다.

         계산된 AIC 값을 비교하여 가장 작은 설명변수인 **Education을 추가하여 최적 모형으로 선정**한다.

     -   5단계 : 2 ~ 4단계를 반복하여 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적의 모형으로 선정한다.

         위의 과정을 반복하여 ```Fertility~Education + Catholic + Infant.Mortality + Agriculture```이 최적의 모형으로 선정되고 마지막 **Step에서 AIC가 189.86으로 계산되고 추가되지 않은 설명변수 Examination의 AIC 값이 190.96**로 나타나 해당 변수를 모형에 추가하지 않고 **최적의 모형을 ```Fertility~Education + Catholic + Infant.Mortality + Agriculture```으로 선정**했다.

     -   6단계 : 다변량 회귀분석에서 종속변수인 출산율(Fertility)에 대한 설명변수들 간의 모형에 대한 통계적 타당성을 가설 검정한다.

         귀무가설 (H<sub>0</sub>) : Education = Catholic = Infant.Mortality = Agriculture = 0

         대립가설 (H<sub>1</sub>) : 적어도 하나의 설명변수는 0이 아니다.

         F-통계량은 24.42이며 p-value 값이 1.717e-10로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형은 통계적으로 매우 유의함을 알 수 있다.

     -   7단계 : 다변량 회귀분석에서 활용된 각 설명변수들의 계수들에 대한 통계적 타당성을 가설 검정한다.

         -   **첫 번째 설명변수인 Education에 대한 통계적 가설검정을 실시**한다.

             귀무가설 (H<sub>0</sub>) : Education = 0

             대립가설 (H<sub>1</sub>) : Education $\neq$ 0 

             t-통계량은 -6.617이며 p-value 값이 5.14e-08 이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 첫 번째 설명변수인 Education은 통계적으로 유의함을 알 수 있다.

         -   **두 번째 설명변수인 Catholic의 경우,** t-통계량은 4.315 이며 p-value 값이 9.50e-05이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 두 번째 설명변수인 Catholic은 통계적으로 유의함을 알 수 있다.

         -   **세 번째 설명변수인 Weight의 경우**, t-통계량은 2.824 이며, p-value 값이 0.00722 이므로 유의수준 5%하에서 대립가설을 채택하고, **네번째 설명변수인 Agriculture의 경우**, t-통계량은 -2.267이며, p-value 값이 0.02857 이므로 유의수준 5%하에서 대립가설을 채택하게된다. 그러므로 모든 설명변수는 통계적으로 유의함을 알 수 있다.

     -   8단계 : 통계적으로 유의성을 확인한 다변량 회귀모형이 전체 데이터를 얼마나 잘 설명하는지 확인하기 위해 결정계수 (R<sup>2</sup>)를 확인한다.

         결정계수를 확인하기 위해 Multiple R-squared:  0.6993,	Adjusted R-squared:  0.6707 로 나타났으며, 이는 전체 데이터를 설계된 다변량 회귀모형이 69.93%, 67.07%를 설명하고 있다고 해석할 수 있다.

     -   최종적으로 다변량 회귀분석 결과를 종합해 보면 추정된 다변량 회귀식은 Fertility = 62.1 - 0.98*Education + 0.13 * Catholic  + 1.08 * Infant.Mortality - 0.16 * Agriculture 이다. **Fertility 에 가장 영향을 많이 끼치는 변수는 Infant.Mortality이며 , 출산율에서 Infant.Mortality 를 가장 많이 신경 써야한다고 결론을 도출** 할 수 있다.

     



#### 4. 주성분 분석(p.158)

##### 가. 개요

-   주성분 분석은 주성분의 선택과 관련하여 출제가 되는 파트로 주성분 분석에 대한 R 프로그램 결과 중 **주성분 분석의 누적 기여율(cumulative proportion)과 시각화 자료인 scree plot, biplot에 대한 해석**은 정확히 숙지



##### 나. 예상문제 (USArrests)

USArrests 데이터는 미국 50개 주에서 폭행, 살인 및 강간에 대한 10만명의 주민에 대한 체포 통계 포함하는 여러개의 변수가 있다. 아래의 결과를 해석하시오

```R
> # pca
> # install.packages("factoextra")
> # install.packages("FactoMineR")
> library(factoextra)
> library(FactoMineR)
     
> pca_rslt = PCA(USArrests, graph = FALSE)
> summary(pca_rslt)

Call:
PCA(X = USArrests, graph = FALSE) 


Eigenvalues
                       Dim.1   Dim.2   Dim.3   Dim.4
Variance               2.480   0.990   0.357   0.173
% of var.             62.006  24.744   8.914   4.336
Cumulative % of var.  62.006  86.750  95.664 100.000

Individuals (the 10 first)
                Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  
Alabama     |  1.574 |  0.986  0.783  0.392 | -1.133  2.596  0.518 |  0.444  1.107  0.080 |
Alaska      |  3.051 |  1.950  3.067  0.409 | -1.073  2.327  0.124 | -2.040 23.343  0.447 |
Arizona     |  2.089 |  1.763  2.507  0.712 |  0.746  1.124  0.127 | -0.055  0.017  0.001 |
Arkansas    |  1.149 | -0.141  0.016  0.015 | -1.120  2.534  0.950 | -0.115  0.074  0.010 |
California  |  3.037 |  2.524  5.137  0.690 |  1.543  4.811  0.258 | -0.599  2.010  0.039 |
Colorado    |  2.114 |  1.515  1.850  0.513 |  0.988  1.971  0.218 | -1.095  6.726  0.268 |
Connecticut |  1.860 | -1.359  1.489  0.534 |  1.089  2.396  0.343 |  0.643  2.321  0.120 |
Delaware    |  1.184 |  0.048  0.002  0.002 |  0.325  0.214  0.075 |  0.719  2.897  0.368 |
Florida     |  3.070 |  3.013  7.321  0.964 | -0.039  0.003  0.000 |  0.577  1.866  0.035 |
Georgia     |  2.366 |  1.639  2.167  0.480 | -1.279  3.305  0.292 |  0.342  0.658  0.021 |

Variables
               Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  
Murder      |  0.844 28.719  0.712 | -0.416 17.488  0.173 |  0.204 11.644  0.042 |
Assault     |  0.918 34.010  0.844 | -0.187  3.534  0.035 |  0.160  7.190  0.026 |
UrbanPop    |  0.438  7.739  0.192 |  0.868 76.179  0.754 |  0.226 14.290  0.051 |
Rape        |  0.856 29.532  0.732 |  0.166  2.800  0.028 | -0.488 66.876  0.238 |
> # scree plot 
> fviz_screeplot(pca_rslt, addlabels = TRUE, ylim = c(0,80))
```

![scree-plot](https://user-images.githubusercontent.com/291782/164728563-68df7929-d200-427e-a6a8-d8f9c07f96ca.png)



##### 다. 해석결과

-   Proportion of Variance (% of var.)는 분산 비율로서 각 주성분이 차지하는 비율을 말하므로 클수록 영향도가 높다는 것을 의미
-   Cumulative Proportion (Cumulative % of var.)는 분산의 누적합계이며, 누적기여율이 **85% 이상이면 주성분의 수로 결정**할 수 있다.
-   위의 분석결과에서 누적기여율이 85% 이상인 **Comp.2 의 누적기여율이 87%이므로 4차원을 2차원으로 축소**할 수 있다.



##### 라. Scree Plot 해석방법

-   scree plot이란 2차원 그래프에서 x축에는 주성분의 개수, y축에는 분산이나 고유값(eigenvalue)을 두어 **주성분 분석에서 요인의 수를 결정하기 위해 사용**된다. 이 외에도 군집분석 등의 분석에서 최적의 군집 설정 등에 사용할 수 있다. 해석방법은 y축의 값이 수평을 유지하기 전 단계로 주성분 개수를 선택한다.
-   위의 그래프로 보면 주성분 3개째에서 그래프의 기울기가 줄어드는 형태를 보이므로 한 개를 뺀 **(3-1=2) 2개 주성분이 적합**하다.



##### 마. biplot  해석방법

```R
fviz_pca_biplot(pca_rslt, reple=FALSE)
```



![pca-biplot](https://user-images.githubusercontent.com/291782/164730920-6e145901-c1a9-485c-b701-29e0364a4b2b.png)

-   biplot 은 원 변수와 성분 (Comp1, 2) 간의 관계를 그래프로 표현한 것으로 그래프를 통해 각 주성분의 의미를 해석하고 각 객체들의 특성을 파악할 수 있다.
-   **화살표는 원 변수와 Comp의 상관계수를 의미**하며, **화살표가 Comp와 평행할수록 상관관계가 크므로** 해당 **Comp에 큰 영향**을 끼친다. 그리고 **화살표가 같은 방향으로 인접해 있을수록 같은 주성분으로 생성될 수도 있음**을 알 수 있다.
-   위의 biplot 에서 Comp1(가로축)을 기준으로 **Murder, Assault, Rape는 같은 방향으로 인접**해 있는 것을 확인할 수 있꼬, Comp2(세로축)을 기준으로 **Urbanpop은 다른 3개와 방향이 다르므로 상관 관계가 낮다.**

-   여기서 이상치는 **Vermont, West Virginia 등은 변수 방향, 상관관계가 동떨어져 이상치**로 판정될 수 있는 데이터이다.
-   이상치의 특성을 파악하라는 문제가 출제된다면, 위 결과에서 이상치라고 판단되는 값들 중 West Virginia는 **범죄율과 도시인구비율이 적으므로 '범죄가 없는 시골'이라고 해석하여 분석 결과를 본 미국 시민들이 그 도시로 몰릴 수**도 있다고 판단할 수 있다.



1.   #### 5. 시계열분석 (p.161)

     ##### 가. 개요

     -   시계열 분석에 대한 R 프로그램 분석 결과 중 **자기상관함수(ACF), 편자기상관함수(PACF)에 따른 모형 선택방법에 대한 해석은 정확히 숙지**해야 된다.

     

     ##### 나. 예상문제 (king)

     -   king 데이터는 영국왕 42명의 사망 시 나이 예제의 데이터로 비계절성을 띄는 시계열 자료이며, 트렌드 요소, 불규칙 요소로 구성되어 있다.

     ```R
     > # 시계열분석
     > king <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat", skip=3)
     Read 42 items
     > head(king)
     [1] 60 43 67 50 56 42
     > summary(king)
        Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
       13.00   44.00   56.00   55.29   67.75   86.00 
     > # ts를 이용하여 king 데이터를 시계열 데이터로 변환 후 king.ts 에 저장
     > king.ts <- ts(king)
     > # diff 함수를 활용해 king 데이터를 1차 차분하여 분석에 사용
     > king.ff1 <- diff(king.ts, differences = 1)
     > # acf 함수에서 
     > # lag.max는 최대 lag를 몇까지 나타낼 것인가를 지정하는 옵션
     > # plot은 ACF 그래프를 나타낼 것인가를 묻는 옵션
     > # 분석 결과에서 각 시점의 자기상관계수 값을 확인 가능
     > # ACF 그래프로 절단점과 모형을 선정 가능
     > king.acf <- acf(king.ff1, lag.max = 20, plot=TRUE)
     > king.acf
     
     Autocorrelations of series ‘king.ff1’, by lag
     
          0      1      2      3      4      5      6      7      8      9     10     11     12     13 
      1.000 -0.360 -0.162 -0.050  0.227 -0.042 -0.181  0.095  0.064 -0.116 -0.071  0.206 -0.017 -0.212 
         14     15     16     17     18     19     20 
      0.130  0.114 -0.009 -0.192  0.072  0.113 -0.093 
     ```

     ![ts-acf](https://user-images.githubusercontent.com/291782/164735304-1ee76849-4633-427e-8782-846e0abe3d85.png)

     

     ##### 다. 해석결과

     -   ACF 그래프는 자기상관함수로 계산된 시차 (1, 2, 3..)에서 **자기상관값과 시차 상관그림**을 함께 그린것으로 **이동평균(MA)모형을 선정**하기 위해 사용하는 그래프이다.
     -   ACF 그래프 확인결과, **lag 1인 지점까지는 점선 구간을 초과**하고 나머지는 점선 구간 안에 있으므로 **lag2에서 절단점**을 가지므로 **MA(1) 모형을 생성**할 수 있다.

     

     ##### 라. 예상문제 (king pacf)

     -   PACF를 통한 적합한 ARIMA 모델 결정을 위한 시계열 분석을 시행한 결과이다. 아래 결과를 해석하시오

     ```R
     > # PACF 결과해석
     > king.pacf <- pacf(king.ff1, lag.max = 20, plot=TRUE)
     > king.pacf
     
     Partial autocorrelations of series ‘king.ff1’, by lag
     
          1      2      3      4      5      6      7      8      9     10     11     12     13     14 
     -0.360 -0.335 -0.321  0.005  0.025 -0.144 -0.022 -0.007 -0.143 -0.167  0.065  0.034 -0.161  0.036 
         15     16     17     18     19     20 
      0.066  0.081 -0.005 -0.027 -0.006 -0.037 
     ```

     ![ts-pacf](https://user-images.githubusercontent.com/291782/164737335-25596406-7d9f-453f-bb35-200b79223990.png)

     

     ##### 마. 해석결과

     -   pacf 분석 결과에서 **각 시점의 편자기상관계수**(Partial autocorrelations) 값을 확인할 수 있고 PACF 그래프로 **절단점과 모형을 선정**할 수 있다.
     -   PACF 그래프는 서로 다른 두 시점 사이의 관계를 분석할 때 중간에 존재하는 값을 제외하고 나타낸 상관계수를 구하여 그린것으로 **자기회귀(AR)모형을 선정하기 위해 사용하는 그래프**이다.
     -   PACF 그래프 확인 결과, lag 3 지점까지 점선을 초과하고 나머지는 점선안에 있으므로, **lag 4에서 절단점**을 가지므로 **AR(3) 모형을 생성**할 수 있다.

     

     ##### 바. 예상문제 (king auto.arima)

     -   아래 결과를 해석하시오

         ```R
         > # auto.arima
         > # install.packages("forecast")
         > library(forecast)
         
         # auto.arima 함수 활용 시 자동으로 ARIMA 모형선정 해줌
         > auto.arima(king.ts)
         Series: king.ts 
         ARIMA(0,1,1) 
         
         Coefficients:
                   ma1
               -0.7218
         s.e.   0.1208
         
         sigma^2 = 236.2:  log likelihood = -170.06
         AIC=344.13   AICc=344.44   BIC=347.56
         
         
         > king.arima <- arima(king, order=c(0, 1, 1))
         > king.forecasts <- forecast(king.arima)
         > king.forecasts
            Point Forecast    Lo 80    Hi 80    Lo 95     Hi 95
         43       67.75063 48.29647 87.20479 37.99806  97.50319
         44       67.75063 47.55748 87.94377 36.86788  98.63338
         45       67.75063 46.84460 88.65665 35.77762  99.72363
         46       67.75063 46.15524 89.34601 34.72333 100.77792
         47       67.75063 45.48722 90.01404 33.70168 101.79958
         48       67.75063 44.83866 90.66260 32.70979 102.79146
         49       67.75063 44.20796 91.29330 31.74523 103.75603
         50       67.75063 43.59372 91.90753 30.80583 104.69543
         51       67.75063 42.99472 92.50653 29.88974 105.61152
         52       67.75063 42.40988 93.09138 28.99529 106.50596
         ```

     

     ##### 사. 해석결과

     -   1단계 auto.arima 결과 해석
         -   auto.arima 분석결과 모형이 ARIMA(0, 1, 1)이 선정되었음
         -   시계열 모형식은 Z<sub>t</sub> = &alpha;<sub>t</sub> + 0.7218 &alpha;<sub>t-1</sub>  이다.
     -   2단계 : forecast 함수 결과 예측
         -   ARIMA(0, 1, 1) 모형 예측했을 때, 43번 ~ 52번째 왕의 사망 시 나이 예측결과는 67.75살로 추정
         -   신뢰구간은 80 ~ 95% 사이
         -   h 인자는 예측하고자 하는 개수를 'h=5' 처럼 5개로 정할 수 있다. ```forecast(king.arima, h=5)```
     -   최종적으로 분석 결과를 종합하면
         -   ACF 그래프를 통해 MA(1) 모형 생성
         -   PCAF 그래프를 통해 **AR(3) 모형** 생성
         -   auto.arima 활용하여 **ARIMA(0, 1, 1) 모형 선정**
         -   ARIMA(0, 1, 1)로 예측 결과, **43 ~ 52번째 왕의 사망 시 나이는 67.65살로 예측한다는 결론을 도출**

     

### 2절  정형 데이터마이닝

#### 1. 로지스틱 회귀분석 (p.165)
##### 가. 개요

-   유의한 설명변수 파악, 회귀 모형의 유의함, 결정 계수 해석, 모형 수식 작성, 오즈비를 이용한 해석 등에 대한 해석은 정확히 숙지해야 함



##### 나. 예상문제 (Default)

-   10,000명의 신용캌드 고객의 체납 여부 (default)와 학생여부(student), 카드잔고(balance), 연봉(income)을 포함하고 있다. 체납 여부를 예측하기 위해 로지스틱 회귀분석을 실시한다. 아래 결과를 해석하시오

```R
> # 로지스틱 회귀분석
> # default 데이터
> # install.packages("ISLR") # Credit Card Default dataset install
> library(ISLR)
> summary(step(glm(default ~ 1, data=Default, family = binomial),
+              scope = list(lower=~1, upper=~student + balance + income),
+              direction="both"))
Start:  AIC=2922.65
default ~ 1

          Df Deviance    AIC
+ balance  1   1596.5 1600.5
+ student  1   2908.7 2912.7
+ income   1   2916.7 2920.7
<none>         2920.7 2922.7

Step:  AIC=1600.45
default ~ balance

          Df Deviance    AIC
+ student  1   1571.7 1577.7
+ income   1   1579.0 1585.0
<none>         1596.5 1600.5
- balance  1   2920.7 2922.7

Step:  AIC=1577.68
default ~ balance + student

          Df Deviance    AIC
<none>         1571.7 1577.7
+ income   1   1571.5 1579.5
- student  1   1596.5 1600.5
- balance  1   2908.7 2912.7

Call:
glm(formula = default ~ balance + student, family = binomial, 
    data = Default)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4578  -0.1422  -0.0559  -0.0203   3.7435  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.075e+01  3.692e-01 -29.116  < 2e-16 ***
balance      5.738e-03  2.318e-04  24.750  < 2e-16 ***
studentYes  -7.149e-01  1.475e-01  -4.846 1.26e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2920.6  on 9999  degrees of freedom
Residual deviance: 1571.7  on 9997  degrees of freedom
AIC: 1577.7

Number of Fisher Scoring iterations: 8
```

-   해석방법

    -   벌점화 방식(AIC)의 변수선택법을 활용한 로지스틱 회귀분석은 아래와 같은 단계로 분석할 수 있다.
    -   1단계 : 변수선택법을 결정하고, 초기 모형을 세팅
    -   2단계 : 선택된 최적 모형의 AIC를 계산
    -   3단계 : 선택된 모형에서 변수를 추가 / 삭제 할 경우의 각 모형에서 AIC를 계산
    -   4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정
    -   5단계 : 2단계에서 4단계를 반복하고 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적 모형으로 선정
    -   6단계 : 각 변수의 계수에 대한 가설검정 실시

-   해석결과

    -   위 분석결과는 6단계 해석이 모두 가능. 분석에 앞서 모형의 구조는 formula를 통해 확인 할 수 있고, step 함수를 통해 종속변수에 대해 설명변수가 없을 경우부터 모든 설명변수가 포함될 때의 회귀모형을 비교해 최적의 회귀방정식을 도출할 수 있다. 또, direction에서 'both'는 단계적 선택법, 'forward'는 전진선택법, 'backward'는 후진제거법을 의미한다.

    -   Default를 종속변수로 설정, 설명변수가 없을 때부터 최대 모든 설명변수가 포함된 회귀식까지 설정하여 최적의 회귀식을 도출

    -   1단계 : 변수 선택법을 결정하고, 초기 모형을 설정한다.

        -   위의 분석결과에서 direction이 both로 설정되어 변수선택법을 단계적 선택법으로 선정
        -   초기 모형은 default ~ 1로 설명변수가 하나도 없는 상태에서 부터 모두 있는 것까지를 비교를 의미

    -   2단계 : 선택된 최적 모형의 AIC를 계산

        -   분석 결과에서 시작 모형은 **default~1이 최적모형**으로 설정되어 있으며 start에서 **AIC 값이 2922.65**로 계산

    -   3단계 : 선택된 모형에서 변수를 추가 / 삭제 할 경우의 각 모형의 AIC를 계산한다.

        -   default~1 모형에 대해 설명변수 3개에 대한 각각의 AIC 값을 계산하여 자유도 등과 함께 나타낸다. **balance의 AIC 값이 1600.5, student 의 AIC 값은 2912.7, income의 AIC 값은 2920.7**로 나타나있다.

    -   4단계 : 각 모형에서 최소의 AIC 모형을 선택하여 최적 모형으로 선정한다.

        -   계산된 AIC 값을 비교하여 가장 작은 설명변수인 **balance를 추가하여 최적 모형으로 선정**한다.

    -   5단계 : 2~4단계를 반복하여 AIC가 더 이상 줄어들지 않을 때 최종모형을 최적의 모형으로 선정한다

        -   위의 과정을 반복하여 default ~ balance + student이 최적의 모형으로 선정되고 마지막 **step 에서 AIC가 1577.68로 계산되고 추가되지 않은 설명변수 income의 AIC 값이 1579.5**로 나타나 해당 변수를 모형에 추가하지 않고 **최적의 모형을 default~balance + student로 선정**했다.

    -   6단계 : 로지스틱 회귀분석에 활용된 각 설명변수들의 계수들에 대한 통계적 타당성을 가설 검정한다.

        -   첫 번째 설명변수인 balance에 대한 통계적 가설검정을 실시한다.
            -   귀무가설 (H<sub>0</sub>) : balance = 0
            -   대립가설 (H<sub>1</sub>) : balance $\neq$ 0
            -   z-통계량은 24.75 이며 p-value 값이 < 2e-16 이므로 귀무가설의 기각역인 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 책택하게 된다ㅏ. 그러므로 추정된 회귀 모형의 첫 번째 설명변수인 balance는 통계적으로 유의함을 알 수 있다.
            -   두 번째 설명변수인 studentYes의 경우 , z-통계량은 -4.846 이며 p-value 값이  1.26e-06 이므로 0.05보다 작게 나타남에 따라 유의수준 5%하에서 대립가설을 채택하게 된다. 그러므로 추정된 회귀모형의 두 번째 설명변수인 studentYes는 통계적으로 유의함을 알수 있다. 그러므로 추정된 회귀모형의 모든 설명변수는 통계적으로 유의함을 알 수 있다.

    -   최종적으로 로지스틱 회귀분석 결과를 종합해 보면 추정된 로지스틱 회귀식은

        $P(X) = \dfrac {1} {1 + exp(-(-10.75 + 0.005738*balance - 0.7149*studentYes))}$

        또 다른 설명변수의 조건이 동일할 떄, studentYes이 1 증가할수록 졸업할 확률이 exp(-0.715) = 0.49배 증가, 즉 학생일수록 체납확률이 낮아딘다고 할 수 있다. 회귀식을 통해 balance가 증가할수록 default는 증가하고, studentYes가 증가할수록 default는 감소한다. default에 가장 영향을 많이 끼치는 변수는 studentYes 이다.


##### 다. 예상문제 (Crabs)

- crabs 데이터는 참게 중 암참게에 붙어사는 숫참게에 대한 데이터이다. 암참게가 부수체를 갖는지 여부에 영향을 미치는 요인을 조사했으며, 암참게가 한 마리 이상의 부수체를 가지면 y=1로 정의, 그렇지 않으면 y=0으로 정의했다. 아래는 부수체(y) 예측에 너비(width)를 이용한 로지스틱회귀모형을 적합했다. 아래의 물음에 답하시오

  ```R
  # > install.packages("glmbb")
  > library(glmbb)
  > head(crabs)
     color spine width satell weight y
  1 medium   bad  28.3      8   3050 1
  2   dark   bad  22.5      0   1550 0
  3  light  good  26.0      9   2300 1
  4   dark   bad  24.8      0   2100 0
  5   dark   bad  26.0      4   2600 1
  6 medium   bad  23.8      0   2100 0
  
  > glm(formula = crabs$y~crabs$width, family=binomial, data=crabs)
  
  Call:  glm(formula = crabs$y ~ crabs$width, family = binomial, data = crabs)
  
  Coefficients:
  (Intercept)  crabs$width  
     -12.3508       0.4972  
  
  Degrees of Freedom: 172 Total (i.e. Null);  171 Residual
  Null Deviance:	    225.8 
  Residual Deviance: 194.5 	AIC: 198.5
  ```

- 해석결과

  1. 너비 x에 대한 부수체의 예측 확률을 구하는 식을 작성하시오

  - 예측확률을 구하는 식은 로지스틱 회귀식을 작성하면 됨
  - 회귀식은 $\dfrac {1} {1 + exp^{12.3508 - 0.4972width}}$ 이다.

  2. 로지스틱 회귀모형은 오즈 (odds)를 이용하여 해석가능. x가 한 단위 증가함에 따라 오즈는 (a) 증가한다. 그러므로 암참게 자료에 대하여 부수체의 오즈는 너비가 1cm 증가함에 따라 (a)=1.64배 증가하는 것으로 추정된다.
     - x가 한 단위 증가함에 따라 오즈는 exp(0.4972) 만큼 증가한다. 또한 x+1에서의 오즈는 x일때의 오즈와 동일하므로 exp(0.4972) 만큼 증가한다. 그러므로 1cm 증가함에 따라 exp(0.4972)=1.64배 증가
  3. 평균 너비값이 x=26.3인 경우에는 부수체의 확률이 0.671로 예측되고 오즈는 (b)=2.07이다. x=27.3(=26.3+1) 일 때의 예측확률은 0.773이고 이데 대한 오즈는 (c)=3.40 임을 확인할 수 있다.
     - x=26.3 일때 (1)에서 구한 로지스틱 회귀식으로 확률을 구하면 0.671이 나타나게 되고 오즈는 $\dfrac {0.671} {1-0.671} = 2.07$로 나타난다. 또한 x=27.3 일 때의 예측확률도 (1)에서 구한 로지스틱 회귀식으로 확률을 구하면 0.773이 나타나게 되고 오즈는 $\dfrac {0.773} {1-0.773} = 3.40$으로 나타난다.



#### 2. 의사결정나무 (p.169)

##### 가. 개요

-   의사결정나무 모델링 결과, 교차타당성오차에 대한 해석을 정확히 숙지



##### 나. 예상문제 (iris)

-   iris 데이터 셋

    ```R
    > library(rpart)
    > head(iris)
      Sepal.Length Sepal.Width Petal.Length Petal.Width Species
    1          5.1         3.5          1.4         0.2  setosa
    2          4.9         3.0          1.4         0.2  setosa
    3          4.7         3.2          1.3         0.2  setosa
    4          4.6         3.1          1.5         0.2  setosa
    5          5.0         3.6          1.4         0.2  setosa
    6          5.4         3.9          1.7         0.4  setosa
    
    > tree <- rpart(Species~., data = iris)
    > tree
    n= 150 
    
    node), split, n, loss, yval, (yprob)
          * denotes terminal node
    
    1) root 150 100 setosa (0.33333333 0.33333333 0.33333333)  
      2) Petal.Length< 2.45 50   0 setosa (1.00000000 0.00000000 0.00000000) *
      3) Petal.Length>=2.45 100  50 versicolor (0.00000000 0.50000000 0.50000000)  
        6) Petal.Width< 1.75 54   5 versicolor (0.00000000 0.90740741 0.09259259) *
        7) Petal.Width>=1.75 46   1 virginica (0.00000000 0.02173913 0.97826087) *
    
    > # install.packages("rpart.plot")
    > library(rpart.plot)
    > prp(tree, type=4, extra=2)
    ```

-   ![dt-iris](https://user-images.githubusercontent.com/291782/165328365-c87729fd-0b86-4d39-b34f-bbfb2bbc0461.png)



-   해석방법
    -   1단계: rpart 함수 설명
    -   2단계 : rpart 결과에서 조건에 따른 분류에 대해 파악
    -   3단계 : 시각화 결과에 대하여 해석하고 인사이트 도출
-   해석결과
    -   1단계 : 의사결정나무 다양한 패키지 중 rpart 패키지에 내장되어 있는 rpart 함수를 사용한다.  rpart 패키지는 CART (classification and Regression Tree)의 아이디어를 구현한 패키지이며, rpart(formula, data)의 기본 형태로 의사결정나무 모형을 구축할 수 있다. 또, rpart 함수 내에 모형의 적합을 위해 control 인자의 rpart.control 등의 명령어를 추가로 활용할 수 있다.
    -   2단계 : 위의 결과에서 n=150은 데이터의 개수가 150개를 의미하고 1) root 부터 해석을 진행한다. 총 150개의 데이터 중 50개를 setosa로 분류하는데, 조건은 **2) Petal.Length < 2.45의 경우 모두 setosa로 분류**한다. 또, 100개 중 50개를 versicolor로 분류하는데 **Petal.Length >= 2.45 이며 Petal.Width < 1.75인 54개를 versicolor로 분류하고, Petal.Length >= 2.45이며 Petal.Width >= 1.75인 46개를 virginica로 분류**했다.
    -   3단계 : rpart 함수 결과와 동일하지만, 시각화 결과만 보여주고 해석 문제 출제될 수 있으니 숙지 필요함. 그래프를 보면 **150개 중 50개는 Petal.Length < 2.5일 때, setosa 분류, Petal.Length >= 2.5이고, Petal.Width < 1.8인 54개 중 49개는 versicolor로, Petal.Width >= 1.8 인 46개 중 45개를 virginica로 분류**했다.
    -   만약 3가지 꽃 종류 중 versicolor 의 판매가가 높다는 정보가 있다면, 도출된 결과를 바탕으로 '**Petal.Length >= 2.5 , Petal.Width < 1.8인 꽃만 추출해 판매한다**' 고 인사이트를 제시해 낼 수 있다.



-   예상문제
    -   아래 결과는 위의 의사결정나무 모형의 교차타당성오차에 대한 결과와 그래프이다. 아래의 결과를 해석하시오

 ```R
> tree$cptable
    CP nsplit rel error xerror       xstd
1 0.50      0      1.00   1.11 0.05372150
2 0.44      1      0.50   0.56 0.05923963
3 0.01      2      0.06   0.10 0.03055050

> opt <- which.min(tree$cptable[, "xerror"])
> cp <- tree$cptable[opt, "CP"]
> prune.c <- prune(tree, cp=cp)
> plotcp(tree)
 ```

![plotcp](https://user-images.githubusercontent.com/291782/165335460-023406d4-59fe-4cc6-bd0c-9cb062c1f509.png)



-   해석방법
    -   1단계 : 함수에 포함된 기능에 대해 설명
    -   2단계 : 분석결과가 의미하는 바를 파악
    -   3단계 : 시각화 결과에 대하여 해석하고 인사이트를 도출
-   해석결과 (p.172)
    -   1단계 : 










     

     


## ADP 데이터 분석 END



## ADsP 데이터 분석 START
### 4장 통계 분석

### 1절 통계분석의 이해

#### 2. 통계자료의 획득 방법 (p.282)

##### 가. 총 조사 / 전수 조사 (census)

-   많은 비용과 시간이 소요되므로 특별한 경우를 제외하고는 사용되지 않음. (eg. 인구주택 총 조사)

##### 나. 표본조사 

-   모집단에서 샘플을 추출하여 진행하는 조사
-   모집단 (population) : 대상 집단 전체
-   원소 (element) : 모집단을 구성하는 개체
-   표본 (sample) : 조사하기 위해 추출한 모집단의 일부 원소
-   모수 (parameter) : 표본 관측에 의해 구하고자 하는 모집단에 대한 정보
-   모집단의 정의, 표본의 크기, 조사 방법, 조사기간, 표본추출방법을 정확히 명시해야 함



##### 다. 표본 추출 방법 4가지 (중요)

1.   단순랜덤 추출법 (simple random sampling)

     -   각 샘플에 번호를 부여하여 n개를 추출하는 방법으로 각 샘플은 선택될 확률이 동일하다. (복원, 비복원 추출)

2.   계통추출법 (systematic sampling)

     -   단순랜덤 추출법의 변형된 방식으로 샘플을 나열하여 K개씩 n개의 구간으로 나누고 첫 구간에서 하나를 임의로 선택한 후에 K개식 띄어서 n 개의 표본을 선택
     -   ![systematic-sampling](https://user-images.githubusercontent.com/291782/161560760-0d60d365-a300-4262-8b09-5e9d64125e21.png)

3.   집락추출법 (cluster random sampling)

     -   군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링하는 방법
     -   ![cluster-random-sampling](https://user-images.githubusercontent.com/291782/161560900-294b7296-b204-41f2-9170-0f60ae4d9fc4.png)

4.   층화추출법 (stratified random sampling)

     -   이질적인 원소들로 구성된 모집단에서 각 계층을 고루 대표할 수 있도록 표본을 추출하는 방법으로, 유사한 원소끼리 몇 개의 층(stratum)으로 나누어서 각 층에서 랜덤 추출하는 방법
     -   ![stratified-random-sampling](https://user-images.githubusercontent.com/291782/161561325-a774c94f-ce60-4430-a2fb-f91dcdf969c2.png)

     

     

     ##### 라. 측정 (measurement)

     측정방법 (아주 중요)

     -   질적처도 : 범주형 자료, 숫자들의 크기 차이가 계산되지 않는 척도
         -   명목척도 : 측정 대상이 어느 **집단**에 속하는지 분류할 때 사용 (성별, 출생지 구분)
         -   순서척도 : 측정 대상의 **서열관계**를 관측하는 척도 (만족도, 선호도, 학년, 신용등급)
     -   양적척도 : 수치형자료, 숫자들의 크기 차이를 계산할 수 있는 척도
         -   구간척도(등간척도) : 측정 대상이 갖고 있는 **속성의 양**을 측정하는 것으로 구간이나 구간 사이의 **간격이 의미가 있는** 자료 (온도, 지수)
         -   비율척도 : 간격(차이)에 대한 비율이 의미를 가지는 자료, **절대적인 기준인 0이 존재**하고 **사칙연산이 가능**하며 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리)

     순서척도는 명목척도와 달리 매겨진 숫자의 크기를 의미있게 활용 가능 (예: 1등이 2등보다 성적이 높다)

     구간척도는 절대적 크기는 측정할 수 없기 때문에 사칙연산 중 더하기와 빼기는 가능. 곱하기나 나눗셈은 불가능

     

     

     #### 3. 통계분석 (p.285)

     

     #### 4. 확률 및 확률분포 (p.285)

     ##### 나. 확률분포

     1.   이산형 확률변수

          -   베르누이 확률분포 (Bernoulli distribution)

              -   결과가 2개만 나오는 경우 **성공 또는 실패** (예. 동전 던기지, 시험의 합격/불합격 등)
              -   $P(X = x) = P^x . (1-p)$<sup>1-x</sup> 
              -   (x= 1 or 0), 기댓값: $E(x) = p$, 분산 :$var(x) = p(1-p)$
              -   예) 추신수가 안타를 칠 확률은 베르누이 분포를 따른다.
          -   이항분포 (Binomial distribution)

              -   베르누이 시행을 n번 반복했을 때 k번 성공할 확률
              -   n번 시행 중에 각 시행의 확률이 p일 때, k번 성공할 확률분포
              -   $P(X = k) = _nC_kP^k(1-p)$<sup>n-k</sup> , $_nC_k = \dfrac {n!}{k!(n-k)!}$
              -   기댓값 : $E(X) = np$, 분산 : $V(X) = np(1-p) $  (단, n과 k가 1이면 베르누이 시행)
              -   추신수가 오늘 경기에서 5번 타석에 들어와서 3번 안타를 칠 확률은 이항분포를 따른다. (n=5, k=3, 안타를 칠 확률 P(x) = 타율로 적용 가능)
              -   성공할 확률 p가 0이나 1에 가깝지 않고 n이 충분히 크면 이항분포는 정규분포에 가까워 진다. 성공할 확률 p가 1/2에 가까우면 종모양이 된다.
          -   기하분포 (Geometric distribution)
              -   성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 X번 실패할 확률
              -   예) 추신수가 오늘 경기에서 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률은 기하분포를 따른다.
          -   다항분포 (Multinomial distribution)
              -   이항분포를 확장한 것으로 세가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포
          -   포아송분포 (Poisson distiribution)
              -   시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포
              -   예) 책에 오타가 5page 당 10개씩 나온다고 할 떄, 한 페이지에 오타가 3개 나올 확률, 추신수가 최근 5경기에서 홈런을 쳤을 경우, 오늘 경기에서 홈런을 못 칠 확률은 포아송 분포
              -   &lambda; (람다) = 정해진 시간 안에 어떤 사건이 일어날 횟수에 대한 기댓값, y= 사건이 일어난 수
              -   $P = \dfrac {\lambda^ne^{-\lambda}} {n!}$ (e는 자연상수)
              -   기댓값 : $E(X) = \lambda$, 분산 : $V(X) = \lambda $

     2.   연속형 확률변수

          - 가능한 값이 실수의 어느 특정구간 전체에 해당하는 확률변수 (확률밀도함수)

          - $ f(x)\ge 0 $     $\int_{-\infty}^{\infty}f(x)dx = 1$

          - 균일분포 (일양분포, Uniform distiribution)

              - 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포)
              - $E(X) = \dfrac {a+b}{2}, Var(X) = {(b-a)^2}{12}$
              - ![uniform-distribution](https://user-images.githubusercontent.com/291782/161756609-ae577e06-5c55-410f-b205-63f1c5afd9b6.png)

          - 정규분포 (Normal distribution)

              - 평균이 &mu; (뮤) 이고, 표준편차가 &sigma; (시그마) 인 X의 확률밀도 함수
              - 표준편차가 클 경우 퍼져보이는 그래프가 나타남
              - 표준정규분포는 평균이 0 이고, 표준편차가 1인 정규분포
              - 정규분포를 표준정규분포로 만들기 위해서는 $Z = \dfrac {X - \mu} {\sigma}$  식을 이용
              - ![normal-distribution](https://user-images.githubusercontent.com/291782/161757336-f8a45f83-945c-4560-98b3-eee70cde4fa1.png)

          - 지수분포 (Exponential distribution)

              - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포이다.
              - 예) 전자렌지의 수명시간, 콜센터에 전화가 걸려올때 까지의 시간, 은행에 고객이 내방하는데 걸리는 시간, 정류소에서 버스가 올 때까지의 시간
              - ![exponential-distribution](https://user-images.githubusercontent.com/291782/161757625-0f01dde3-c578-4d62-b92d-8e2c667ec95c.png)

          - t분포 (t-distribution)

              - 표준정규분포와 같이 평균이 0을 중심으로 좌우가 동일한 분포를 따른다.
              - 표본이 커져서 (30개 이상) 자유도가 증가하면 표준정규분포와 거의 같은 분포가 된다.
              - 데이터가 연속형일 경우 활용한다.
              - **두 집단의 평균이 동일한지** 알고자 할 때 검정통계량으로 활용된다.
              - ![t-distribution](https://user-images.githubusercontent.com/291782/161783614-810d0d10-ffe0-483c-99c2-93a7f7159e2f.png)

          - X<sup>2</sup>-분포 (chi-square distribution, 카이제곱분포)

              - 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포
              - **두 집단 간의 동질성 검정에 활용**된다. (범주형 자료에 대해 얻어진 관측값과 기대값의 차이를 보는 적합성 검정에 활용)
              - ![x2-distribution](https://user-images.githubusercontent.com/291782/161784229-f6906799-74d1-4fd5-a6c6-06bcf9cadaaa.png)

          - F-분포 (F-distribution)

              - **두 집단간 분산의 동일성 검정**에 사용되는 검정 통계량 분포
              - 확률변수는 항상 양의 값만을 갖고 X<sup>2</sup> 분포와 달리 자유도를 2개 가지고 있으며 자유도가 커질수록 정규분포에 가까워진다.
              - ![f-distribution](https://user-images.githubusercontent.com/291782/161784584-b404679b-b455-40d9-91f8-1ffbb0589f98.png)

              



#### 5. 추정과 가설검정 (p.293)

##### 가. 추정의 개요

1.   확률표본 (random sample)
     -   확률분포는 분포를 결정하는 평균, 분산 등의 모수(parameter)를 가지고 있다.
     -   특정한 확률분포로부터 독립적으로 반복해 표본을 추출하는 것이다.
     -   각 관찰값들은 서로 독립적이며 동일한 분포를 갖는다.
2.   추정
     -   표본으로부터 미지의 모수를 추측하는 것이다.
     -   추정은 점추정 (point estimation)과 구간추정(interval estimation)으로 구분된다.
     -   점추정 (point estimation)
         -   '**모수가 특정한 값일 것**'이라고 추정하는 것이다.
         -   표본의 평균, 중위수, 최빈값 등을 사용한다.
         -   불편성 (unbiasedness) : 표본에서 얻는 추정량의 **기댓값**은 모집단의 모수와 차이가 없다.
         -   효율성 (efficiency) : 추정량의 분산이 작을수록 좋다.
         -   일치성 (consistency) : 표본의 크기가 아주 커지면, 추정량이 모수와 거의 같아진다.
         -   충족성 (sufficient) : 추정량은 모수에 대하여 모든 정보를 제공한다.
         -   표본평균 (sample mean) : 모집단의 평균(모평균)을 추정하기 위한 추정량. 확률표본의 평균값.
         -   $\overline{X} = \dfrac {1}{n}\displaystyle \sum_{i=1}^{n}X_i$
         -   표본분산 (sample variance) : 모집단의 분산 (모분산)을 추정하기 위한 추정량
         -   $S^2 = \dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n} (X_i - \overline{X})^2$
     -   구간추정 (interval estimation)
         -   **모수가 특정한 구간에 있을 것이라고 선언하는 것**이다.
         -   항상 추정량의 분포에 대한 전제가 주어져야 하고, 구해진 구간 안에 모수가 있을 가능성의 크기 (신뢰수준(confidence interval))가 주어져야 한다.
         -   95% 신뢰수준 하에서 모평균의 신뢰구간 (모분산을 알때는 분자에 &sigma;(시그마)를 넣고, 모분산을 모를땐 분자에 s를 넣는다.)
         -   모분산 &sigma;<sup>2</sup> 이 알려져 있는 경우
         -   $(\overline{X} - 1.96 \dfrac {\sigma}{\sqrt{n}}, \overline{X} + 1.96 \dfrac {\sigma}{\sqrt{n}})$ , 표준정규분포 N(0, 1)를 따르는 $Z = \dfrac {\overline{X} - \mu} {\dfrac {\sigma} {\sqrt{n}}}$
         -   모분산 &sigma;<sup>2</sup> 이 알려져 있지 않은 경우에는 모분산 대신 표본분산을 사용
         -   $(\overline{X} - 2.26 \dfrac {S}{\sqrt{n}}, \overline{X} + 2.26 \dfrac {S}{\sqrt{n}})$ , 자유도가 n-1인 t-분포를 따르는 $T = \dfrac {\overline{X} - \mu} {\dfrac {S} {\sqrt{n}}}$



##### 나. 가설검정

-   귀무가설 (null hypothesis, H<sub>0</sub>) : **비교하는 값과 차이가 없다, 동일하다**를 기본개념으로 하는 가설
-   대립가설 (alternative hypothesis, H<sub>1</sub>) : **뚜렷한 증거가 있을 때 주장하는 가설**
-   검정통계량 (test statistic) : 관찰된 표본으로부터 구하는 통계량, 검정 시 가설의 진위를 판단하는 기준
-   **유의수준** (significance level, &alpha;) : 귀무가설을 기각하게 되는 확률의 크기로 '귀무가설이 옳은데도 이를 기각하는 확률의 크기'
-   기각역 (critical region, C) : 귀무가설이 옳다는 전제하에서 구한 검정통계량의 분포에서 확률이 유의수준 &alpha;인 부분 (반대는 채택역 (acceptance region))
-   ![hypothesis](https://user-images.githubusercontent.com/291782/161790640-44316c2d-58db-4eb9-94e8-182c00887aa5.png)
-   ![alpha-beta](https://user-images.githubusercontent.com/291782/161790831-f7bb96a8-8f59-4a83-b303-0b85d53a9c76.png)





#### 6. 비모수 검정 (p.296)

모집단의 모수에 대한 검정은 **모수적 검정**과 **비모수적 검정**으로 구분한다.

##### 가. 모수적 방법

-   모집단의 분포에 대한 가정을 하고, 그 가정하에서 검정통계량과 검정통계량의 분포를 유도해 검정을 실시하는 방법

##### 나. 비모수적 방법

-   자료가 추출된 **모집단의 분포에 대한 아무 제약을 가하지 않고 검정을 실시**하는 방법
-   관측된 자료가 특정 분포를 따른다고 가정할 수 없는 경우에 이용
-   관측된 **자료의 수가 많지 않거나** (30개 미만), 자료가 개체간의 **서열관계를 나타내는 경우**에 이용

##### 다. 모수적 검정과 비모수적검정의 차이점

1.   가설의 설정
     -   모수적 검정 : 가정된 모수의 분포에 대해 가설을 설정
     -   비모수적 검정 : 가정된 분포가 없으므로 가설은 단지 '분포의 형태가 동일하다' 또는 '분포의 형태가 동일하지 않다'와 같이 **분포의 형태에 대해 설정**한다.
2.   검정 방법
     -   모수적 검정 : 관측된 자료를 이용해 구한 **표본평균, 표본분산** 등을 이용해 검정을 실시
     -   비모수적 검정 : 절대적인 크기에 의존하지 않고 **관측값들의 순위**(rank)나 **두 관측값 차이의 부호** 등을 이용해 검정

##### 라. 비모수 검정의 예

-   부호검정 (sign test), 윌콕슨의 순위합검정 (rank sum test), 윌콕슨의 부호순위합검정 (Wilcoxon signed rank test), 만-위트니의 U 검정, 런검정 (run test), 스피어만의 순위상관계수





### 2절 기초 통계분석

#### 1. 기술통계 (Descriptive Statistics) (p.298)
##### 가. 기술통계의 정의

- 자료의 특성을 표, 그림, 통계량 등을 사용하여 쉽게 파악할 수 있도록 정리/요약하는 것
- 대략저인 통계적 수치를 계산해봄으로써 데이터에 대한 대략적인 이해와 앞으로 분석에 대한 통찰력을 얻기 유리



##### 나. 통계량에 의한 자료 정리

1. 중심위치의 측도
   - 표본평균 (sample mean) : $\overline{X} = \dfrac {1}{n}(X_1 + X_2 + ... X_n) = \displaystyle \sum_{i=1}^{n} \dfrac{X_i}{n}$
   - 중앙값 (median) : 크기순으로 나열 시 중앙에 위치하는 값
     - n이 홀수인 경우 : $\dfrac {(n+1)}{2}$
     - n이 짝수인 경우 : $\dfrac {n}{2}$ 번째 값과 $\dfrac {(n+1)}{2} + 1$번째 값의 평균
2. 산포의 측도
   - 대표적인 산포도 (dispersion)는 분산, 표준편차, 범위 및 사분위수범위
   - 분산 : $S^2 = \dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n}(X_i - \overline{X})^2 = \dfrac {1}{n-1}(\displaystyle \sum_{i=1}^{n}X_i^2 - n\overline{X}^2) $
   - 표준편차 : $S = \sqrt{S^2} = \sqrt{\dfrac {1}{n-1} \displaystyle \sum_{i=1}^{n} (X_i - \overline{X})^2}$
   - 사분위수범위 (interquartile range) : IQR = Q3 - Q1
   - 사분위수 : Q1 (25백분위수), Q2 (50백분위수), Q3 (75백분위수)
   - 백분위수 (percentile) : $\dfrac {(n-1)p} {100 + 1}$번째 값
   - 변동계수 (coefficient of variation) : $V = \dfrac {S} {\overline{X}}$
   - 평균의 표준오차 : $SE(X) = \dfrac {S} {\sqrt{n}}$
   - ![percentile-iqr](https://user-images.githubusercontent.com/291782/162159496-84d2c7a6-f413-4748-ba66-ea6d5a62c71b.png)
3. 분포의 형태에 관한 측도
   - 왜도 (skewness) :분포의 비대칭정도를 나타내는 측도. 왜도가 양수인 경우 왼쪽에 밀집되어 있고, 오른쪽으로 긴 꼬리를 갖는 분포, 음수인 경우는 반대. 왜도가 0 일 경우는 좌우 대칭인 분포
     - ![skewness](https://user-images.githubusercontent.com/291782/162161124-9884c740-21f4-42f1-898f-997cfa30063e.png)
   - 첨도 (kurtosis) : 분포의 중심에서 뾰족한 정도를 나타내는 측도
     - ![kurtosis](https://user-images.githubusercontent.com/291782/162162023-a35083f7-7fbf-4389-9e61-7a552528df08.png)



##### 다. 그래프를 이용한 자료 정리

1. 히스토그램 : 도수분포표를 그래프로 나타낸 것
2. 막대그래프와 히스토그램의 비교
   - 막대그래프 : **범주형 (category)**으로 구분된 데이터 (예. 직업, 종교, 음식)를 표시하며 범주의 순서를 의도에 따라 바꿀수 있다.
   - 히스토그램 : **연속형(continuous)**으로 표시된 데이터 (키, 몸무게, 성적, 연봉)을 표현하며 임의의 순서를 바꿀수 없고 막대의 간격이 없다.



#### 2. 인과관계의 이해 (p.303)
##### 가. 용어

-   종속변수 (반응변수, y) : 다른 변수의 영향을 받는 변수
-   독립변수 (설명변수, x) : 영향을 주는 변수
-   산점도에서 확인할 사항
    -   두 변수 사이의 선형관계(직선관계)가 성립하는가?
    -   두 변수 사이의 함수관계(직선관계 또는 곡선관계)가 성립하는가?
    -   이상값이 존재하는가?
    -   몇 개의 집단으로 구분(층별)되는 가?



##### 나. 공분산 (covariance)

-   두 확률변수 X, Y의 방향성의 조합(선형성)이다.
-   공분산의 부호가 + 이면 양의 방향성, - 이면 음의 방향성을 가짐
-   X, Y가 서로 독립이면 $Cov (X, Y) = 0$ 이다.



#### 3. 상관분석 (Correlation Analysis)

##### 가. 상관분석 정의

-   두 변수의 상관관계를 알아보기 위해 상관계수 (correlation coefficient)를 이용하며, 그 공식은 아래와 같다.
-   $r = \dfrac {cov(x, y)} {S_x ⅹ S_y} = \dfrac {\displaystyle \sum_{i=1}^n [(x - \overline{x})(y - \overline{y})]} {n(S_x ⅹ S_y)}$



##### 나. 상관관계의 특성

-   $0.7 \lt  r \le 1$ : 강한 양(+)의 상관관계, $0 \lt  r \le 0.3$ : 거의 상관 음다. ,r = 0 : 상관관계(선형, 직선)가 없다.
-   $-1 \le  r \lt 0.7$ : 강한 음(-)의 상관관계, $-0.7 \le  r \lt 0.3$ : 약한 음(-)의 상관관계



##### 다. 상관분석의 유형

|   구분   | 피어슨                                                  | 스피어만                                                  |
| :------: | ------------------------------------------------------- | --------------------------------------------------------- |
|   개념   | 등간척도 이상으로 측정된 두 변수들의 상관관계 측정 방식 | 서열척도인 두 변수들의 상관관계 측정 방식                 |
|   특징   | 연속형 변수, 정규성 가정, 대부분 많이 사용              | 순서형 변수, 비모수적 방법, 순위를 기준으로 상관관계 측정 |
| 상관계수 | 피어슨 $r$ (적률상관계수)                               | 순위상관계수 $(p, 로우)$                                  |

>   피어슨 스피어만 구분 Tip : 스피어만, 서열척도, 순서, 순위상관계수 등의 단어는 모두 "ㅅ" 으로 시작



##### 라. 상관분석을 위한 R코드

```R
# 분산
var(x,y = NULL, na.rm = FALSE)

# 공분산
cov(x,y = NULL, use="everything",
   method = c("pearson", "kendall", "spearman"))

# 상관관계
cor(x,y = NULL, use = "everything",
   method = c("pearsono", "kendall", "spearman"))
# 상관관계 Hmisc 패키지의 rcorr 사용
rcorr(matrix(data명), type=c("pearson", "kendall", "spearman"))
```



##### 마. 상관분석의 가설 검정

-   상관계수 $r$이 0이면 입력변수 x와 출력변수 y 사이에는 아무런 관계가 없다. (귀무가설: $r = 0$, 대립가설 $r\neq 0$)
-   t 검정통계량을 통해 얻은 p-value 값이 0.05이하인 경우, 대립가설을 채택하게 되어 우리가 구한 상관계수를 활용할 수 있게 됨



##### 바. 상관분석 예제

mtcars 데이터셋의 마일(mpg), 총마력(hp)의 상관관계 분석

```R
> data("mtcars")
> a <- mtcars$mpg
> b <- mtcars$hp
> cov(a, b)
[1] -320.7321
> cor(a, b)
[1] -0.7761684
> cor.test(a, b, method="pearson")

	Pearsons product-moment correlation

data:  a and b
t = -6.7424, df = 30, p-value = 1.788e-07
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.8852686 -0.5860994
sample estimates:
       cor 
-0.7761684 
```

-   분석결과 : 공분산 (covariance)은 -320.73, 상관계수 (correlation coefficient)는 -0.776
-   따라서 mpg와 hp는 음의 방향성을 가지며, 상관계수로 강한 음의 상관관계가 있음을 알 수 있음
-   cor.test를 이용해 나온 p-value 가 1.788e-07로 유의수준 0.05보다 작게 나타나므로 mpg와 hp가 상관관계가 있다고 할 수 있다.



### 3절 회귀분석 (중요. 3 ~ 5문제)

#### 1. 회귀분석의 개요 (p.309)

##### 가. 회귀분석 정의

- 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법
- 변수들 사이의 인과관계를 밝히고 모형을 적합하여 관심있는 변수를 예측하거나 추론하기 위한 분석방법
- 독립변수 개수가 하나이면 단순선형회귀분석, 독립변수가 두 개 이상이면 다중선형회귀분석으로 분석 할 수 있다.



##### 나. 회귀분석의 변수

- 영향을 받는 변수 (y) : 반응변수 (response variable), 종속변수 (dependent variable), 결과변수 (outcome variable)
- 영향을 주는 변수 (x) : 설명변수 (explanatory variable), 독립변수 (independent variable), 예측변수 (predicator variable)



##### 다. 선형회귀분석의 가정

- **선형성** : 입력변수와 출력변수의 관계가 선형이다
- 등분산성 : 오차의 분산이 입력변수와 무관하게 일정하다.
- 독립성 : 입력변수와 오차는 관련이 없다. 독립성을 알아보기 위해 Dubrin-Watson 통계량 사용. 주로 시계열 데이터에서 많이 활용
- 비상관성 : 오차들끼리 상관이 없다.
- 정상성 (정규성) : 오차의 분포가 정규분포를 따른다. Q-Q plot, Kolmogolov-Smirnov 검정, Shaprio-Wilk 검정 등을 활용하여 정규성을 확인



##### 라. 그래프를 활용한 선형회귀분석의 가정 검토

| 선형성                                                       | 등분산성                                                     | 정규성                                                       |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![linear](https://user-images.githubusercontent.com/291782/162400779-89b3cc2b-e948-4f2e-a217-8251396f5225.png) | ![residuals](https://user-images.githubusercontent.com/291782/162400366-c11a9068-d55f-4f77-aebb-3432abe8678f.png) | ![normal-qqplot](https://user-images.githubusercontent.com/291782/162400224-35785847-68a1-4025-8ac9-8b9dca8fc9f0.png) |



##### 마. 가정에 대한 검증

- 단순선형회귀분석 : 입력변수와 출력변수간 선형성을 점검하기 위해 산점도를 확인
- 다중선형회귀분석 : 선형회귀분석의 가정인 선형성, 등분산성, 독립성, 정상성이 모두 만족하는지 확인



#### 2. 단순선형회귀분석 (p.311)

- 독립변수가 종속변수에 미치는 영향을 추정하는 통계 기법
- ![linear-regression](https://user-images.githubusercontent.com/291782/162449647-a02d3869-0d0b-4c51-a083-90a63717c624.png)



##### 가. 회귀분석에서의 검토사항

-   회귀계수들이 유의미한가? 
    -   해당 계수의 t 통계량의 p-value 가 0.05보다 작으면 해당 회귀계수가 통계적으로 유의미하다고 볼 수 있다.
-   모형이 얼마나 설명력을 갖는가?
    -   결정계수(R<sup>2</sup>)를 확인한다. 결정계수는 0 ~ 1 값을 가지며, 높은 값을 가질수록 추정된 회귀식의 설명력이 높다.
-   모형이 데이터를 잘 적합하고 있는가?
    -   잔차를 그래프로 그리고 회귀진단을 한다.



##### 나. 회귀계수의 추정 (최소제곱법, 최소자승법)



##### 다. 회귀분석의 검정

1.   회귀계수의 검정

-   10년간의 에어컨 예약대수와 판매대수 (단위: 1,000대)

| 예약대수(X) | 19   | 23   | 26   | 29   | 30   | 38   | 39   | 46   | 49   |
| ----------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 판매대수(Y) | 33   | 51   | 40   | 49   | 50   | 69   | 70   | 64   | 89   |

위 데이터에 대한 단순회귀분석을 실시

```R
> x <- c(19, 23, 26, 29, 30, 38, 39, 46, 49)
> y <- c(33, 51, 40, 49, 50, 69, 70, 64, 89)
> lm(y~x) # linear regression

Call:
lm(formula = y ~ x)

Coefficients:
(Intercept)            x  
      6.409        1.529  

> summary (lm(y~x))

Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-12.766  -2.470  -1.764   4.470   9.412 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.4095     8.9272   0.718 0.496033    
x             1.5295     0.2578   5.932 0.000581 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.542 on 7 degrees of freedom
Multiple R-squared:  0.8341,	Adjusted R-squared:  0.8104 
F-statistic: 35.19 on 1 and 7 DF,  p-value: 0.0005805
```

-   X의 회귀계수인 t 통계량에 대한 p-value 가 0.0005805로 나타나, 유의수준 0.05보다 작으므로 회귀계수의 추정치들이 통계적으로 유의함
-   결정계수 (R<sup>2</sup>) 0.8341로 높게 나타나 회귀식이 데이터를 적절하게 설명하고 있다 할 수 있음
-   회귀분석 결과 "판매대수 = 6.409 + 1.529 * 예약대수"의 회귀식을 구할 수 있다.



2.   결정계수

![r2](https://user-images.githubusercontent.com/291782/162452511-f3bca308-98f2-4dd9-a7d4-fffad044fe47.png)

-   전체제곱합 (total sum of squares, SST) : $\displaystyle \sum_{i=1}^n(y_i - \overline{y})^2$
-   회귀제곱합 (regression sum of squares, SSR) : $\displaystyle \sum_{i=1}^n(\hat{y}_i - \overline{y})^2$
-   오차제곱합 (error sum of squares, SSE) : $\displaystyle \sum_{i=1}^n(y_i - \hat{y})^2$
-   결정계수 (R<sup>2</sup>)는 전제제곱합 (SST)에서 회귀제곱합 (SSR)의 비율 (SSR / SST), 0 &le; R<sup>2</sup> &le; 1 (SST = SSR + SSE)



3.   회귀직선의 적합도 검토

     -   R2는 독립변수가 종속변수의 변동을 몇 %를 설명하는지 나타내는 지표
     -   다변량 회귀분석에서는 독립변수의 수가 많아지면 R2 가 높아지므로 독립변수가 유의하든 않든 R2가 높아지는 단점이 있음
     -   위 단점을 보완하기 위해 수정 결정계수 (R<sub>a</sub><sup>2</sup>: adjusted R<sup>2</sup>)를 활용한다. 수정결정 계수는 결졍계수보다 작은 값으로 산출되는 특징
     -   수정결정계수 : $1 - \dfrac {(n-1)(1 - R^2)}{n - k - 1} = 1 - \dfrac {(n-1) Ⅹ (\dfrac {SSE} {SST})} {n - k - 1} = 1 - (n - 1) \dfrac {MSE} {SST}$
     -   (k: 독립변수 개수, n : 데이터의 개수)
     -   오차(error) : 모집단에서 실제값이 회귀선과 비교해 볼 때 나타나는 차이 (정확치와 관측지의 차이)
     -   잔차 (residual) : 표본에서 나온 관측값이 회귀선과 비교해볼 때 나타나는 차이

     

     

     #### 3. 다중선형회귀분석 (p.315)

     ##### 가. 다중선형회귀분석 (다변량회귀분석)

     -   다중회귀식 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon$

     -   모형의 통계적 유의성

         -   모형의 통계적 유의성은 F-통계량으로 확인

         -   유의수준 5% 하에서 F-통계량의 p-value가 0.05보다 작으면 통계적으로 유의함

         -   F-통계량이 크면 p-value가 0.05보다 작아지고 이렇게 되면 귀무가설을 기각한다.

             >   귀무가설 : $H_0: \beta_1 = \beta_2 = ... \beta_k = 0$ vs 대립가설 : $H_1 : \beta_1 \neq \beta_2 \neq ... \neq \beta_k \neq 0$

             | 요인 |      제곱합      | 자유도 |      제곱평균       |   F-통계량    |
             | :--: | :--------------: | :----: | :-----------------: | :-----------: |
             | 회귀 | 회귀제곱합(SSR)  |   k    |    MSR = SSR / k    | F = MSR / MSE |
             | 오차 | 오차제곱합(SSE)  | n-k-1  | MSE = SSE / (n-k-1) |               |
             |  계  | 전체제곱합 (SST) |  n-1   |                     |               |

         -   모형의 설명력은 결정계수(R<sup>2</sup>)나 수정결정계수(R<sub>a</sub><sup>2</sup>)를 확인
         -   모형의 적합성 : 잔차와 종속변수의 산점도로 확인
         -   데이터가 전제하는 가정을 만족하는가? 선형성, 독립성, 등분산성, 비상관성, 정상성
         -   다중공선성 (multicollinearity)
             -   다중회귀분석에서 설명변수들 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란
             -   다중공선성 검사방법
                 -   분산팽창요인 (VIF) : 4보다 크면 다중공선성이 존재, 10보다 크면 심각한 문제가 있는것으로 해석
                 -   상태지수 : 10이상이면 문제가 있음, 30보다 크면 심각한 문제가 있음

     

3.   #### 4. 회귀분석의 종류 (p.316)

     -   단순회귀 : $Y = \beta_0 + \beta_1X + \epsilon$ : 독립변수가 1개이며 종속변수와의 관계가 직선
     -   다중회귀 : $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_kX_k + \epsilon $ : 독립변수가 k개 이며 종속변수와의 관계가 선형 (1차함수)
     -   로지스틱회귀 : $P(y) = \dfrac {1} {1 + exp[-(\beta_0 + \beta_1X_1 + ... + \beta_kX_k + \epsilon)]}$  : 종속변수가 범주형(2진변수)인 경우에 적용되며, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장될 수 있음
     -   다항회귀 : K=2이고 2차 함수인 경우
         -   $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_{11}X_1^2 + \beta_{22}X_2^2 + \beta_{12}X_1X_2 + \epsilon$ 
         -   독립변수와 종속변수와의 관계가 1차함수 이상인 관계 (단, k=1이면 2차 함수 이상)
     -   곡선회귀
         -   2차 곡선인 경우 : $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon $
         -   3차 곡선인 경우 : $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon $
         -   독립변수가 1개이며 종속변수와의 관계가 곡선
     -   비선형회귀 : $Y = \alpha e^{-\beta X} + \epsilon $ : 회귀식의 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형



#### 5. 회귀분석 사례 (p.316)



#### 6. 최적회귀방정식 (p.319)

-   읽었는데 잘 이해가 안됨
-   ==F-Value, T-value, AIC, BIC 다시 공부해라 (중요)==
-   



### 4절 시계열 분석

#### 1. 시계열 자료 (p.328)

##### 나. 시계열 자료의 종류

- 비정상성 시계열 자료 : 시계열 분석을 실시할 떄 다루기 어려운 자료로 대부분의 시계열 자료가 이에 해당
- 정상성 시계열 자료 : 비정상성 시계열을 핸들링해 다루기 쉬운 시계열 자료로 변환한 자료



#### 2. 정상성 (p.328)

정상성은 평균이 일정, 분산이 일정, 공분산도 단지 시차에만 의존하고 실제 특정 시점 t, s에는 의존하지 않을 떄 만족함

##### 가. 평균이 일정할 경우

- 모든 시점에 대해 일정한 평균을 가짐
- 평균이 일정하지 않은 **시계열은 차분(difference)**을 통해 정상화 할 수 있다.

> 차분이란?
>
> - 현시점 자료에서 전시점 자료를 빼는 것
> - 일반차분 (regular difference) : 바로 전 시점의 자료를 빼는 방법
> - 계절차분 (seasonal difference) : 여러 시점 전의 자료를 뺴는 방법, 주로 계절성을 갖는 자료를 정상화 하는데 사용



##### 나. 분산이 일정

- 분산도 시점에 의존하지 않고 일정해야함
- 분산이 일정하지 않을 경우 **변환 (transformation)을 통해 정상화** 할 수 있다.



##### 다. 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않는다.     



#### 3. 시계열 자료 분석방법 (p.329)

##### 가. 분석방법

- 회귀분석 (계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등이 있다.



##### 나. 자료 형태에 따른 분석 방법

- 일변량 시계열 분석
    - Box-Jenkins (ARMA), 지수 평활법, 시계열 분해법 등이 있다.
    - 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심을 갖는 경우의 시계열 분석
- 다중 시계열
    - 계량경제 모형, 전이함수 모형, 개입분석, 상태공간 분석, 다변량 ARIMA 등
    - 여러개의 시간(t)에 따른 변수들을 활용하는 시계열 분석
    - 예) 이자율, 인플레이션이 환율에 미치는 요인



##### 다. 이동평균법

- 개념
    - 추세를 파악하여 다음 기간을 예측하는 방법
    - n개의 시계열 데이터를 m기간으로 이동평균하면 n-m+1개의 이동평균 데이터가 생성된다.
- 특징
    - 간단하고 쉽게 미랠르 예측가능, 자료의 수가 많고 안정된 패턴을 보이는 경우 예측의 품질이 높음
    - 특정 기간안에 속하는 시계열에 대해서는 동일한 가중치를 부여
    - 불규칙변동이 심하지 않은 경우에는 짧은 기간(m의 개수가 적음), 반대로 불규칙변동이 심한 경우 긴 기간 (m의 개수가 많음)의 평균을 사용
    - 이동평균에서 가장 중요한 것은 적절한 기간을 사용하는 것. 즉, 적절한 n의 개수를 결정하는 것.



##### 라. 지수평활법 (Exponential Smoothing)

- 개념
    - 일정기간의 평균을 이용하는 이동평균법과 달리 모든 시계열 자료를 사용하여 평균을 구하며, 시간의 흐름에 따라 최근 시계열에 더 많은 가중치를 부여하여 미래를 예측하는 방법
    - <img width="593" alt="es" src="https://user-images.githubusercontent.com/291782/162599656-cb8a5076-7f8d-4160-a29c-223be2fd0570.png">
    - 여기서 F<sub>n+1</sub>은 n시점 다음의 예측값, &alpha;는 지수평활계수, Z<sub>n</sub>은 n시점의 관측값이며, 지수평활계수가 과거로 갈수록 지수형태로 감소하는 형태인 것을 확인할 수 있음
    - 다음은  [블로그](https://ko.logpresso.com/documents/time-series) 글을 참조하여 조금 더 쉽게 설명
    - $S_t = \alpha Y_{t-1} + (1-\alpha)S_{t-1}$
    - S<sub>t</sub> : 다음 예측치, Y<sub>t-1</sub> : 현재 값, S<sub>t-1</sub> : 이전 예측치, &alpha; : 0보다 크고 1보다 작은 스무딩 매개 변수
- 특징
    - 단기간에 발생하는 불규칙변동을 평활하는 방법
    - 자료의 수가 많고, 안정된 패턴을 보이는 경우일수록 예측 품질이 높음
    - 지수평활법에서 가중치의 역할을 하는 것은 지수평활계수(&alpha;)이며, 불규칙변동이 큰 시계열의 경우 지수평활계수는 작은 값을, 불규칙변동이 작은 시계열의 경우, 큰 값의 지수평활계수를 적용(generally, &alpha; is between 0.05 and 0.3)
    - 지수평활계수는 예측오차 (실제 관측치와 예측치 사이의 잔차제곱합)를 비교하여 예측오차가 가장 작은 값을 선택하는 것이 바람직함
    - 지수평활계수는 과거로 갈수록 지속적으로 감소함
    - 지수평활법은 불규칙변동의 영향을 제거하는 효과가 있으며, 중기 예측 이상에 주로 사용됨
    - 단, 단순지수 평활법의 경우, 장기추세나 계절변동이 포함된 시계열의 예측에는 적합하지 않음



#### 4. 시계열모형 (p.331)

##### 가. 자기회귀 모형 (AR모형, autoregressive model)

- p 시점 전의 자료가 현재 자료에 영향을 주는 모형

- $Z_t = \Phi_1Z_{t-1} + \Phi_2Z_{t-2} + ... + \Phi_pZ_{t-p} + \alpha_t$

- >- Z<sub>t</sub> : 현재 시점의 시계열 자료
    >- Z<sub>t-1</sub>, Z<sub>t-2</sub> ..., Z<sub>p</sub> : 이전, 그 이전 시점 p의 시계열 자료
    >- &Phi;<sub>p</sub> : p 시점이 현재에 어느 정도 영향을 주는지를 나타내는 모수
    >- &alpha;<sub>t</sub> : 백색잡음과정 (white noise process) : 시계열분석에서 오차항을 의미
    >- 평균이 0, 분산이 &sigma;<sup>2</sup>, 자기공분산이 0인 경우를 뜻하며, 시계열간 확률적 독립인 경우 강(strictly) 백색잡음 과정이라고 한다. 백색잡음 과정이 정규분포를 따를 경우 이를 가우시안(Gaussian) 백색잡음과정이라고 한다.

- AR(1) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &alpha;<sub>t</sub>, 직전 시점 데이터로만 분석

- AR(2) 모형 : Z<sub>t</sub> = &Phi;<sub>1</sub>Z<sub>t-1</sub> + &Phi;<sub>2</sub>Z<sub>t-2</sub> + &alpha;<sub>t</sub>, 연속된 2시점 정도의 데이터로 분석

- AR(2) 모형의 자기상관함수(ACF)와 편자기상관함수(PACF)

- <img width="584" alt="acf-pacf" src="https://user-images.githubusercontent.com/291782/162600723-a762b3b5-6461-4b8c-972e-c100914b3da5.png">



##### 나. 이동평균 모형 (MA 모형, Moving Average model)

- 유한한 개수의 백색잡음의 결합이므로 언제나 정상성을 만족
- 1차 이동평균모형 (MA1 모형)은 이동평균모형 중에서 가장 간단한 모형으로 시계열이 같은 시점의 백색잡음과 바로 전 시점의 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub> -  &phi;<sub>2</sub>&alpha;<sub>t-2</sub> - ... -  &phi;<sub>p</sub>&alpha;<sub>t-p</sub> 
- 2차 이동평균모형 (MA2 모형)은 바로 전 시점의 백색잡음과 시차가 2인 백색잡음의 결합으로 이뤄진 모형
- Z<sub>t</sub> = &alpha;<sub>t</sub> - &phi;<sub>1</sub>&alpha;<sub>t-1</sub>
- AR모형과 반대로 ACF에서 절단점을 갖고, PACF가 빠르게 감소
- $Z_t = \alpha_t - \phi1\alpha_{t-1} - \phi_2\alpha_{t-2}$



##### 다. 자기회귀누적이동평균 모형 (ARIMA(p,d,q) 모형, autoregressive integrated moving average model)

- ARIMA 모형은 비정상시계열 모형이다

- ARIMA 모형은 차분이나 변환을 통해 AR모형이나 MA모형, 이 둘을 합친 ARMA 모형으로 정상화 할 수 있다.

- p는 AR모형, q는 MA모형과 관련이 있는 차수

- 시계열 {Z<sub>t</sub>}의 d번 차분한 시계열이 ARMA (p, q) 모형이면, 시계열 {Z<sub>t</sub>}는 차수가 p,d,q인 ARIMA 모형, 즉 ARIMA(p,d,q) 모형을 갖는다고 한다.

- d=0이면 ARMA(p, q) 모형이라 부르고, 이 모형은 정상성을 만족한다. (ARMA (0 , 0)일 경우 정상화가 불필요)

- p=0 이면 IMA (d, q) 모형이라 부르고, d번 차분하면 MA(q) 모형을 따른다.

- q = 0이면 ARI (p, d) 모형이라 부르며, d번 차분한 시계열이 AR (p) 모형을 따른다.

    > ARIMA (0, 1, 1)의 경우에는 1차분 후 MA(1) 활용
    >
    > ARIMA (1, 1, 0)의 경우에는 1차분후 AR(1) 활용
    >
    > ARIMA (1, ,1 2)의 경우에는 1차분 후 AR(1), MA(2), ARMA (1, 2) 선택 활용
    >
    > => 이런 경우 가장 간단한 모형을 선택하거나 AIC 를 적용하여 점수가 가장 낮은 모형을 선정



##### 라. 분해 시계열

- 시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법을 말하며 회귀분석적인 방법을 주로 사용

- 분해식의 일반적 정의 : $Z_t = f(T_t, S_t, C_t, I_t)$

    > T<sub>t</sub> : 경향(추세)요인 : 자료가 오르거나 내리는 추세, 선형, 이차식 형태, 지수적 형태 등
    >
    > S<sub>t</sub> : 계절요인 : 요일, 월, 사계절 각 분기에 의한 변화 등 고정된 주기에 따라 자료가 변하는 경우
    >
    > C<sub>t</sub> : 순환요인 : 경제적이나 자연적인 이유 없이 알려지지 않은 주기를 가지고 변화하는 자료
    >
    > I<sub>t</sub> : 불규칙요인 : 위의 세 가지 요인으로 설명할 수 없는 오차에 해당하는 요인



##### 마. R을 이용한 시계열 분석

- 맥북 documents의 R 폴더 확인



### 5절 다차원척도법

#### 1. 다차원척도법 (multidimensional scaling) (p.340)

- 객체간 근접성 (proximilty)을 시각화 하는 통계기법
- 개체들을 대상으로 변수들을 측정한 후 개체들 사이의 유사성 / 비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석방법
- <img width="659" alt="multi-dimension-scaling" src="https://user-images.githubusercontent.com/291782/162624740-2e70ee2b-4e6a-44fb-bcf8-30d0a34a0e49.png">



#### 2. 다차원척도법 목적

- 데이터속에 잠재해 있는 패턴(pattern), 구조를 찾아냄
- 소수차원의 공간에 기하학적으로 표현
- 데이터 축소 (data reduction)의 목적으로 다차원척도법을 이용. 즉 데이터에 포함되는 정보를 끄집어내기 위해서 다차원척도법을 탐색수단으로써 사용



#### 3. 다차원척도법 방법

- 개체들의 거리 계산에는 **유클리드 거리행렬을 활용**한다.

    $d_{ii} = \sqrt{(x_{il} - x_{il})^2 + ... + (x_{iR} - x_{ir})^2}$

- 최적모형의 적합은 부적합도를 최소로 하는 방법으로 일정 수준이하로 될 때까지 반복해서 수행

- |   STRESS    |         적합도 수준          |
    | :---------: | :--------------------------: |
    |      0      |        완벽 (perfect)        |
    |  0.05 이내  |    매우 좋은 (excellent)     |
    | 0.05 ~ 0.10 |     만족 (satisfactory)      |
    | 0.10 ~ 0.15 | 보통 (acceptable, but doubt) |
    |  0.15 이상  |         나쁨 (poor)          |



#### 4. 다차원척도법 종류 (p.341)

##### 가. 계량적 MDS (Metric MDS)

- 데이터가 **구간척도나 비율척도인 경우 활용**한다. (전통적인 다차원척도법)



##### 나. 비계량적 MDS (nonmetric MDS)

- 데이터가 **순서척도**인 경우 활용. 개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환 (monotone transformation)하여 거리를 생성한 후 적용



### 6절 주성분 분석

#### 1. 주성분분석 (Principal Component Analysis) (p.345)

- 여러 변수들의 변량을 '주성분 (principal component)'이라는 **서로 상관성이 높은 변수들의 선형 결합**으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법
- 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로 첫 번째 주성분과 상관성이 없어서(낮아서) 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만듦



#### 2. 주성분분석의 목적

- 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 해줌
- 다중공선성이 존재하는 경우, **상관성이 없는(적은) 주성분으로 변수들을 축소**하여 모형 개발에 활용된다. **회귀분석** 등의 모형 개발 시 입력변수들간의 상관관계가 높은 **다중공선성(multicollinearity)**이 존재할 경우 모형이 잘 못 만들어져 문제가 생김
- 차원을 축소한 후에 **군집분석을 수행하면 군집화 결과와 연산속도를 개선**할 수 있다.



#### 3. 주성분분석 vs 요인분석

##### 가. 요인분석(factor analysis)

- 등간척도 (혹은 비율척도)로 측정한 두 개 이상의 변수들에 잠재되어 있는 공통인자를 찾아내는 기법



##### 나. 공통점

- 모두 데이터를 축소하는데 활용됨. 원래 데이터를 활용해서 몇 개의 새로운 데이터를 만들 수 있다.



##### 다. 차이점

1. 생성된 변수의 수
    - 요인분석은 몇 개라고 지정 없이 (2 or 3, 4, 5..) 만들 수 있다.
    - 주성분분석은 제1주성분, 제2주성분, 제3주성분 정도로 활용된다. (대개 4개 이상은 넘지 않음)
2. 생성된 변수의 이름
    - 요인분석은 분석자가 요인의 이름을 명명한다.
    - 주성분분석은 주로 제1주성분, 제2주성분 등으로 표현
3. 생성된 변수들 간의 관계
    - 요인분석은 새 변수들은 대등한 관계를 갖고 '어떤 것이 더 중요하다'라는 의미는 없다. 단, 분류/예측에 그 다음 단계로 사용된다면 그 때 중요성의 의미가 부여된다.
    - 주성분분석은 제1주성분이 가장 중요하고, 그 다음 제2주성분이 중요하게 취급
4. 분석 방법의 의미
    - 요인분석은 목표변수를 고려하지 않고 그냥 데이터가 주어지면 변수들을 비슷한 성격들로 묶어서 새로운 [잠재] 변수들을 만든다.
    - 주성분분석은 목표 변수를 고려하여 목표 변수를 잘 예측/분류하기 위하여 원래 변수들의 선형 결합으로 이루어진 몇 개의 주성분(변수)들을 찾아내게 된다.



#### 4. 주성분의 선택법 (p.346)

- 주성분분석의 결과에서 누적기여율(cumulative proportion)이 85% 이상이면 주성분의 수로 결정할 수 있다.
- <img width="643" alt="pca-ex1" src="https://user-images.githubusercontent.com/291782/162625936-20559085-98d6-49f3-831f-8e9310c8d8c9.png">
- <img width="372" alt="scree-plot" src="https://user-images.githubusercontent.com/291782/162625976-440be6b8-33af-4388-aced-f7c9f5becc6c.png">



#### 5. 주성분 분석 사례 (p.347)

- USArrests 자료를 이용한 분석





### 5장 정형 데이터 마이닝

### 1절 데이터마이닝의 개요

#### 1. 데이터마이닝 (p.385)

##### 가. 개요

- 데이터마이닝은 대용량 데이터에서 의미있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법



##### 나. 통계분석과의 차이점

- 통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 데이터마이닝은 다양한 수리 알고리즘을 이용해 **데이터베이스의 데이터로부터 의미있는 정보를 찾아내는 방법을 통칭**



##### 다. 종류

- 정보를 찾는 방법론에 따른 종류
    - 인공지능 (Artificial Intelligence)
    - 의사결정나무 (Decision Tree)
    - K-평균군집화 (K-means clustering)
    - 연관분석 (Association Rule)
    - 회귀분석 (Regression)
    - 로짓분석 (Logit Analysis)
    - 최근접이웃 (Nearest Neighborhood)
- 분석대상, 활용목적, 표현방법에 따른 분류
    - 시각화분석 (Visualization Analysis)
    - 분류 (Classification)
    - 군집화 (Clustering)
    - 포케스팅 (Forecasting)



#### 2. 데이터마이닝의 분석방법

- 지도학습 (Supervisied Learning)
    - 의사결정나무 (DT), 인공신경망 (Artifician Neural Network), 일반화 선형 모형 (GLM, Generalized Linear Model)
    - 회귀분석 (regression analysis), 로지스틱 회귀분석 (logistic regression analysis), 사례기반 추론 (case-based reasoning), 최근접이웃법 (KNN)
- 비지도학습 (Unsupervised Learning)
    - OLAP (On-Line Analytical Processing), 연관성 규칙발견 (Association Rule Discovery, Market Basket)
    - 군집분석 (K-Means Clustering), SOM (Slef Organizing Map)



#### 3. 분석 목적에 따른 작업 유형과 기법

- 예측 (Predictive Modeling)
    - 분류규칙 (Classification) : 과거의 데이터로부터 고객 특성을 찾아내어 분류모형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것으로 목표 마케팅 및 고객 신용평가 모형에 활용됨 (사용기법 : 회귀분석, 판별분석, 신경망, 의사결정나무)
- 설명 (Descriptive Modeling)
    - 연관규칙 (Association) : 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업. 제품이나 서비스의 교차판매 (cross selling), 매장진열 (display), 첨부우편 (attached mailings), 사기적발 (fraud detection) 등의 다양한 분야에 활용됨 (사용기법 : 동시발생 매트릭스)
    - 연속규칙 (sequence) : 연관 규칙에 시간관련 정보가 포함된 형태. 고객의 구매이력 (history) 속성이 반드시 필요. 목표 마케팅 (target marketing), 일대일 마케팅 (one to one marketing)에 활용. (사용기법 : 동시발생 매트릭스)
    - 데이터 군집화 (Clustering) : 고객 레코드들을 유사한 특성을 지닌 몇개의 소그룹으로 분할하는 작업. 작업의 특성이 분류규칙 (Classification)과 유사하나 분석대상 데이터에 결과 값이 없으며, 판촉활동이나 이벤드 대상을 선정하는데 활용 (사용기법 : K-Means Clustering)



#### 4. 데이터마이닝 추진 단계 (p.387)

1. 목적 설정
2. 데이터 준비
3. 가공
4. 기법적용
5. 검증



#### 5. 데이터마이닝을 위한 데이터 분할

- 구축용 (training data, 50%) : 추정용, 훈련용 데이터라고도 불리며 데이터마이닝 모델을 만드는데 활용
- 검정용 (validation data, 30%) : 구축과 모형의 과대추정 또는 과소추정을 미세 조정하는데 활용
- 시험용 (test data, 20%) : 모델의 성능을 검증하는데 활용
- 데이터의 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우
    - 홀드아웃(hold-out) 방법 : 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용하는 방법. 학습용 데이터와 시험용으로 분리
    - 교차확인 (cross-validation) 방법 : 주어진 데이터를 k개의 하부집단으로 구분하여, k-1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습. k번 반복 측정한 결과를 평균낸 값을 최종적으로 사용. k-fold 교차분석을 주로 많이 사용.



#### 6. 성과분석 (p.389)

##### 가. 오분류에 대한 추정치

- |      |          | 예측     | 예측     |      |
  | ---- | -------- | -------- | -------- | ---- |
  |      |          | Positive | Negative |      |
  | 실제 | Positive | TP       | FN       | P    |
  | 실제 | Negative | FP       | TN       | N    |

  - 참긍정률(TPR)  = $\dfrac{TP}{TP+FN}$ = 재현율(Recall) = 민감도(Sensitive) = ROC의 세로축
  - 거짓긍정률(FPR) = $\dfrac {FP}{FP+TN}$ = (1 - 특이도(Specificity)) = ROC의 가로축
  - 정확도(Accuracy, 정분류율) = $\dfrac {TP+TN}{P+N}$
  - 오분류율(Error Rate) : $1 - Accuracy = \dfrac {FN + FP} {P + N}$
  - 정밀도(Precision) = $\dfrac {TP}{TP+FP}$
  - 재현도(Recall) = 민감도(Sensitive) = $\dfrac {TP}{TP+FN}$ = FPR(참긍정률)
  - 특이도(Specificity, TNR, True Negative Rate) = $\dfrac{TN}{TN+FP}$
  - F1-Score 에 들어가는 지표는? 정밀도(Precision) 와 재현율(Recall, 민감도)
    - 식 = $2 × \dfrac {Precision × Recall}{Precision + Recall}  $ 
    - 재현율과 정밀도 값이 모두 클 때 F1-Score도 큰 값을 가진다
    - F1-Score는 민감도와 정밀도를 합한 **성능평가지표**로 0~1 사이의 값을 가진다. 1이 좋음



##### 나. ROCR 패키지로 성과분석

1. ROC Curve (Receiver Operating Characteristic Curve)
   - ROC 커브란 가로축을 FPR (False Positive Rate = 1 - 특이도) 값으로 두고, 세로축을 TPR (True Positive Rate, 민감도) 값으로 두어 시각화한 그래프
   - 2진 분류 (binary classfication)에서 모형의 성능을 평가하기 위해 많이 사용되는 척도
   - ROC 곡선 아래의 면적을 의미하는 AUROC (Area Under ROC) 값이 크면 클수록 (1에 가까울수록 ) 모형의 성능이 좋다고 평가.
   - AUROC : 0.9 ~ 1.0 (excellent), 0.8 ~ 0.9 (good), 0.7 ~ 0.8 (fair), 0.6 ~ 0.7 (poor), 0.5 ~ 0.6 (fail)



##### 다. 이익도표 (Lift chart)

1. 이익도표의 개념

   - 이익도표는 분류모형의 성능을 평가하기 위한 척도로, 분류된 관측치에 대해 얼마나 예측이 잘 이루어졌는지를 나타내기 위해 임의로 나눈 각 등급별로 반응검출율, 반응률, 리프트 등의 정보를 산출하여 나타내는 도표

2. 이익도표의 활용 예시

   ![lift-chart](https://user-images.githubusercontent.com/291782/162685389-03b19520-81e3-49ca-b533-8c816820138f.png)

   - 전체 2000명 중 381명 구매
   - Frequency of 'buy' : 2000명 중 실제로 구매한 사람
   - % Captured of response : 반응검출율: 해당 등급의 실제 구매자 / 전체 구매자
   - % response : 반응률 = 해당 등급의 실제 구매자 / 200명 (=2000명 / 10구간)
   - Lift : 향상도 : 반응률 / 기본 향상도 (좋은 모델이라면 Lift가 빠른 속도로 감소해야 한다.)

   

   







### 2절 분류분석

#### 1. 분류분석과 예측분석 (p.396)

##### 가. 분류분석의 정의

- 데이터가 어떤 그룹에 속하는지 예측하는데 사용
- 클러스터링과 유사하지만, 분류분석은 각 그룹이 정의되어 있음
- 교사학습 (supervised learning)에 해당하는 예측기법



##### 나. 예측분석의 정의

- 시계열분석처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것
- 여러 개의 설명변수(독립변수)가 아닌, 한 개의 설명변수로 생각하면 됨



##### 다. 분류분석과 예측분석의 공통점과 차이점

- 공통점
  - 레코드의 특정 속성의 값을 미리 알아맞히는 점
- 차이점
  - 분류  :레코드(튜플)의 **범주형 속성**의 값을 알아 맞춤
  - 예측 : 레코드 (튜플)의 **연속형 속성**의 값을 알아 맞춤



##### 라. 예

- 분류
  - 학생들의 국어, 영어, 수학 점수를 통해 내신등급을 맞추는 것
  - 카드회사에서 회원들의 가입 정보를 통해 1년 후 신용등급을 맞추는 것
- 예측
  - 학생들의 여러 가지 정보를 입력하여 수능점수를 맞추는 것
  - 카드회사 회원들의 가입정보를 통해 년 매출액을 맞추는 것



##### 마. 분류 모델링

- 신용평가모형 (우량, 불량)
- 사기방지모형 (사기, 정상)
- 이탈모형 (이탈, 유지)
- 고객세분화 (VVIP, VIP, GOLD, SIVER, BRONZE)



##### 바.  분류 기법

- 회귀분석, 로지스틱 회귀분석 (logistic regression)
- 의사결정나무 (DT), CART (Classification and Regression Tree), C5.0
- 베이지안분류 (Bayesian classification), Naive Bayesian
- 인공신경망 (ANN, artificial neural network)
- SVM (support vector machine, 지지도벡터기계)
- KNN (K-nearest neighborhood)
- 규칙기반의 분류와 사례기반추론 (Case-Based Reasoning)



#### 2. 로지스틱 회귀분석 (Logistic Regression) (p.397)

- 반응변수가 범주형인 경우 적용되는 회귀분석모형

- 신규 설명변수 추정 및 기준치에 따라 분류하는 목적(분류모형)으로 활용

- 이때 모형의 적합을 통해 추정된 확률을 사후확률(Posterior Probability)라고 함

  > 오즈비(odds ratio) : 오즈(odds)는 성공할 확률이 실패할 확률의 몇 배인지를 나타내는 확률
  >
  > ex) 16강에 한국과 브라질이 진출을 성공/실패할 확률과 각각의 오즈와 오즈비는 아래와 같음
  >
  > |  구분  | 16강 성공확률 | 16강 실패확률 |
  > | :----: | :-----------: | :-----------: |
  > | 브라질 |      0.8      |      0.2      |
  > |  한국  |      0.1      |      0.9      |
  >
  > odds (브라질) : $\dfrac {0.8} {1 - (0.8)} = \dfrac {0.8} {0.2} = 4$
  >
  > odds (한국) : $\dfrac {0.1} {1-0.1} = \dfrac {1} {9}$
  >
  > Odds ratio : $\dfrac {odds(브라질)} {odds(한국)} = \dfrac {4}{\dfrac {1}{9}} = 36$
  >
  > 오즈비가 36 이 나타나 브라질이 16강에 진출할 확률이 한국의 16강 진출 확률보다 36배 높다고 볼 수 있다.

- 선형회귀분석과 로지스틱 회귀분석 비교

  |    목적     |  선형회귀분석  |         로지스틱 회귀분석          |
  | :---------: | :------------: | :--------------------------------: |
  |  종속변수   |  연속형 변수   |               (0, 1)               |
  | 계수 추정법 |   최소제곱법   |           최대우도추정법           |
  |  모형 검정  | F-검정, T-검정 | 카이제곱 검정 (X<sup>2</sup>-test) |

  > 최대우도추정법 (MLE : Maximum Likelihood Estimation) : 모수가 미지의 &theta; (theta)인 확률분포에서 뽑은 표본(관측치) x들을 바탕으로 &theta;를 추정하는 기법

- glm() 함수를 활용하여 로지스틱 회귀분석 실행

- R코드 : glm(종속변수 ~ 독립변수1 +...+ 독립변수k, family=binomial, data=데이터셋명)



#### 3. 의사결정나무

##### 가. 정의

- 분류함수를 의사결정 규칙으로 이뤄진 **나무 모양으로 그리는 방법**
- 의사결정나무는 주어진 **입력값에 대하여 출력값을 예측하는 모형**으로 분류나무와 회귀나무 모형이 있다.



##### 나. 예측력과 해석력

- 기대 집단의 사람들 중 가장 많은 반응을 보일 **고객의 유치방안을 예측**하고자 하는 경우에는 **예측력**에 치중한다.
- 신용평가에서는 심사 결과 부적격 판정이 나온 경우 고객에게 부적격 **이유를 설명**해야 하므로 **해석력**에 치중한다.



##### 다. 의사결정나무의 활용

1. 세분화
2. 분류
3. 예측
4. 차원축소 및 변수선택
5. 교호작용 효과의 파악 
   - 여러 개의 예측변수들을 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우
   - 범주의 병햡 또는 연속형 변수의 이산화



##### 라. 의사결정나무의 특징

- 장점
  - 결과를 누구에게나 설명하기 용이
  - 만드는 방법이 계산적으로 복잡하지 않음
  - 대용량 데이터에서도 빠르게 만들 수 있음
  - 비정상 잡음 데이터에 대해서도 민감함 없이 분류 가능
  - 한 변수와 상관성이 높은 다른 불필요한 변수가 있어도 크게 영향을 받지 않음
  - 설명변수나 목표변수에 수치형변수와 범주형변수를 모두 사용 가능하다.
  - 모형 분류 정확도가 높다.
- 단점
  - 새로운 자료에 대한 과적합 발생할 가능성이 높다.
  - 분류 경계선 부근의 자료값에 대해서 오차가 크다.
  - 설명변수 간의 중요도를 판단하기 쉽지 않다.



##### 마. 의사결정나무의 분석 과정

- 분석과정은 크게 성장(growing), 가지치기(pruning), 타당성 평가, 해석 및 예측으로 이뤄짐



##### 바. 나무의 성장

- 분리기준 : 이산형 목표변수

  |       기준값        | 분리기준                                                     |
  | :-----------------: | ------------------------------------------------------------ |
  | 카이제곱 통계량 P값 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
  |      지니 지수      | 지니 지수를 감소시켜주는 예측변수와 그 때의 최적분리에 의해서 자식마디를 선택 |
  |    엔트로피 지수    | 엔트로피 지수가 가장 작은 예측 변수와 이 때의 최적분리에 의해 자식마디를 형성 |

- 분리기준 : 연속형 목표변수

  |        기준값        | 분리기준                                                     |
  | :------------------: | ------------------------------------------------------------ |
  | 분산분석에서 F통계량 | P값이 가장 작은 예측변수와 그 때의 최적분리에 의해서 자식마디를 형성 |
  |    분산의 감소량     | 분산의 감소량을 최대화하는 기준의 최적분리에 의해서 자식마디를 형성 |

  

#### 4. 불순도의 여러 가지 측도 (p.405)

- 목표변수가 범주형 변수인 의사결정나무의 분류규칙을 선택하기 위해서는 카이제곱 통계량, 지니지수, 엔트로피 지수를 활용

1. 카이제곱 통계량

   - 각 셀에 대한 ((실제도수 - 기대도수)의 제곱 / 기대도수)의 합
   - 기대도수 = 행의 합계 X 열의 합계 / 전체 합계
   -  $X^2 =\displaystyle \sum_{i=1}^k \dfrac {(O_i - B_i)^2} {B_i}$ (k: 범주의 수, O: 실제도수, B: 기대도수)

2. 지니지수

   - 노드의 불순도를 나타내는 값
   - 지니지수의 값이 클수록 이질적(Diversity)이며 순수도(Purity)가 낮다고 볼 수 있다.
   - $Gini(T) = 1 - \displaystyle \sum_{l=1}^kP_l^2$

   ![gini-index](https://user-images.githubusercontent.com/291782/163111164-6b8ab5b6-f0be-4f50-a05e-7677a384df1f.png)

3. 엔트로피 지수

   - 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도

   - 엔트로피 지수의 값이 클수록 순수도(purity)가 낮다고 볼 수 있다.

   - 엔트로피 지수가 가장 작은 예측 변수와 이때의 최적분리 규칙에 의해 자식마디를 형성

   - $Entropy(T) = -(\displaystyle \sum_{l=1}^kP_llog_2P_l)$

     ![entropy-index](https://user-images.githubusercontent.com/291782/163111601-5dbfaf6e-dc4c-43d1-b1fc-993931bcd2d5.png)

     ![index-ex](https://user-images.githubusercontent.com/291782/163112135-a23e077a-5b2d-4cbe-9a13-93c8fd036ab7.png)

     ![gini-ex](https://user-images.githubusercontent.com/291782/163118061-6084da81-8347-4b19-8ec6-fb7a40ccd977.png)



#### 5. 의사결정나무 알고리즘 (p.408)

1. CART (Classification And Regression Tree)
   - 가장 많이 활용되는 의사결정나무 알즘으로 불순도의 측도로 출력(목적) 변수가 **범주형일 경우 지니지수**를, **연속형인 경우 이진분리**(binary split)를 사용
   - 개별 입력변수 뿐만 아니라 입력변수들의 선형겹할들 중에서 최적의 분리를 찾을 수 있다.
2. C4.5와 C5.0
   - CART와는 다르게 각 마디에서 다지분리(multiple split)가 가능하며 범주형 입력변수에 대해서는 범주의 수만큼 분리가 일어난다.
   - 불순도의 측도로는 **엔트로피지수**를 사용한다.
3. CHAID (Chi-squared Automatic Interaction Detection)
   - 가지치기를 하지 않고 적당한 크기에서 나무모형의 성장을 중지하며 입력변수가 반드시 범주형 변수이어야 한다.
   - 불순도의 측도로는 **카이제곱** 통계량을 사용



#### 6. 의사결정나무 예시 (p.408)

```R
> # install.packages("party")
> # party 패키지를 이용하여 의사결정나무 사용 
> library(party)
> # 7:3 으로 train , test 데이터 나누기
> idx <- sample(2, nrow(iris), replace = TRUE, prob = c(0.7, 0.3))
> train.data <- iris[idx==1,]
> test.data <- iris[idx==2,]
> head(train.data)
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
4           4.6         3.1          1.5         0.2  setosa
5           5.0         3.6          1.4         0.2  setosa
6           5.4         3.9          1.7         0.4  setosa
7           4.6         3.4          1.4         0.3  setosa
9           4.4         2.9          1.4         0.2  setosa
10          4.9         3.1          1.5         0.1  setosa
> iris.tree <- ctree(Species~., data=train.data)
> plot(iris.tree)
```

![dt-plot1](https://user-images.githubusercontent.com/291782/163331102-a0caafe6-b2e8-4b1e-9516-5028ad874a3a.png)

```R
> plot(iris.tree, type="simple")
```

![dt-plot-simple](https://user-images.githubusercontent.com/291782/163331226-b5ca401e-fba8-4ce0-b210-9a1dd09cd533.png)

```R
> # 예측된 데이터와 실제 데이터 비교
> table(predict(iris.tree), train.data$Species)
            
             setosa versicolor virginica
  setosa         36          0         0
  versicolor      0         36         3
  virginica       0          0        30

> # test data를 적용하여 정확성 확인 
> test.pre <- predict(iris.tree, newdata=test.data)
> table(test.pre, test.data$Species)
            
test.pre     setosa versicolor virginica
  setosa         12          0         0
  versicolor      2         12         1
  virginica       0          2        16
```





### 3절 앙상블 분석

#### 1. 앙상블 (ensemble) (p.412)

##### 가. 정의

- 여러 예측모형들을 만든 후 예측모형들을 조합하여 하나의 최종 예측 모형을 만드는 방법으로 다중 모델 조합(combining multiple models), 분류기 조합(classifier combination)이 있다.





##### 나. 학습방법의 불안정성

- 가장 안정적인 방법으로는 1-nearest neighbor (가장 가까운 자료만 변하지 않으면 예측 모형이 변하지 않음), 선형회귀모형(최소제곱법으로 추정해 모형 결정)이 존재한다.
- 가장 불안정한 방법으로는 의사결정나무가 있다.



##### 다. 앙상블 기법의 종류

1. 배깅
   - 여러개의 붓스트랩(bootstrap) 자료를 생성하고 각 붓스트랩 자료에 예측 모형을 만든 후 결합하여 최종 예측모형을 만드는 방법이다. 붓스트랩은(bootstrap)은 주어진 자료에서 동일한 크기의 표본을 랜덤 복원추출로 뽑은 자료를 의미
   - 보팅(voting)은 여러 개의 모형으로부터 산출된 결과를 다수결에 의해서 최종 결과를 선정하는 과정
   - 최적 의사결정나무를 구축 시 가장 어려운 작업이 가지치기(pruning)이지만 배깅에서는 가지치기를 하지 않고 최대한 성장한 의사결정나무들을 활용
2. 부스팅
   - 예측력이 약한 모형(weak learner)들을 결합하여 강한 예측모형을 만드는 방법
   - 부스팅 방법 중 Freund & Schapire가 제안한 **Adaboost**는 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 각각 가중치를 설정하고 n개의 분류기를 결합하여 최종 분류기를 만드는 방법을 제안하였다. (단, 가중치의 합은 1)
   - 훈련 오차를 빨리 그리고 쉽게 줄일 수 있음
   - 예측 오차가 향상되어 Adaboost의 성능이 배깅보다 뛰어난 경우가 많다.
3. 랜덤포레스트 (random forest)
   - 의사결정나무의 특징인 분산이 크다는 점을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 주어 **약한 학습기들을 생성한 후 이를 선형결합하여 최종 학습기를 만드는 방법**
   - random forest 패키지는 random input 에 따른 forest of tree를 이용한 분류방법
   - 수천 개의 변수를 통해 변수제거 없이 실행되므로 정확도 측면에서 좋은 성과를 보인다.
   - 이론적 설명이나 최종 결과해석이 어렵다는 단점이 있지만 예측력이 매우 높은 것으로 알려져 있다. 특히 입력변수가 많은 경우, 배깅과 부스팅보다 비슷하거나 좋은 예측력을 보인다.



### 4절 인공신경망 분석

#### 1. 인공신경망 분석 (ANN) (p.418)

##### 가. 인공신경망이란?

- 뇌를 기반으로 한 추론 모델
- 뉴런은 기본적인 정보처리 단위



##### 나. 인공신경망의 연구

- 헵(Hebb) : 신경세포(뉴런) 사이의 연결강도(weight)를 조정하여 학습규칙을 개발
- 로젠블럿(Rosenblatt, 1955) : 퍼셉트론 (perceptron)이라는 인공세포를 개발
- 비선형성의 한계점 발생 : XOR (eXclusive OR) 문제를 풀지 못하는 한계를 발견
- 홉필드(Hopfild), 러멜하트(Rumelhart), 맥클랜드(McClelland) : 역전파알고리즘(Backpropagation)을 활용하여 비선형성을 극복한 다계층 퍼셉트론으로 새로운 인공신경망 모형이 등장했다.



##### 라. 인공신경망의 학습     

- 신경망은 가중치를 반복적으로 조정하며 학습한다.
- 뉴런은 링크(Link)로 연결되어 있고, 각 링크에는 수치적인 가중치가 있다.



##### 마. 인공신경망의 특징

- 뉴런의 활성화 함수

    - **시그모이드 함수**의 경우 로지스틱 회귀분석과 유사하며, 0 ~ 1의 확률값을 가진다.

        <img width="878" alt="sigmoid-fn" src="https://user-images.githubusercontent.com/291782/163423927-052bd171-81cc-45b6-97d8-70ba67c1a4e5.png">

    - softmax 함수 : 표준화지수 함수로도 불리며, 출력값이 여러개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수

        - $y_i = \dfrac {exp(z_j)} {\displaystyle \sum_{i=1}^Lexp(z_i)}, j = 1, ... ,L$

    - Relu함수 : 입력값이 0 이하는 0, 0 초과는 x값을 가지는 함수. 최근 딥러닝에서 많이 활용하는 활성화 함수

        - $Y^{relu} = \begin{cases} 0,\; if \quad x \le 0 \\ x, \; if \quad x \gt 0  \end{cases}$

    - 단일 뉴런의 학습 (단층 퍼셉트론)

        - 퍼셉트론은 선형 결합기와 하드 리미터로 구성된다.

        - 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눈다.

            <img width="818" alt="hyperplane" src="https://user-images.githubusercontent.com/291782/163426523-78c739e8-fe97-499d-8d72-f111438f3988.png">



##### 바. 신경망 모형 구축 시 고려사항

1. 입력 변수
2. 가중치의 초기값과 다중 최소값 문제
3. 학습모드
4. 은닉층(hidden layer)과 은닉노드(hidden node)의 수
5. 과대 적합 문제



### 5절 군집분석

#### 1. 군집분석 (p.424)
##### 가. 개요

- 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간의 상이성을 규명하는 분석 방법
- 특성에 따라 고객을 여러 개의 배타적인 집단으로 나누는 것



##### 나. 특징

1. 요인분석과의 차이점 : 요인분석은 유사한 변수를 함께 묶어주는 것이 목적
2. 판별분석과의 차이점 : 판별분석은 사전에 집단이 나누어져 있는 자료를 통해 새로운 데이터를 기존의 집단에 할당하는 것이 목적



#### 2. 거리 (p.425)

##### 가. 연속형 변수의 경우

- 유클리디안 거리 (Euclidean distance) : 데이터간의 유사성을 측정할 때 많이 사용하는 거리. 통계적 개념이 내포되어 있지 않아 변수들의 산포 정도가 전혀 감안되어 있지 않았다.

    $d(x,y) = \displaystyle \sqrt{(x_1 - y_1)^2 + \dots + (x_p - y_p)^2} = \sqrt{(x - y)^`(x-y)}$

- 표준화 거리 (statistical distance) : 해당 변수의 표준편차로 척도 변환 후 유클리디안 거리를 계산하는 방법. 표준화하게 되면 척도의 차이, 분산의 차이로 인한 왜곡을 피할 수 있다.

    $d(x , y) = \sqrt{(x - y)^`D^{-1}(x - y)} , \quad D = diag$ {$s_{11}, \dots , s_{pp}$}

- 마할라노비스 (Mahalanobis) 거리 : 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리 (standardized distance). 두 벡터 사이의 거리를 산포를 의미하는 표본공분산으로 나눠주어야 하며, 그룹에 대한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란한다.

    $d(x, y) = \sqrt {(x-y)^`S^{-1} (x - y)} \quad S = ${$S_{ij}$}는 공분산행렬

- 맨하탄거리 (Manhattan) 거리 : 유클리디안 거리와 함께 가장 많이 사용되는 거리로 맨하탄 도시에서 건물을 가기 위한 최단 거리를 구하기 위해 고안딘 거리.

    $d(x,y) = \displaystyle \sum_{i=1}^p |x_i - y_i|$

- 민코우스키 (Minkowski) 거리 : 맨하탄 거리와 유클리디안 거리를 한 번에 표현한 공식으로 L1 거리 (맨하탄), L2 거리(유클리디안 거리)라 불리고 있다.

    $d(x, y) = [\displaystyle \sum_{i=1}^p |x_i - y_i|^m]^{1/m} \quad m=1, m=2$



##### 나. 범주형 변수의 경우

- 자카드 거리 : $1 - J(A, B) = \dfrac {|A \cup B| - |A \cap B|} {|A \cup B|}$

- 자카드 계수 : $J(A, B) = \dfrac {|A \cap B|} {|A \cup B|}$

- 코사인 거리 : 문서를 유사도를 기준으로 분류 혹은 그룹핑 할 떄 유용하게 사용한다.

    $d_{cos}(A, B) = 1 - \dfrac {A \cdot B } {||A||_2 \cdot || B ||_2}$

    <img width="855" alt="cos-dist" src="https://user-images.githubusercontent.com/291782/163675115-997131bf-6f0b-4ddf-a8ea-afd69bb1ea62.png">

    

    #### 3. 계층적 군집분석 (p.426)

    - 계층적 군집방법은 n개의 군집으로 시작해 점차 군집의 개수를 줄여 나가는 방법
    - 방법에는 합병형 방법 (agglomerative: bottom-up) 과 분리형 방법 (Divisive : top-down)이 있다.

    

    ##### 가. 최단연결법 (single linkage, nearest neighbor)

    - n*n 거리행렬에서 거리와 가장 가까운 데이터를 묶어서 군집을 형성

    - 군집과 군집 또는 데이터와의 거리를 계산 시 최단거리(min)를 거리로 계산하여 거리행렬 수정을 진행한다.

        <img width="864" alt="nearest-neighbor" src="https://user-images.githubusercontent.com/291782/163675248-f9e952bb-a8d2-47bb-a1f3-d7eeb5f0c658.png">

    

    ##### 나. 최장연결법 (complete linkage, farthest neighbor)

    - 군집과 군집 또는 데이터와의 거리를 계산할 때 최장거리 (max)로 거리를 계산하여 거리행렬을 수정하는 방법

        <img width="867" alt="farthest-neighbor" src="https://user-images.githubusercontent.com/291782/163675302-c5ce2e2f-f964-472b-8d3c-1994bbc3ac39.png">

    

    ##### 다. 평균연결법 (average linkage)

    - 군집과 군집 또는 데이터와의 거리를 계산할 때 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법

        <img width="861" alt="mean-linkage" src="https://user-images.githubusercontent.com/291782/163675340-b400d381-04fd-42f6-9a6f-bc9671680442.png">

    

    

    ##### 라. 와드연결법 (ward linkage)

    - 군집내 편차들의 제곱합을 고려한 방법
    - 군집 간 정보의 손실을 최소화하기 위해 군집화를 진행

    

    ##### 마. 군집화

    - 거리행렬을 통해 가장 가까운 거리의 객체들간의 관계를 규명하고 덴드로그램을 그림

    - 덴드로그램을 보고 군집의 개수를 변화해 가면서 적절한 군집 수를 선정. (보통 5개 이상은 잘 활용하지 않음)

    - 군집화 단계

        1. 거리행렬을 기준으로 덴드로그램을 그림
        2. 덴드로그램 최상단부터 세로축의 개수에 따라 가로선을 그어 군집의 개수를 선택
        3. 각 객체들의 구성을 고려해서 적절한 군집수를 선정

        <img width="845" alt="dendrogram" src="https://user-images.githubusercontent.com/291782/163675452-b00ff6c3-2012-46ff-9414-f8dac0405b89.png">





#### 4. 비계층적 군집분석 (p.429)

n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최소화한 군집을 형성하는 것

##### 가. K-평균 군집분석 (k-means clustering)의 개념

- 주어진 데이터를 k개의 클러스터로 묶는 알고리즘으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작



##### 나. K-평균 군집분석 (k-means clustering) 과정

- 원하는 군집의 개수와 초기 값(seed)들을 정해 seed 중심으로 군집을 형성
- 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류
- 각 군집의 seed 값을 다시 계산
- 모든 개체가 군집으로 할당될 때까지 위 과정들을 반복



##### 다. K-평균 군집분석의 특징

- 거리 계산을 통해 군집화가 이루어지므로 **연속형 변수에 활용이 가능**
- K개의 **초기 중심값은 임의로 선택이 가능**하며 가급적이면 멀리 떨어지는 것이 바람직하다.
- 초기 중심값을 임의로 선택할 때 일렬(위아래, 좌우)로 선택하면 군집 혼합되지 않고 층으로 나누어질 수 있어 주의하여야 한다. **초기 중심값의 선정에 따라 결과가 달라**질 수 있다.
- 초기 중심으로부터의 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 **탐욕적(greedy) 알고리즘**이므로 안정된 군집은 보장하나 최적이라는 보장은 없다.
- 장점
    - 알고리즘이 단순하며, 빠르게 수행되어 분석 방법 적용이 용이
    - 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있다.
    - 내부 구조에 대한 사전정보가 없어도 의미있는 자료구조를 찾을 수 있다.
    - 다양한 형태의 데이터에 적용이 가능
- 단점
    - 군집의 수, 가중치와 거리 정의가 어렵다.
    - 사전에 주어진 목적이 없으므로 결과 해석이 어렵다.
    - 잡음이나 이상값의 영향을 많이 받는다.
    - 볼록한 형태가 아닌 (non-convex) 군집이 (예를 들어 U형태의 군집) 존재할 경우에는 성능이 떨어진다.
    - 초기 군집수 결정에 어려움이 있다.





#### 5. 혼합 분포 군집 (mixture distribution clustering) (p.431)

##### 가. 개요

- 모형 기반 (model-based)의 군집 방법이며, 모집단 모형으로부터 나왔다는 가정하에서 모수와 함께 가중치를 자료로부터 추정하는 방법을 사용
- K개의 각 모형은 군집을 의미하며, 각 데이터는 추정된 K개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집의 분류가 이루어진다.
- 혼합모형에서 모수와 가중치의 추정 (최대가능도추정)에는 **EM알고리즘**이 사용됨



##### 나. 혼합 분포모형으로 설명할 수 있는 데이터 형태

<img width="769" alt="image" src="https://user-images.githubusercontent.com/291782/163675800-e3cf8239-129f-469a-97ec-bc981a843bc3.png">

- (a)는 자료의 분포형태가 다봉형의 형태를 띠므로 단일 분포로의 적합은 적절하지 않으며, 대략 3개 정도의 정규분포 결합을 통해 설명될 수 있을것으로 생각할 수 있다.
- (b)의 경우에도 여러 개의 이변량 정규분포의 결합을 통해 설명될 수 있을 것이다. 두 경우 모두 반드시 정규분포로 제한할 필요는 없다.



##### 다. EM (Expectation-Maximization) 알고리즘의 진행 과정

<img width="942" alt="em-algo" src="https://user-images.githubusercontent.com/291782/163675876-58a4834d-0dbe-49d2-aeae-c2e22e2e963d.png">



##### 라. EM 알고리즘의 진행 과정

1) 혼합분포 군집모형의 특징
    - K-평균군집의 절차와 유사하지만 **확률분포를 도입하여 군집을 수행**한다.
    - 군집을 몇개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있다.
    - EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있다.
    - 군집의 크기가 너무 작으며 추정의 정도가 떨어지거나 어려울 수 있다.
    - K-평균군집과 같이 **이상치 자료에 민감**하므로 사전에 조치가 필요하다.





#### 6. SOM (Self Organizing Map) (p.432)

##### 가. 개요

- 자기조직화지도 (SOM) 알고리즘은 코호넨 (Kohonen)에 의해 제시, 개발되었으며 코호넨 맵(kohonen maps)이라고도 알려져 있다.

- **SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬**하여 지도의 형태로 형상화 한다.

    <img width="466" alt="image" src="https://user-images.githubusercontent.com/291782/163676024-410958e8-d1fa-46b2-abe2-26a2e42e6fb6.png">



##### 나. 구성

- SOM 모델은 위 그림과 같이 두 개의 인공신경망 층으로 구성되어 있다.
    1. 입력층 (input layer: 입력벡터를 받는 층)
        - **입력 변수의 개수와 동일하게 뉴런 수가 존재**한다.
        - 입력층의 자료는 학습을 통하여 경쟁층에 정렬되는데, 이를 지도 (map)라 부른다.
        - 입력층에 있는 뉴런은 경쟁층에 있는 뉴런들과 연결되어 있으며, 이때 완전연결 (fully connected)되어 있다.
    2. 경쟁층 (competitive layer : 2차원 격자(grid)로 구성된 층)
        - 입력벡터의 특성에 따라 벡터가 한 점으로 클러스터링 되는 층
        - SOM은 경쟁 학습으로 각각의 뉴런이 입력 벡터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하며 학습한다.
        - 입력 층의 표본 벡터에 가장 가까운 프로토타입 벡터를 선택해 BMU(Best Matching Unit)라고 하며, 코호넨의 승자 독점의 학습 규칙에 따라 위상학적 이웃 (topological neighbors)에 대한 연결 강도를 조정한다.



##### 다. 특징

- 고차원의 데이터를 저차원의 **지도 형태로 형상화**하기 때문에 시각적으로 이해가 쉽다.
- 실제 데이터가 유사하면 지도상에서 가깝게 표현된다. 이런 특징 때문에 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보인다.
- 역전파 (Back Propagation) 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스 (feed-forward flow)를 사용함으로써 속도가 매우 빠른다. 실시간 학습처리를 할 수 있는 모형이다.



##### 라. SOM과 신경망 모형의 차이점

|         구분          |          신경망 모형          |                  SOM                  |
| :-------------------: | :---------------------------: | :-----------------------------------: |
|       학습방법        |         오차역전파법          |             경쟁학습방법              |
|         구성          |    입력층, 은닉층, 출력층     | 입력층, 2차원 격자(grid)형태의 경쟁층 |
| 기계 학습 방법의 분류 | 지도학습(Supervised learning) |   비지도학습(Unsupervised learning)   |





#### 7. 최신 군집분석 기법들 (p.434)

##### 가. iris 데이터를 활용한 기법 확인

1. Hierarchical Clustering (R 예제)

    ```R
    > idx <- sample(1:dim(iris)[1], 40)
    > iris.s <- iris[idx,]
    > iris.s$Species <- NULL
    > hc <- hclust(dist(iris.s), method="ave")
    > plot(hc, hang=-1, labels=iris$Species[idx])
    ```

    ![iris-dendrogram](https://user-images.githubusercontent.com/291782/163717239-dfae4a63-53d9-4f32-839a-557368465b31.png)

    

2. K-means Clustering (R 예제) 꼭 해봐라

    -   비계층적 군집방법으로 사용가능

    2-1 군집화

    ```R
    > # k-means
    > # 군집화 
    > data(iris)
    > newiris <- iris
    > newiris$Species <- NULL
    > kc <- kmeans(newiris, 3)
    > # 결과비교 
    > table(iris$Species, kc$cluster)
                
                  1  2  3
      setosa     50  0  0
      versicolor  0  2 48
      virginica   0 36 14
    > # 군집화 그래프
    > plot(newiris[c("Sepal.Length", "Sepal.Width")], col=kc$cluster)
    ```

    ![k-means-cluster](https://user-images.githubusercontent.com/291782/163717403-9c05da80-59ce-476a-931f-078491605354.png)

    




### 6절 연관분석

연관분석의 개념, 측도와 장단점을 완벽히 학습해야 함

#### 1. 연관규칙 (p.437)

##### 가. 연관규칙분석(Association analysis)의 개념

- 연관성 분석은 흔히 장바구니분석(market basket analysis) 또는 서열분석 (sequence analysis) 이라고 불린다.
- 기업의 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 적용
- 장바구니 분석 : 장바구니에 무엇이 같이 들어 있는지에 대한 분석
- 서열분석 : A를 산 다음에 B를 산다.



##### 나. 연관규칙의 형태

- 조건과 반응의 형태 (if-then)로 이루어져 있다. (if A then B: 만일 A가 일어나면 B가 일어난다.)



##### 다. 연관규칙의 측도

- 산업의 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 한다.

1. 지지도 (support)

    - 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율로 정의
    - 지지도 = $P(A \cap B) = \dfrac {A와 B가 동시에 포함된 거래수} {전체 거래수} = \dfrac {A \cap B} {전체}$

2. 신뢰도 (confidence)

    - 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률. 연관성의 정도를 파악 가능
    - 신뢰도 = $\dfrac {P(A \cap B)} {P(A)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수} = \dfrac {지지도} {P(A)}$

3. 향상도 (Lift)

    - A가 구매되지 않았을 떄 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률의 증가 비이다. 연관규칙 A&rarr;B는 품목 A와 품목 B의 구매가 서로 관련이 없는 경우에 향상도가 1이 된다.

    - 향상도 = $\dfrac {P(B|A)} {P(B)} = \dfrac {P(A \cap B)} {P(A)P(B)} = \dfrac {A와 B가 동시에 포함된 거래수} {A를 포함하는 거래수 \; X \; B를 포함하는 거래수} = \dfrac {신뢰도} {P(B)}$ 

        <img width="935" alt="support-confidence-lift" src="https://user-images.githubusercontent.com/291782/163676755-3b1a3876-f2d0-4ce5-b748-1bf5f4038e19.png">



##### 라. 연관규칙의 절차

- 최소 지지도 결정 > 품목 중 최소 지지도를 넘는 품목 분류 > 2가지 품목 집합 생성 > 반복적으로 수행해 빈발품목 집합을 찾음



##### 마. 연관규칙의 장점과 단점

- 장점
    - 탐색적인 기법으로 조건 반응으로 표현되는 연관성 분석의 결과를 쉽게 이해할 수 있다.
    - 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없으므로 유용하게 활용 된다.
    - 사용이 편리한 분석 데이터의 형태로 거래 내용에 대한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 갖는다.
    - 분석을 위한 계산이 간단한다.
- 단점 (개선방안)
    - 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어난다.
    - 이를 개선하기 위해 유사한 품목을 한 범주로 일반화 한다.
    - 너무 세분화한 품목을 갖고 연관성 규칙을 찾으면 의미없는 분석이 될 수도 있다.



##### 바. 순차패턴 (sequence analysis)

- 동시에 구매될 가능성이 큰 상품군을 찾아내는 연관성분석에 시간이라는 개념을 포함시켜 순차적으로 구매 가능성이 큰 상품군을 찾아내는 것



#### 2. 기존 연관성분석의 이슈 (p.439)

- 대용량 데이터에 대한 연관성분석이 불가능
- 시간이 많이 걸리거나 실행 시 다운되는 현상이 발생할 수 있다.





#### 4. 연관성분석 활용방안 (p.440)

- 장바구니 분석의 경우 실시간 상품추천을 통한 교차판매에 응용
- 순차패턴 분석은 A를 구매한 사람에게 B를 구매하지 않는 경우, B를 추천하는 교차판매 캠페인에 사용



#### 5. 연관성분석 예제 (p.441)

-   분석내용 : Groceries 데이터셋은 식료품 판매점의 1달 동안의 POS 데이터이며, 총 169개의 제품과 9835건의 거래건수를 포함하고 있다. 거래내역을 **inspect** 함수로 확인할 수 있으며, **apriori** 함수로 최소지지도와 신뢰도는 각각 0.01, 0.3으로 설정한 뒤 연관규칙분석을 실시했다.

-   ```R
    > # 연관성분석
    > # Groceries 데이터셋
    > #install.packages("arules") # Groceries 데이터셋을 위한 패키지 설치
    > library(arules)
    > # 분석결과
    > data(Groceries)
    > inspect(Groceries[1:3])
        items                                                      
    [1] {citrus fruit, semi-finished bread, margarine, ready soups}
    [2] {tropical fruit, yogurt, coffee}                           
    [3] {whole milk}
    
    
    > apriori(Groceries, parameter = list (support = 0.01, confidence=0.3))
    Apriori
    
    Parameter specification:
     confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext
            0.3    0.1    1 none FALSE            TRUE       5    0.01      1     10  rules TRUE
    
    Algorithmic control:
     filter tree heap memopt load sort verbose
        0.1 TRUE TRUE  FALSE TRUE    2    TRUE
    
    Absolute minimum support count: 98 
    
    set item appearances ...[0 item(s)] done [0.00s].
    set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].
    sorting and recoding items ... [88 item(s)] done [0.00s].
    creating transaction tree ... done [0.00s].
    checking subsets of size 1 2 3 4 done [0.00s].
    writing ... [125 rule(s)] done [0.00s].
    creating S4 object  ... done [0.00s].
    set of 125 rules 
    
    
    > inspect(sort(rules, by=c("lift"), decreasing=TRUE)[1:20])
         lhs                                       rhs                support    confidence coverage   lift     count
    [1]  {citrus fruit, other vegetables}       => {root vegetables}  0.01037112 0.3591549  0.02887646 3.295045 102  
    [2]  {tropical fruit, other vegetables}     => {root vegetables}  0.01230300 0.3427762  0.03589222 3.144780 121  
    [3]  {beef}                                 => {root vegetables}  0.01738688 0.3313953  0.05246568 3.040367 171  
    [4]  {citrus fruit, root vegetables}        => {other vegetables} 0.01037112 0.5862069  0.01769192 3.029608 102  
    [5]  {tropical fruit, root vegetables}      => {other vegetables} 0.01230300 0.5845411  0.02104728 3.020999 121  
    [6]  {other vegetables, whole milk}         => {root vegetables}  0.02318251 0.3097826  0.07483477 2.842082 228  
    [7]  {whole milk, curd}                     => {yogurt}           0.01006609 0.3852140  0.02613116 2.761356  99  
    [8]  {root vegetables, rolls/buns}          => {other vegetables} 0.01220132 0.5020921  0.02430097 2.594890 120  
    [9]  {root vegetables, yogurt}              => {other vegetables} 0.01291307 0.5000000  0.02582613 2.584078 127  
    [10] {tropical fruit, whole milk}           => {yogurt}           0.01514997 0.3581731  0.04229792 2.567516 149  
    [11] {yogurt, whipped/sour cream}           => {other vegetables} 0.01016777 0.4901961  0.02074225 2.533410 100  
    [12] {other vegetables, whipped/sour cream} => {yogurt}           0.01016777 0.3521127  0.02887646 2.524073 100  
    [13] {tropical fruit, other vegetables}     => {yogurt}           0.01230300 0.3427762  0.03589222 2.457146 121  
    [14] {root vegetables, whole milk}          => {other vegetables} 0.02318251 0.4740125  0.04890696 2.449770 228  
    [15] {whole milk, whipped/sour cream}       => {yogurt}           0.01087951 0.3375394  0.03223183 2.419607 107  
    [16] {citrus fruit, whole milk}             => {yogurt}           0.01026945 0.3366667  0.03050330 2.413350 101  
    [17] {onions}                               => {other vegetables} 0.01423488 0.4590164  0.03101169 2.372268 140  
    [18] {pork, whole milk}                     => {other vegetables} 0.01016777 0.4587156  0.02216573 2.370714 100  
    [19] {whole milk, whipped/sour cream}       => {other vegetables} 0.01464159 0.4542587  0.03223183 2.347679 144  
    [20] {curd}                                 => {yogurt}           0.01728521 0.3244275  0.05327911 2.325615 170  
    >
    ```






     









서술형공부
데이터를 찾자D
ADsP 5절 데이터 변형
## ADsP 데이터 분석 END
